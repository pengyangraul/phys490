{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Matrices I\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "    html-to-math: katex\n",
    "    html-math-method: katex\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Motivation\n",
    "Linear algebra pops up almost everywhere in physics, so the matrix-related techniques developed below will be used repeatedly in later lectures. As a result, we will spend lots of time on matrices. We will take the time to introduce several numerical techniques in detail. \n",
    "\n",
    "## Examples from Physics\n",
    "We discuss some elementary examples from undergraduate physics.\n",
    "\n",
    "### Rotations in two dimensions\n",
    "Consider a two-dimensional Cartesian coordinate system. A point $\\boldsymbol{r} = (x,y)^T$ can be rotated counter-clockwise through an angle $\\theta$ about the origin, producing a new point $\\boldsymbol{r}' = (x',y')^T$. The two points' coordinates are related as follows:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The $2\\times 2$ matrix appearing here is an example of a _rotation matrix_ in Euclidean space. If you know $\\boldsymbol{r}'$ and wish to calculate $\\boldsymbol{r}$, you need to solve this system of two linear equations. \n",
    "\n",
    "### Electrostatic potentials\n",
    "Assume you have $n$ electric charges $q_j$ (which are unknown) held at the positions $\\boldsymbol{R}_j$ (which are known).  Further assume that you have measured the electric potential $\\phi(r_i)$ at the $n$ known positions $\\boldsymbol{r}_i$. From the definition of the potential (as well as the fact that the potential obeys the principle of superposition), we see that:\n",
    "$$\n",
    "\\phi(\\boldsymbol{r}_i) = \\sum_{j=0}^{n-1}\\left(\\frac{k q_j}{|\\boldsymbol{r}_i - \\boldsymbol{R}_j|}\\right),\n",
    "$$\n",
    "where $i = 0,1,\\dots,n-1$.\n",
    "If you assume you have four charges, the above relation turns into the following $4\\times 4$ linear systems of equations:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_3|\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\phi(\\boldsymbol{r}_0) \\\\ \\phi(\\boldsymbol{r}_1) \\\\ \\phi(\\boldsymbol{r}_2) \\\\ \\phi(\\boldsymbol{r}_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "which needs to be solved for the 4 unknowns $q_0$, $q_1$, $q_2$ and $q_3$.\n",
    "\n",
    "### Principle moments of inertia\n",
    "In study of the rotation of a rigid body about an arbitrary axis in three dimensions, you may have encountered the moment of inertia tensor:\n",
    "$$\n",
    "I_{\\alpha \\beta} = \\int \\rho(\\boldsymbol{r}) \\left(\\delta_{\\alpha \\beta}r^2 - \\boldsymbol{r}_\\alpha \\boldsymbol{r}_\\beta\\right)d^3 r,\n",
    "$$\n",
    "where $\\rho(r)$ is the mass density, $\\alpha$ and $\\beta$ denote Cartesian components, and $\\delta_{\\alpha \\beta}$ is the Kronecker delta. \n",
    "\n",
    "The moment of inertia tensor is represented by a $3\\times 3$ matrix: \n",
    "$$\n",
    "\\boldsymbol{I} = \n",
    "\\begin{pmatrix}\n",
    "I_{xx} & I_{xy} & I_{xz} \\\\\n",
    "I_{yx} & I_{yy} & I_{yz} \\\\\n",
    "I_{zx} & I_{zy} & I_{zz}.\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "This is a symmetric matrix. It is possible to choose a coordinate system such that the off-diagonal elements vanish. \n",
    "This axes of this coordinate system are known as the _principal axes_ for the body at the origin. Then the moment of inertian tensor is represented by a diagonal matrix, with diagonal elements $I_0$, $I_1$, and $I_2$, known as the principal moments. This is an instance of the \"eigenvalue problem\".\n",
    "\n",
    "## The problems to be solved\n",
    "First, we look at the problem where we have $n$ unknowns $x_i$, along with $n\\times n$ coefficients $A_{ij}$ and $n$ constants $b_i$:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A_{00} & A_{01} & \\dots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\dots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\dots & A_{n-1,n-1} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_{n-1}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where we used a comma to separate two indices when this was necessary to avoid confusion. \n",
    "These are $n$ equations linear in $n$ unknowns. \n",
    "\n",
    "In compact matrix form, this problem is written as \n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b},\n",
    "$$\n",
    "where $\\boldsymbol{A}$ is called the _coefficient matrix_. This is a problem that we will spend considerable time solving in this lecture. \n",
    "We will be doing this mainly by using the _augmented coefficient matrix_ which places together the elements of $\\boldsymbol{A}$ and $\\boldsymbol{b}$, i.e.:\n",
    "$$\n",
    "(\\boldsymbol{A}|\\boldsymbol{b})= \\left(\n",
    "\\begin{matrix}\n",
    "A_{00} & A_{01} & \\dots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\dots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\dots & A_{n-1,n-1} \n",
    "\\end{matrix}\\right|\n",
    "\\left.\n",
    "\\begin{matrix}\n",
    "b_0 \\\\ b_1 \\\\ \\vdots \\\\b_{n-1}\n",
    "\\end{matrix}\n",
    "\\right).\n",
    "$$\n",
    "For now we assume the determinant of $\\boldsymbol{A}$ satisfy $|\\boldsymbol{A}| \\neq 0$.\n",
    "\n",
    "In a course on linear algebra you have seen examples of legitimate operations one can carry out while solving the system of linear equations. \n",
    "Such operations change the elements of $\\boldsymbol{A}$ and $\\boldsymbol{b}$, but leave the solution vector $\\boldsymbol{x}$ unchanged. \n",
    "More generally, we are allowed to carry the following elementary row operations:\n",
    "\n",
    "- _Scaling_: each row/equation may be multiplied by a constant (multiplies $|\\boldsymbol{A}|$ by the same constant).\n",
    "- _Pivoting_: two rows/equations may be interchanged (changes sign of $|\\boldsymbol{A}|$).\n",
    "- _Elimination_: a row/equation may be replaced by a linear combination of that row/equation with any other row/equation (doesn't change $|\\boldsymbol{A}|$).\n",
    "\n",
    "Keep in mind that these are operations that are carried out on the augmented coefficient matrix $(\\boldsymbol{A}|\\boldsymbol{b})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we wish to tackle the standard form of the matrix eigenvalue problem:\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{v} = \\lambda \\boldsymbol{v}.\n",
    "$${#eq-eigenvalue}\n",
    "Here, both $\\lambda$ and the column vector $\\boldsymbol{v}$ are unknown. This $\\lambda$ is called an _eigenvalue_ and $\\boldsymbol{v}$ is called an _eigenvector_.\n",
    "\n",
    "Let's sketch one possible approach to solve this problem.  We can move everything to the left-hand side, we have\n",
    "$$\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = \\boldsymbol{0},\n",
    "$$\n",
    "where $\\boldsymbol{I}$ is the $n\\times n$ identity matrix and $\\boldsymbol{0}$ is an $n\\times 1$ column vector made up of $0$s. \n",
    "It is easy to see that we are faced with a system of $n$ linear equations: the coefficient matrix here is $A - \\lambda \\boldsymbol{I}$. \n",
    "\n",
    "The trivial solution is $\\boldsymbol{v} = 0$. In order for a non-trivial solution to exist, we must have vanishing determinant $|\\boldsymbol{A} - \\lambda \\boldsymbol{I}| = 0$.\n",
    "In other words, the matrix $\\boldsymbol{A} - \\lambda \\boldsymbol{I}$ is singular. Expanding the determinant gives us a polynomial equation, known as the _characteristic equation_:\n",
    "$$\n",
    "(-1)^n\\lambda^n + c_{n-1} \\lambda^{n-1} + \\cdots + c_1 \\lambda + c_0 = 0.\n",
    "$$\n",
    "\n",
    "Thus, an $n \\times n $ matrix has at most $n$ distinct eigenvalues, which are the roots of the characteristic polynomial. When a root occurs twice, we say that root has multiplicity $2$. If a root occurs only once, in other words if it has multiplicity 1, we are dealing with a _simple_ eigenvalue.\n",
    "\n",
    "Having calculated the eigenvalues, one way to evaluate the eigenvectors is simply by using @eq-eigenvalue again. \n",
    "\n",
    "- Specifically, for a given/known eigenvalue, $\\lambda_i$, one tries to solve the system of linear equations $(\\boldsymbol{A}-\\lambda_i\\boldsymbol{I})\\boldsymbol{v}_i = 0$ for $\\boldsymbol{v}_i$. \n",
    "- For each value $\\lambda_i$, we will not be able to determine unique values of $\\boldsymbol{v}_i$, so we will limit ourselves to computing the relative values of the components of $\\boldsymbol{v}_i$. \n",
    "- We will in the following use the notation $(v_j)_0$, $(v_j)_1$ etc. to denote the $n$ elements of the column vector $\\boldsymbol{v}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "We now turn to a discussion of practical error estimation in work with matrices. we will provide some general derivations and examples of when a problem is \"well-conditioned\", typically by using matrix perturbation theory (i.e., by checking what happens if there are uncertainties in the input data).\n",
    "\n",
    "After some preliminary comments, examples, and definitions, we will investigate quantitatively how linear systems, eigenvalues, and eigenvectors depend on the input data. We will be examining in each case the simplest scenario but, hopefully, this will be enough to help you grasp the big picture. \n",
    "\n",
    "## From _a posteriori_ to _a priori_ Estimates\n",
    "\n",
    "Let us look at a specific $2\\times 2$ linear system, namely $\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b}$ for the case where\n",
    "$$\n",
    "(\\boldsymbol{A}|\\boldsymbol{b}) = \\left(\n",
    "\\begin{matrix}\n",
    "0.2161 & 0.1441 \\\\\n",
    "1.2969 & 0.8648 \n",
    "\\end{matrix}\\ \\right|\n",
    "\\left.\n",
    "\\begin{matrix}\n",
    "0.1440 \\\\\n",
    "0.8642\n",
    "\\end{matrix}\n",
    "\\right).\n",
    "$${#eq-linear_eq}\n",
    "\n",
    "Simply put, there are two options on how to analyze errors: \n",
    "\n",
    "a. an _a priori_ analysis, in which case we try to see how easy/hard the problem is to solve before we begin solving it.\n",
    "b. an _a posteriori_ analysis, where we have produced a solution, and attempt to see how good it is.\n",
    "\n",
    "Let us start with the latter option, an _a posteriori_ approach.  Say you are provided with the following approximate solution to the problem defined in @eq-linear_eq:\n",
    "$$\n",
    "\\tilde{\\boldsymbol{x}}^T = (0.9911 \\quad -0.4870).\n",
    "$${#eq-approximate_sol}\n",
    "\n",
    "One way of testing how good a solution is, is to evaluate the residue vector:\n",
    "$$\n",
    "\\boldsymbol{r} = \\boldsymbol{b} - \\boldsymbol{A} \\tilde{\\boldsymbol{x}}.\n",
    "$$\n",
    "Plugging in @eq-approximate_sol, we find the residue vector\n",
    "$$\n",
    "\\boldsymbol{r}^T = (-10^{-8} \\quad 10^{-8})\n",
    "$$\n",
    "which might naturally lead you to the conclusion that our approximate solution $\\tilde{\\boldsymbol{x}}$ is pretty good!\n",
    "\n",
    "However, here’s the thing: the exact solution to our problem is actually:\n",
    "$$\n",
    "\\boldsymbol{x}^T = (-2 \\quad 2).\n",
    "$$\n",
    "The approximate solution $\\tilde{\\boldsymbol{x}}$ doesn't contain even a single correct significant figure!\n",
    "\n",
    "With the disclaimer that there’s much more that could be said at the a posteriori level, we now drop this line of attack and turn to an _a priori_ analysis: could we have realized that solving the problem in @eq-linear_eq was difficult? How could we know that there's something pathological about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude of Determinant?\n",
    "\n",
    "### Example 1\n",
    "In an attempt to see what is wrong with our previous example in @eq-linear_eq, we start to make small perturbation to the input data. \n",
    "Imagine we didn't know the values of the coefficients in $\\boldsymbol{A}$ all that precisely. Would anything change? \n",
    "\n",
    "Let us take\n",
    "$$\n",
    "\\Delta \\boldsymbol{A} = \n",
    "\\begin{pmatrix}\n",
    "0.0001 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We want to study the effect of this perturbation on the solution, namely\n",
    "$$\n",
    "(\\boldsymbol{A} + \\Delta \\boldsymbol{A})(\\boldsymbol{x} + \\Delta \\boldsymbol{x}) = \\boldsymbol{b},\n",
    "$$\n",
    "where $\\boldsymbol{b}$ is kept fixed/unperturbed. \n",
    "For the specific case studied here, we find\n",
    "$$\n",
    "(\\boldsymbol{x} + \\Delta \\boldsymbol{x})^T  = ()\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. -2.]\n",
      "[-2.31294090e-04  9.99653059e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[0.2161, 0.1441],[1.2969, 0.8648]])\n",
    "deltaA = np.array([[0.0001, 0],[0, 0]])\n",
    "b = np.array([0.1440, 0.8642])\n",
    "# we use np.linalg.solve(A,b) to solve the equation Ax = b.\n",
    "x = np.linalg.solve(A,b)\n",
    "x_dx = np.linalg.solve(A+deltaA,b)\n",
    "\n",
    "print(x)\n",
    "print(x_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.31294090e-04,  9.99653059e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df1fa0d82bdabb5288f7efc0788d29c4d5bb5f690328690a3d32d2cd65de760c"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
