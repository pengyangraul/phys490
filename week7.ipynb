{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Matrices III\"\n",
    "subtitle: \"Eigenvalue Problem\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "    html-to-math: katex\n",
    "    html-math-method: katex\n",
    "    callout-appearance: minimal\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn to the \"second half\" of linear algebra, namely the matrix eigenvalue problem:\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i.\n",
    "$$\n",
    "The main trick we employed in the previous section is no longer applicable: subtracting a multiple of a row from another row (i.e., the elimination procedure) changes the eigenvalues of the matrix, so it's not an operation we'll be carrying out in what follows. \n",
    "\n",
    "We will be selective and study the special case where our $n \\times n$ matrix $\\boldsymbol{A}$ has $n$ eigenvalues $\\lambda_i$ that are all distinct. This simplifies things considerably, since it means that the $n$ eigenvectors $\\boldsymbol{v}_i$ are linearly independent.  In this case, it is easy to show (as you will discover when you solve the relevant problem) that the following relation holds:\n",
    "$$\n",
    "\\boldsymbol{V}^{-1}\\boldsymbol{A}\\boldsymbol{V} = \\boldsymbol{\\Lambda},\n",
    "$$\n",
    "where $\\boldsymbol{\\Lambda}$ is the diagonal \"eigenvalue matrix\" made up of the eigenvalues $\\lambda_i$: \n",
    "$$\n",
    "\\boldsymbol{\\Lambda} = \n",
    "\\begin{pmatrix}\n",
    "\\lambda_0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\lambda_1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\lambda_{n-1}\n",
    "\\end{pmatrix}\n",
    "$${#eq-diagonalization}\n",
    "and $\\boldsymbol{V}$ is the \"eigenvector matrix\", whose columns are the right eigenvectors $\\boldsymbol{v}_i$:\n",
    "$$\n",
    "\\boldsymbol{V} = (\\boldsymbol{v}_0\\quad \\boldsymbol{v}_1 \\quad \\dots \\quad \\boldsymbol{v}_{n-1}).\n",
    "$$\n",
    "\n",
    "@eq-diagonalization shows how we can _diagonalize_ a matrix $\\boldsymbol{A}$. As solving the eigenproblem is often called _diagonalizing a matrix_.\n",
    "\n",
    "In the following, we shall not assuming that our matrices are sparse or even symmetric, while many problems in physics lead to symmetric matrices, not all do. On the other hand, the eigenvalue problem for nonsymmetric matrices is messy, since the eigenvalues do not need to be real. In what follows, we will study only nonsymmetric matrices that have real (and _distinct_) eigenvalues. \n",
    "\n",
    "You have learned that $\\det(\\boldsymbol{A}−\\lambda\\boldsymbol{I}) = 0$ leads to a characteristic equation (namely a polynomial set to 0). However, finding roots of polynomial is very often an ill-conditioned problem, even when the corresponding eigenvalue problem is perfectly well-conditioned. \n",
    "\n",
    "Thus, it's wiser, instead, to transform the matrix into a form where it's easy to read the eigenvalues off, while ensuring that the eigenvalues of the starting and final matrix are the same.\n",
    "\n",
    "The methods we do employ to computationally solve the eigenvalue problem are iterative; this is different from the system-solving in the previous section, where some methods\n",
    "were direct and some were iterative. \n",
    "\n",
    "We shall in the following introduce the state-of-the-art QR method, currently the gold standard for the case where one requires all eigenvalues. Before we get to it, though, we will discuss the power and inverse-power methods: these help pedagogically, but will also turn out to be conceptually similar to the full-blown QR approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Method\n",
    "\n",
    "As already mentioned, we will start with the simplest possible method, which turns out to be intellectually related to more robust methods. The general problem we are trying to solve is $\\boldsymbol{A}\\boldsymbol{v}_i = \\lambda_i\\boldsymbol{v}_i$: $\\lambda_i$ are the true eigenvalues and $\\boldsymbol{v}_i$ are the true eigenvectors (all of which are currently unknown). Since we're making the assumption that all $n$ eigenvalues are distinct, we are free to sort them such that:\n",
    "$$\n",
    "|\\lambda_0| > |\\lambda_1| > \\cdots > |\\lambda_{n-1}|.\n",
    "$$\n",
    "\n",
    "The power method (in its simplest form) will give us access to only one eigenvalue and eigenvector pair. Specifically, it will allow us to evaluate the largest eigenvalue λ0 (also known as the _dominant_ eigenvalue) and the corresponding eigenvector $\\boldsymbol{v}_0$.\n",
    "\n",
    "## Algorithm: First Attempt\n",
    "\n",
    "We start from an _ad hoc_ guess and then see how we can improve it, as is standard in iterative approaches. The method tells us to start from a vector $\\boldsymbol{z}^{(0)}$ and simply multiply it with the matrix A to get the next vector in the sequence:\n",
    "$$\n",
    "\\boldsymbol{z}^{(k)} = \\boldsymbol{A} \\boldsymbol{z}^{(k-1)}, \\quad k = 1,2,\\dots.\n",
    "$$\n",
    "\n",
    "Obviously, we have $\\boldsymbol{z}^{(1)} = \\boldsymbol{A}\\boldsymbol{z}^{(0)}$, $\\boldsymbol{z}^{(2)} = \\boldsymbol{A}\\boldsymbol{z}^{(1)} = \\boldsymbol{A}^2\\boldsymbol{z}^{(0)}$, and so on. This means \n",
    "$$\n",
    "\\boldsymbol{z}^{(k)} = \\boldsymbol{A}^k\\boldsymbol{z}^{(0)}.\n",
    "$$\n",
    "\n",
    "To see why this has to do with calculating eigenvalues, we express our starting vector $\\boldsymbol{z}^{(0)}$ as a linear combination of the (unknown) eigenvectors:\n",
    "$$\n",
    "\\boldsymbol{z}^{(0)} = \\sum_{i=0}^{n-1} c_i \\boldsymbol{v}_i \n",
    "$$\n",
    "where the coefficient $c_i$ are also unknown. \n",
    "\n",
    "Thus, we have at the $k$-th iteration,\n",
    "$$\n",
    "\\boldsymbol{z}^{(k)} = \\boldsymbol{A}^k \\boldsymbol{z}^{(0)} = \\sum_{i = 0}^{n-1} c_i \\boldsymbol{A}^k \\boldsymbol{v}_i \n",
    "= \\sum_{i = 0}^{n-1}c_i \\lambda_i^k \\boldsymbol{v}_i = c_0 \\lambda_0^k \\boldsymbol{v}_0 + \\lambda_0^k \\sum_{i=1}^{n-1} c_i \\left(\\frac{\\lambda_i}{\\lambda_0}\\right)^k \\boldsymbol{v}_i.\n",
    "$$\n",
    "\n",
    "Since $\\lambda_0$ is the largest eigenvalue in magnitude, we have $(\\lambda_i/\\lambda_0)^k \\to 0$ as $k\\to \\infty$ for $i=1,2,\\dots$.\n",
    "\n",
    "Thus, as long as $c_0\\neq 0$, we have \n",
    "$$\n",
    "\\lim_{k\\to\\infty}\\boldsymbol{z}^{(k)} =  \\lim_{k\\to\\infty} c_0 \\lambda_0^k \\left(\\boldsymbol{v}_0 + \\sum_{i=1}^{n-1} c_i \\left(\\frac{\\lambda_i}{\\lambda_0}\\right)^k \\boldsymbol{v}_i \\right) \\propto \\boldsymbol{v}_0.\n",
    "$$\n",
    "This means as $k$ becomes larger and larger, the vector $\\boldsymbol{z}^{(k)}$ will be closer and closer in parallel to $\\boldsymbol{v}_0$, the eigenvector of $\\boldsymbol{A}$ corresponding to the eigenvalue with the largest magnitude.\n",
    "\n",
    "To evaluate the eigenvalue $\\lambda_0$, we introduce the _Rayleigh quotienta_ of a vector $\\boldsymbol{x}$ as follows:\n",
    "$$\n",
    "\\mu(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}^T \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^T \\boldsymbol{x}}.\n",
    "$$\n",
    "If $\\boldsymbol{x}$ is an eigenvector, then $\\mu(\\boldsymbol{x})$ obviously gives the eigenvalue. If $\\boldsymbol{x}$ is not an eigenvector, then $\\mu(\\boldsymbol{x})$ is the nearest substitute to an eigenvalue (in the least-square sense, which you will learn in later lectures). \n",
    "\n",
    "Now consider $\\boldsymbol{z}^{(k)}$, we have that\n",
    "$$\n",
    "\\lim_{k\\to \\infty}\\mu(\\boldsymbol{z}^{(k)}) = \\lambda_0.\n",
    "$$\n",
    "In other words, for finite $k$, $\\mu(\\boldsymbol{z}^{(k)})$ is our best estimate for $\\lambda_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: Normalizing\n",
    "\n",
    "We could also discuss how to get the dominant eigenvector, $\\boldsymbol{v}_0$, from $\\boldsymbol{z}^{(k)}$.\n",
    "However, we observe that we have ignored a problem in our earlier derivation: in the previous expression for $\\boldsymbol{z}^{(k)}$, the $\\lambda_0^k$ will become unbounded (if $|\\lambda_0|>1$) or tend to 0 (if $\\lambda_0<1$). In order to remedy this, we decide to scale the sequence $\\boldsymbol{z}^{(k)}$ between steps. \n",
    "\n",
    "The simplest way to accomplish such a scaling is to introduce a new sequence $\\boldsymbol{q}^{(k)}$ which has the convenient property that $\\|\\boldsymbol{q}^{(k)} \\| = 1$. \n",
    "In the following we are employing the Euclidean norm implicitly. To do this, we scale $\\boldsymbol{z}^{(k)}$ with its norm,\n",
    "$$\n",
    "\\boldsymbol{q}^{(k)} = \\frac{\\boldsymbol{z}^{(k)}}{\\| \\boldsymbol{z}^{(k)} \\|},\n",
    "$$ \n",
    "which gives $\\|\\boldsymbol{q}^{(k)} \\| =1$. In this case, the Reyleigh quotient is\n",
    "$$\n",
    "\\mu(\\boldsymbol{q}^{(k)})  = [\\boldsymbol{q}^{(k)}]^T \\boldsymbol{A} \\boldsymbol{q}^{(k)}.\n",
    "$$\n",
    "\n",
    "Thus, our new normalized power-method algorithm can be summarized as the following sequence of steps\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\boldsymbol{z}^{(k)} = \\boldsymbol{A}\\boldsymbol{q}^{(k-1)} \\\\\n",
    "\\boldsymbol{q}^{(k)} = \\frac{\\boldsymbol{z}^{(k)}}{\\|\\boldsymbol{z}^{(k)}\\|} \\\\\n",
    "\\mu(\\boldsymbol{q}^{(k)})  = [\\boldsymbol{q}^{(k)}]^T \\boldsymbol{A} \\boldsymbol{q}^{(k)}.\n",
    "\\end{gather*}\n",
    "$$ \n",
    "Thus, $\\boldsymbol{q}^{(k)}$ will approximately equal the dominant (normalized) eigenvector $\\boldsymbol{v}_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "- Note that we write a function `mag()` to compute the magnitude, or the norm, of a vector. Here we used `np.sum(xs*xs)`. We can equivalently use `xs@xs`.\n",
    "- The function `power()` takes in a matrix and an optional parameter of how many times to iterate. \n",
    "- We start out with $\\boldsymbol{q}^{(0)}$, a unit-norm vector. \n",
    "- We then multiply $\\boldsymbol{A}$ with the unit-norm vector, to produce a non-unit-norm vector $\\boldsymbol{z}^{(k)}$. \n",
    "- We proceed to normalize $\\boldsymbol{z}^{(k)}$ to get $\\boldsymbol{q}^{(k)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.44421209 0.48212489 0.51726163 0.55015599]\n",
      "2 [0.4443962  0.4821814  0.5172089  0.55000734]\n",
      "3 [0.44439562 0.48218122 0.51720907 0.55000781]\n",
      "4 [0.44439562 0.48218122 0.51720906 0.55000781]\n",
      "5 [0.44439562 0.48218122 0.51720906 0.55000781]\n",
      " \n",
      "21.316662663452007\n",
      "[0.44439562 0.48218122 0.51720906 0.55000781]\n",
      " \n",
      "21.316662663452043\n",
      "[0.44439562 0.48218122 0.51720906 0.55000781]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def testcreate(n,val):\n",
    "    A = np.arange(val,val+n*n).reshape(n,n)\n",
    "    A = np.sqrt(A)\n",
    "    bs = (A[0,:])**2.1\n",
    "    return A, bs\n",
    "\n",
    "def mag(xs):\n",
    "    return np.sqrt(np.sum(xs*xs))  \n",
    "\n",
    "def power(A,kmax=6):\n",
    "    zs = np.ones(A.shape[0])\n",
    "    qs = zs/mag(zs)\n",
    "    for k in range(1,kmax):\n",
    "        zs = A@qs\n",
    "        qs = zs/mag(zs)\n",
    "        print(k,qs)\n",
    "\n",
    "    lam = qs@A@qs\n",
    "    return lam, qs\n",
    "\n",
    "def testeigone(f,A,indx=0):\n",
    "    eigval, eigvec = f(A)\n",
    "    print(\" \"); print(eigval); print(eigvec)\n",
    "    npeigvals, npeigvecs = np.linalg.eig(A)\n",
    "    print(\" \")\n",
    "    print(npeigvals[indx]); print(npeigvecs[:,indx])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    A, bs = testcreate(4,21)\n",
    "    testeigone(power,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above code we stopped the iteration after 6 iterations. We could have also introduced a termination criterion, \n",
    "$$\n",
    "\\sum_{j=0}^{n-1} \\left| \\frac{q_j^{(k)} - q_j^{(k-1)}}{q_j^{(k)}} \\right| \\leq \\epsilon,\n",
    "$$\n",
    "which you will implement in your homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Count\n",
    "\n",
    "- For iterative methods, the total operation count depends on the actual number of iterations required, which we generally cannot predict ahead of time. \n",
    "- Any operation count we encounter will also have to be multiplied by $m$, where $$ is the number of actual iterations needed.\n",
    "- The bulk of the work is carried out by $\\boldsymbol{z}^{(k)} = \\boldsymbol{A}\\boldsymbol{q}^{(k-1)}$, which is a matrix-vector multiplication, which costs $\\sim 2n^2$ operations. \n",
    "- We also have to evaluate the norm of $\\boldsymbol{z}^{(k)}$, which is a vector-vector multiplication and costs $\\sim 2n$. \n",
    "- Finally, we need to calculate the Rayleigh quotient, which consists of a single matrix-vector multiplication $\\sim 2n^2$ and a vector-vector multiplication $\\sim 2n$,\n",
    "- Adding up the highest power, we have $\\sim 2(m+1)n^2$ for the total cost.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse-Power Method with Shifting\n",
    "We will now discuss a variation of the power method, which will also allow us to evaluate one eigenvalue and eigenvector pair. This time, it will be for the eigenvalue with the smallest magnitude (in absolute value).\n",
    "\n",
    "## Algorithm\n",
    "We start from definition $\\boldsymbol{A}\\boldsymbol{v}_i\\boldsymbol{\\lambda}_i \\boldsymbol{v}_i$. Multiplying on the left with $\\boldsymbol{A}^{-1}$, we find $\\boldsymbol{v}_i = \\lambda_i\\boldsymbol{A}^{-1}\\boldsymbol{v}_i$. Dividing both sides of this equation with $\\lambda_i$, we get\n",
    "$$\n",
    "\\boldsymbol{A}^{-1}\\boldsymbol{v}_i = \\lambda_i^{inv}\\boldsymbol{v}_i\n",
    "$$\n",
    "where $\\lambda_i^{inv}$ is an eigenvalue of the inverse matrix, with $\\lambda_i^{inv} = 1/\\lambda_i$. Now, we can apply the power method\n",
    "$$\n",
    "\\boldsymbol{z}^{(k)} = \\boldsymbol{A}^{-1}\\boldsymbol{q}^{(k-1)}.\n",
    "$$\n",
    "Since the power method determines the largest eigenvalue of $\\boldsymbol{A}^{-1}$, our method then allow us to evaluate the smallest eigenvalue $\\lambda_{n-1}$ of $\\boldsymbol{A}$. Note that $\\boldsymbol{q}^{(k)}$ is the estimate of the eigenvector of $\\boldsymbol{A}^{-1}$, it is an eigenvector of $\\boldsymbol{A}$ as well. \n",
    "For the eigenvalue estimate, we can evaluate the Rayleigh quotient. \n",
    "\n",
    "In practice, one can avoid the costly evaluation of the matrix inverse. Instead, at each iteration, we solve for $\\boldsymbol{z}^{(k)}$\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{z}^{(k)} = \\boldsymbol{q}^{(k-1)}. \n",
    "$$\n",
    "\n",
    "At every step we solve the same system with fixed $\\boldsymbol{A}$, but different right-hand sides. We can thus perform (just once) an LU decomposition, $\\boldsymbol{A} = \\boldsymbol{L} \\boldsymbol{U}$. Then in each iteration we simply need to calculate $\\boldsymbol{z}^{(k)}$ by forward substitution and then backward substitution. \n",
    "\n",
    "## Eigenvalue Shifting\n",
    "Before implement the inverse-power method, we want to further refine it. Let's start with $\\boldsymbol{A}\\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i$. We can subtract from both sides $s\\boldsymbol{v}_i$, with a scaler $s$. We then obtain\n",
    "$$\n",
    "(\\boldsymbol{A} - s\\boldsymbol{I})\\boldsymbol{v}_i = (\\lambda_i - s) \\boldsymbol{v}_i, \n",
    "$$\n",
    "which gives a new eigenvalue problem \n",
    "$$\n",
    "\\boldsymbol{A}^{*}\\boldsymbol{v}_i = \\lambda_i^* \\boldsymbol{v}_i \n",
    "$$\n",
    "with $\\boldsymbol{A}^{*} = \\boldsymbol{A} - s\\boldsymbol{I}$ and $\\lambda_i^* = \\lambda_i - s$. We can apply the inverse-power method for $\\boldsymbol{A}^*$, which solves the smallest eigenvalue $\\lambda_i^*$. The latter actually corresponds to the eigenvalue $\\lambda_i$ of the original matrix $\\boldsymbol{A}$ which is closest to $s$.\n",
    "\n",
    "To get the estimate of the eigenvalue $\\lambda_i$ of $\\boldsymbol{A}$ (of $\\boldsymbol{A}^*$ as well), we could directly use $\\mu(\\boldsymbol{q}^{(k)}) = [\\boldsymbol{q}^{(k)}]^T\\boldsymbol{A}\\boldsymbol{q}^{(k)}$.\n",
    "\n",
    "Another application, would be to find out the eigenvector if we already know an estimate the a particular eigenvalue of $\\boldsymbol{A}$. We can simply choose the shift from this estimate, and calculate $\\boldsymbol{q}^{(k)}$, which will converge to the corresponding eigenvector. \n",
    "\n",
    "One last comment: the convergence of the power method is determined by the ratio $|\\lambda_1/\\lambda_0|$. On the other hand, the unshifted inverse-power method's convergence is converged by $|\\lambda_{n-1}/\\lambda_{n-2}|$. If $|\\lambda_{n-1}|$ is much smaller than $|\\lambda_{n-2}|$, then the convergence is fast. \n",
    "Thus, we can choose a shift value $s$ very close to $\\lambda_{n-1}$, such that $|\\lambda_{n-1}^*|\\ll |\\lambda_{n-2}^*|$, in order to _accelarate convergence._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We implement the inverse-power method in the following code. Comparing our new function to `power()`, we notice three main differences.\n",
    "\n",
    "1. At the start we shift our original matrix to produce $\\boldsymbol{A}^∗ = \\boldsymbol{A} − s\\boldsymbol{I}$. At the end of the process, we evaluate the Rayleigh quotient for the original matrix $\\boldsymbol{A}$, which allows us to evaluate the eigenvalue $\\lambda_i$ of the original matrix that is closest to the hand-picked shift $s$.\n",
    "2. We implement a self-stopping criterion with the `if...break`. We have been careful to create a new copy of our vector each time through the loop, via `qs = np.copy(qnews)`. We also check for sign flips and adjust it accordingly, otherwise the iteration will not converge. \n",
    "3. `power()` employ `@` in the bulk of the code. Here, we have to solve a linear system of equations: by first LU-decomposing $\\boldsymbol{A^*}$ (only once, outside the loop.) Then, inside the loop we only use the forward and backward substitutions, which are considerably less costly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.44065813 0.48090955 0.51822387 0.55316403] 0.30563801879330116\n",
      "2 [0.44464049 0.48226426 0.51714199 0.54980012] 0.019975893536692543\n",
      "3 [0.44437955 0.48217577 0.51721346 0.55002143] 0.0013112849763802009\n",
      "4 [0.44439667 0.48218158 0.51720878 0.55000691] 8.603442270303673e-05\n",
      "5 [0.44439555 0.4821812  0.51720908 0.55000787] 5.6449639949461415e-06\n",
      "6 [0.44439562 0.48218122 0.51720906 0.5500078 ] 3.7038142862653805e-07\n",
      "7 [0.44439562 0.48218122 0.51720906 0.55000781] 2.430173570284188e-08\n",
      "8 [0.44439562 0.48218122 0.51720906 0.55000781] 1.5945032439464706e-09\n",
      " \n",
      "21.316662663486134\n",
      "[0.44439562 0.48218122 0.51720906 0.55000781]\n",
      " \n",
      "21.316662663452043\n",
      "[0.44439562 0.48218122 0.51720906 0.55000781]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def testcreate(n,val):\n",
    "    A = np.arange(val,val+n*n).reshape(n,n)\n",
    "    A = np.sqrt(A)\n",
    "    bs = (A[0,:])**2.1\n",
    "    return A, bs\n",
    "\n",
    "def forsub(L,bs):\n",
    "    n = bs.size\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        xs[i] = (bs[i] - L[i,:i]@xs[:i])/L[i,i]\n",
    "    return xs\n",
    "\n",
    "def backsub(U,bs):\n",
    "    n = bs.size\n",
    "    xs = np.zeros(n)\n",
    "    for i in reversed(range(n)):\n",
    "        xs[i] = (bs[i] - U[i,i+1:]@xs[i+1:])/U[i,i]\n",
    "    return xs\n",
    "\n",
    "def ludec(A):\n",
    "    n = A.shape[0]\n",
    "    U = np.copy(A)\n",
    "    L = np.identity(n)\n",
    "\n",
    "    for j in range(n-1):\n",
    "        for i in range(j+1,n):\n",
    "            coeff = U[i,j]/U[j,j]\n",
    "            U[i,j:] -= coeff*U[j,j:]\n",
    "            L[i,j] = coeff\n",
    "    return L, U\n",
    "\n",
    "\n",
    "def termcrit(xolds,xnews):\n",
    "    errs = np.abs((xnews - xolds)/xnews)\n",
    "    return np.sum(errs)\n",
    "\n",
    "def mag(xs):\n",
    "    return np.sqrt(np.sum(xs*xs))  \n",
    "\n",
    "def testeigone(f,A,indx=0):\n",
    "    eigval, eigvec = f(A)\n",
    "    print(\" \"); print(eigval); print(eigvec)\n",
    "    npeigvals, npeigvecs = np.linalg.eig(A)\n",
    "    print(\" \")\n",
    "    print(npeigvals[indx]); print(npeigvecs[:,indx])\n",
    "\n",
    "#def invpowershift(A,shift=20,kmax=200,tol=1.e-2):\n",
    "def invpowershift(A,shift=20,kmax=200,tol=1.e-8):\n",
    "    n = A.shape[0]\n",
    "    znews = np.ones(n)\n",
    "    qnews = znews/mag(znews)\n",
    "    Astar = A - np.identity(n)*shift\n",
    "    L, U = ludec(Astar)\n",
    "\n",
    "    for k in range(1,kmax):\n",
    "        qs = np.copy(qnews)\n",
    "        ys = forsub(L,qs)\n",
    "        znews = backsub(U,ys)\n",
    "        qnews = znews/mag(znews)\n",
    "\n",
    "        if qs@qnews<0:\n",
    "            qnews = -qnews\n",
    "\n",
    "        err = termcrit(qs,qnews)\n",
    "        print(k, qnews, err)\n",
    "\n",
    "        if err < tol:\n",
    "            lam = qnews@A@qnews\n",
    "            break\n",
    "    else:\n",
    "        lam = qnews = None\n",
    "\n",
    "    return lam, qnews\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    A, bs = testcreate(4,21)\n",
    "    testeigone(invpowershift,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Count\n",
    "- LU decomposition $\\sim 2n^3/3$ operations, this is only once\n",
    "- The following are done for each loop:\n",
    "  - Forward substitution and backward substitution costs both $\\sim n^2$\n",
    "  - Norm evaluation costs $\\sim 2n$\n",
    "  - The total cost $\\sim 2mn^2$, where $m$ is the number of loops\n",
    "- Without the LU decomposition overhead, the cost of the inverse-power method scales similarly to the one for the direct power method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Method\n",
    "The (direct or inverse) power method that we've discussed so far gives us only one eigenvalue at a time (either the largest or the smallest). As we saw, you could combine the latter method with eigenvalue shifting and then try to step through all the eigenvalues of your matrix. In the present section, we will discuss a robust and scalable method used to evaluate all the eigenvalues of a matrix at one go. The name _QR method_ originates from the _QR decomposition (or factorization)_. \n",
    "\n",
    "We will also make a slight detour into similarity transformations and the related approach known as \"simultaneous iteration\". We'll try to keep the terminology straight: we use the QR _decomposition_ in order to express a matrix as the product of two other matrices, while we use the QR _method_ in order to evaluate all eigenvalues of a matrix。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR decomposition\n",
    "The QR decomposition starts with a matrix $\\boldsymbol{A}$ and decomposes it into the product of an orthogonal matrix $\\boldsymbol{Q}$ and an upper-triangular matrix $\\boldsymbol{R}$. (This upper triangular matrix is called $\\boldsymbol{R}$ and not $\\boldsymbol{U}$ for historical reasons.) Symbolically, we say that any real square matrix can be factorized as\n",
    "$$\n",
    "\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{R}.\n",
    "$$\n",
    "\n",
    "Let me remind you that a matrix is called _orthogonal_ if the transpose is equal to the inverse $\\boldsymbol{Q}^{-1} = \\boldsymbol{Q}^T$, or $\\boldsymbol{Q}\\boldsymbol{Q}^T = \\boldsymbol{I}$. In the following, let us derive the QR decomposition. \n",
    "\n",
    "### Evaluating $\\boldsymbol{Q}$\n",
    "We start by constructing the orthogonal matrix $\\boldsymbol{Q}$. We will employ an old method which you may have encountered in a course on linear algebra: _Gram-Schmidt orthogonaliztion_. Let us write our starting matrix $\\boldsymbol{A}$ in terms of its columns $\\boldsymbol{a}_j$:\n",
    "$$\n",
    "\\boldsymbol{A} = \\left(\\boldsymbol{a}_0 \\quad \\boldsymbol{a}_1 \\quad \\dots \\quad \\boldsymbol{a}_{n-1} \\right).\n",
    "$$ \n",
    "\n",
    "Our task now is to start from assuming that the column vectors $\\boldsymbol{a}_j$ are linearly independent and try to produce an orthonormal set of column vectors $\\boldsymbol{q}_j$. When we've accomplished that task, we will have already produced our orthogonal matrix $\\boldsymbol{Q}$: \n",
    "$$\n",
    "\\boldsymbol{Q} = \\left(\\boldsymbol{q}_0 \\quad \\boldsymbol{q}_1 \\quad \\dots \\quad \\boldsymbol{q}_{n-1} \\right).\n",
    "$$\n",
    "since $\\boldsymbol{Q}$ will be made up of the orthonormal column vectors $\\boldsymbol{q}_j$ we just constructed.  \n",
    "\n",
    "We will build these orthonormal $\\boldsymbol{q}_j$ column vectors one at a time. \n",
    "\n",
    "- The first vector $\\boldsymbol{q}_0$ is very easy to produce: $\\boldsymbol{q}_0 = \\boldsymbol{a}_0 / \\| \\boldsymbol{a}_0 \\|$.\n",
    "- To construct $\\boldsymbol{q}_1$, which should be orthogonal to $\\boldsymbol{q}_0$ (or $\\boldsymbol{a}_0$), we take the Gram-Schmidt prescription: take the second vector $\\boldsymbol{a}_1$ and subtract out its component in the direction of $\\boldsymbol{q}_0$:\n",
    " $$.\n",
    " \\boldsymbol{a}_1' = \\boldsymbol{a}_1 - (\\boldsymbol{q}_0^T\\boldsymbol{a}_1)\\boldsymbol{q}_0.\n",
    " $$\n",
    " This is the part of $\\boldsymbol{a}_1$ that does not point in the direction of $\\boldsymbol{q}_0$. Then we simply perform the normalization: $\\boldsymbol{q}_1 = \\boldsymbol{a}_1'/\\|\\boldsymbol{a}_1' \\|$. \n",
    "- Following this procedure, we construct the next vector $\\boldsymbol{q}_2$, by first constructing\n",
    "  $$\n",
    "  \\boldsymbol{a}_2' = \\boldsymbol{a}_2 - (\\boldsymbol{q}_0^T \\boldsymbol{a}_2)\\boldsymbol{q}_0 - (\\boldsymbol{q}_1^T\\boldsymbol{a}_2)\\boldsymbol{q}_1,\n",
    "  $$\n",
    "  and then normalizing $\\boldsymbol{q}_2 = \\boldsymbol{a}_2'/\\|\\boldsymbol{a}_2' \\|$.\n",
    "- Generally, we have for $j = 0, 1, \\dots, n-1$\n",
    "  $$\n",
    "  \\begin{gather*}\n",
    "  \\boldsymbol{a}_j' = \\boldsymbol{a}_j - \\sum_{i = 0}^{j-1} (\\boldsymbol{q}_i^T a_j)\\boldsymbol{q}_i \\\\\n",
    "  \\boldsymbol{q}_j = \\boldsymbol{a}_j'/\\|\\boldsymbol{a}_j' \\|.\n",
    "  \\end{gather*}\n",
    "  $${#eq-evaluating-Q}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating $\\boldsymbol{R}$\n",
    "We now turn to the matrix $\\boldsymbol{R}$. Let us assume $\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{R}$ holds, and try to determine $\\boldsymbol{R}$. We rewritethis as\n",
    "$$\n",
    "\\left(\\boldsymbol{a}_0 \\quad \\boldsymbol{a}_1 \\quad \\dots \\quad \\boldsymbol{a}_{n-1} \\right) = \n",
    "\\left(\\boldsymbol{q}_0 \\quad \\boldsymbol{q}_1 \\quad \\dots \\quad \\boldsymbol{q}_{n-1} \\right)\n",
    "\\begin{pmatrix}\n",
    "R_{00} & R_{01} & R_{02} &\\cdots & R_{0,n-1} \\\\\n",
    "0 & R_{11} & R_{12} & \\cdots & R_{1,n-1} \\\\\n",
    "0 & 0 & R_{22} & \\cdots & R_{2,n-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & R_{n-1, n-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "We can explicitly carry out the multiplication, and find\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 &= R_{00}\\boldsymbol{q}_0 \\\\\n",
    "\\boldsymbol{a}_1 &= R_{01}\\boldsymbol{q}_0 + R_{11}\\boldsymbol{q}_1 \\\\\n",
    "\\boldsymbol{a}_2 &= R_{02}\\boldsymbol{q}_0 + R_{12}\\boldsymbol{q}_1 + \\boldsymbol{R}_{22}\\boldsymbol{q}_2 \\\\\n",
    "& \\vdots \\\\\n",
    "\\boldsymbol{a}_j &= R_{0j}\\boldsymbol{q}_0 + R_{1j}\\boldsymbol{q}_1 +\\cdots +  \\boldsymbol{R}_{jj}\\boldsymbol{q}_j \\\\\n",
    "& \\vdots \\\\\n",
    "\\boldsymbol{a}_{n-1} &= R_{0,n-1}\\boldsymbol{q}_0 + R_{1,n-1}\\boldsymbol{q}_1 +\\cdots +  \\boldsymbol{R}_{n-1,n-1}\\boldsymbol{q}_{n-1} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "These equations can be solved:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\boldsymbol{q}_0 = \\boldsymbol{a}_0/R_{00}, \\quad \\boldsymbol{q}_1 = \\frac{\\boldsymbol{a}_1 - R_{01}\\boldsymbol{q}_0}{R_{11}}, \n",
    "\\quad \\boldsymbol{q}_2 = \\frac{\\boldsymbol{a}_2 - R_{02}\\boldsymbol{q}_0 - R_{12}\\boldsymbol{q}_1}{R_{22}}, \\dots \\\\ \n",
    "&\\boldsymbol{q}_j = \\frac{\\boldsymbol{a}_j - \\sum_{i = 0}^{j-1}R_{ij}\\boldsymbol{q}_i}{R_{jj}}, \\quad \\dots, \\quad\n",
    "\\boldsymbol{q}_{n-1} = \\frac{\\boldsymbol{a}_{n-1} - \\sum_{i = 0}^{n-2} R_{i,n-1}\\boldsymbol{q}_i}{R_{n-1,n-1}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Compare with @eq-evaluating-Q, we have \n",
    "$$\n",
    "\\boldsymbol{R}_{ij} = \\boldsymbol{q}_i^T \\boldsymbol{a}_j, \\quad j = 0,1,\\dots,n-1, \\quad i = 0,1,\\dots, j-1 \\\\\n",
    "\\boldsymbol{R}_{jj} = \\| \\boldsymbol{a}_j' \\| = \\|\\boldsymbol{a}_j - \\sum_{i = 0}^{j-1} R_{ij}\\boldsymbol{q}_i \\|, \\quad j = 0,1,\\dots,n-1\n",
    "$$\n",
    "\n",
    "Note that here we choose $R_{jj}>0$, which makes the QR decomposition _uniquely determined_.\n",
    "\n",
    "Crucially, both $R_{ij}$ and $R_{jj}$ are quantities that we have already evaluated in the process of constructing the matrix $\\boldsymbol{Q}$. That means we can carry out those computations in parallel, building up the matrices $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Decomposition: Implementation\n",
    "The QR decomposition is implemented in the following code.\n",
    "\n",
    "In order to check the performance of our algorithm, we computed two quantities. \n",
    "- The error in QR decomposition $\\|\\boldsymbol{A} - \\boldsymbol{Q}\\boldsymbol{R}\\| $\n",
    "- The orthogonality $\\|\\boldsymbol{Q}^T\\boldsymbol{Q} - \\boldsymbol{I}\\|$\n",
    "\n",
    "We have tried on $4\\times 4$, $6\\times 6$ and $8 \\times 8$ matrices. We found that the error in QR decomposition is small. However, the orthogonality is very poor. This is due to the _classical Gram-Schmidt_ procedure behaves poorly in the presence of roundoff errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.0 0.007574779389820895\n",
      "6 2.3498992183808826e-15 2.446997224625433\n",
      "8 4.5288390936029406e-15 3.508585247394872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def qrdec(A):\n",
    "    n = A.shape[0]\n",
    "    Ap = np.copy(A)\n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j):\n",
    "            R[i,j] = Q[:,i]@A[:,j]\n",
    "            Ap[:,j] -= R[i,j]*Q[:,i]\n",
    "\n",
    "        R[j,j] = mag(Ap[:,j])\n",
    "        Q[:,j] = Ap[:,j]/R[j,j]\n",
    "    return Q, R\n",
    "\n",
    "def testcreate(n,val):\n",
    "    A = np.arange(val,val+n*n).reshape(n,n)\n",
    "    A = np.sqrt(A)\n",
    "    bs = (A[0,:])**2.1\n",
    "    return A, bs\n",
    "\n",
    "def mag(xs):\n",
    "    return np.sqrt(np.sum(xs*xs))  \n",
    "    \n",
    "def testqrdec(A):\n",
    "    n = A.shape[0]\n",
    "    Q, R = qrdec(A)\n",
    "    diffa = A - Q@R\n",
    "    diffq = np.transpose(Q)@Q - np.identity(n) \n",
    "    print(n, mag(diffa), mag(diffq))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for n in range(4,10,2):\n",
    "        A, bs = testcreate(n,21)\n",
    "        testqrdec(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity transformations\n",
    "\n",
    "We now make a quick detour to introduce another related concept. Recall that if we diagonalize a matrix $\\boldsymbol{A}$, we manage to find the matrices $\\boldsymbol{V}$ and $\\boldsymbol{\\Lambda}$ such that\n",
    "$$\n",
    "\\boldsymbol{V}^{-1}\\boldsymbol{A}\\boldsymbol{V} = \\boldsymbol{\\Lambda}\n",
    "$$\n",
    "where $\\boldsymbol{\\Lambda}$ contains the eigenvalues of $\\boldsymbol{A}$ and $\\boldsymbol{V}$ is made up of the eigenvectors of $\\boldsymbol{A}$.\n",
    "\n",
    "Assume there exists another (non-singular) matrix, $\\boldsymbol{S}$, such that:\n",
    "$$\n",
    "\\boldsymbol{A}' = \\boldsymbol{S}^{-1} \\boldsymbol{A} \\boldsymbol{S}. \n",
    "$$\n",
    "This is known as a _similarity transformation_ and we say $\\boldsymbol{A}$ and $\\boldsymbol{A}'$ are similar. \n",
    "\n",
    "Now, if $\\boldsymbol{A}\\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i$, we have\n",
    "$$\n",
    "\\boldsymbol{S}\\boldsymbol{A}'\\boldsymbol{S}^{-1} \\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i.\n",
    "$$\n",
    "If you multiply on the left with $\\boldsymbol{S}^{-1}$ you get\n",
    "$$\n",
    "\\boldsymbol{A}' \\boldsymbol{S}^{-1} \\boldsymbol{v}_i = \\lambda_i \\boldsymbol{S}^{-1}\\boldsymbol{v}_i. \n",
    "$$\n",
    "If you define $\\boldsymbol{v}_i' \\equiv \\boldsymbol{S}^{-1}\\boldsymbol{v}_i$, we have $\\boldsymbol{A}'\\boldsymbol{v}_i' = \\lambda_i \\boldsymbol{v}_i'$. \n",
    "\n",
    "**We find that similar matrices have the same eigenvalues!** The eigenvectors are then related by similarity transformations. \n",
    "\n",
    "As a special case, if $\\boldsymbol{S}$ is orthogonal (or unitary), we then say $\\boldsymbol{A}$ and $\\boldsymbol{A}'$ are _orthogonally (or unitarily) similar_. Let us stay with real matrices, so the orthogonality gives\n",
    "$$\n",
    "\\boldsymbol{A}' = \\boldsymbol{Q}^T \\boldsymbol{A} \\boldsymbol{Q}. \n",
    "$$\n",
    "\n",
    "Since similarity transformation has the property that preserves the eigenvalues, we can then read off the eigenvalues along the diagonal of $\\boldsymbol{A}'$ (or $\\boldsymbol{A}$) if we can find a $\\boldsymbol{Q}$ such that $\\boldsymbol{A}'$ is triangular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous Iteration: First Attempt\n",
    "The method of simultaneous iteration is a generalization of the power method to more than one eigenvectors. We assume that our eigenvalues are distinct so that they can be sorted. For the power method, we started with a vector $\\boldsymbol{z}^{(0)}$ and then multiplied with $\\boldsymbol{A}$ repeatedly. If we expand $\\boldsymbol{z}^{(0)}$ by the eigenvectors, then we have\n",
    "$$\n",
    "\\boldsymbol{z}^{(k)} = \\sum_{i = 0}^{n-1} c_i \\lambda_i^k \\boldsymbol{v}_i. \n",
    "$$\n",
    "\n",
    "We can generalize this approach to the case of more eigenvectors. Let us construct the initial guess\n",
    "$$\n",
    "\\boldsymbol{Z}^{(0)} = \\left(\\boldsymbol{z}_{0}^{(0)} \\quad \\boldsymbol{z}_{1}^{(0)} \\quad \\cdots \\quad \\boldsymbol{z}_{n-1}^{(0)} \\right).\n",
    "$$\n",
    "For example, one can take $\\boldsymbol{Z}^{0} = \\boldsymbol{I}$. We can compute\n",
    "$$\n",
    "\\boldsymbol{Z}^{(k)} = \\boldsymbol{A}^{k}\\boldsymbol{Z}^{(0)} \n",
    "= \\left(\\boldsymbol{z}_0^{(k)} \\quad \\boldsymbol{z}_1^{(k)} \\quad \\boldsymbol{z}_2^{(k)} \\quad \\cdots \\quad \\boldsymbol{z}_{n-1}^{(k)} \\right).\n",
    "$$\n",
    "\n",
    "If we simply makes $k$ larger and larger, all $\\boldsymbol{z}_{j}^{(k)}$ will converge to $\\boldsymbol{v}_0$.  This is disappointing, even if we normalize these vectors $\\boldsymbol{z}_j^{(k)}$ at each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous Iteration: Orthonormalizing\n",
    "Upon closer inspection, we realize what's going on: since we are now dealing with more than one eigenvector, normalizing columns is not enough: what we need to do, instead, is to ensure that the dependence of one column on any of the other columns is projected out. That is, in addition to normalizing, we also need to _orthogonalize_. We can ensure that we are dealing with orthonormal vectors by carrying out a QR decomposition at each step.\n",
    "\n",
    "We'll proceed to give the prescription for the simultaneous iteration method (also known as orthogonal iteration) and later explore some of its fascinating properties. Let us consider\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\boldsymbol{Z}^{(k)} = \\boldsymbol{A} \\boldsymbol{Q}^{(k-1)} \\\\\n",
    "\\boldsymbol{Z}^{(k)} = \\boldsymbol{Q}^{(k)} \\boldsymbol{R}^{(k)},\n",
    "\\end{gather*}\n",
    "$$\n",
    "where we start with $\\boldsymbol{Q}^{(0)} = \\boldsymbol{I}$. \n",
    "\n",
    "If we multiply from right $\\boldsymbol{R}^{(k-1)}$ on both sides in the first of the above equations, we get\n",
    "$$\n",
    "\\boldsymbol{Z}^{(k)}\\boldsymbol{R}^{(k-1)} = \\boldsymbol{A}\\boldsymbol{Q}^{(k-1)}\\boldsymbol{R}^{(k-1)} = \\boldsymbol{A}\\boldsymbol{Z}^{(k-1)}.\n",
    "$$\n",
    "Multiplying on the right $\\boldsymbol{R}^{(k-2)}$ on both sides, we then obtain\n",
    "$$\n",
    "\\boldsymbol{Z}^{(k)}\\boldsymbol{R}^{(k-1)}\\boldsymbol{R}^{(k-2)} = \\boldsymbol{A}\\boldsymbol{Z}^{(k-1)}\\boldsymbol{R}^{(k-2)} = \\boldsymbol{A}^2\\boldsymbol{Z}^{(k-2)}.\n",
    "$$\n",
    "\n",
    "We can continue this procedure, and finally arrive at\n",
    "$$\n",
    "\\boldsymbol{Z}^{(k)}\\boldsymbol{R}^{(k-1)}\\boldsymbol{R}^{(k-2)}\\cdots\\boldsymbol{R}^{(1)}  = \\boldsymbol{A}^{k-1}\\boldsymbol{Z}^{(1)}.\n",
    "$$\n",
    "\n",
    "On the other hand, we have\n",
    "$$\n",
    "\\boldsymbol{Z}^{(1)} = \\boldsymbol{A}\\boldsymbol{Q}^{(0)} = \\boldsymbol{A},\n",
    "$$\n",
    "which combined with the previous equation gives\n",
    "$$\n",
    "\\boldsymbol{Z}^{(k)}\\boldsymbol{R}^{(k-1)}\\boldsymbol{R}^{(k-2)}\\cdots\\boldsymbol{R}^{(1)} = \\boldsymbol{A}^k.\n",
    "$$\n",
    "Note that $\\boldsymbol{Z}^{(k)} = \\boldsymbol{Q}^{(k)}\\boldsymbol{R}^{(k)}$, we can rewrite the left-hand side of the above equation and obtain\n",
    "$$\n",
    "\\boldsymbol{A}^k = \\boldsymbol{Q}^{(k)} \\boldsymbol{R}^{(k)}\\boldsymbol{R}^{(k-1)}\\cdots\\boldsymbol{R}^{(1)} = \\boldsymbol{Q}^{(k)}\\mathcal{R}^{(k)},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathcal{R}^{(k)} = \\prod_{j=k}^{1}\\boldsymbol{R}^{(j)}\n",
    "$$\n",
    "is also upper triangular, since it is a product of upper triangular matrices. \n",
    "\n",
    "Thus, this allow us to construct orthonormal bases for the $k$-th power of $\\boldsymbol{A}$. For symmetric $\\boldsymbol{A}$, $\\boldsymbol{Q}^{(k)}$ converges to the eigenvectors. For a nonsymmetric $\\boldsymbol{A}$, $\\boldsymbol{Q}^{(k)}$ converges toward the orthogonal \"factor\" of the eigenvector matrix. In either case, $\\boldsymbol{Q}^{(k)}$ is related to the eigenvectors of the matrix $\\boldsymbol{A}$.\n",
    "\n",
    "After obtaining $\\boldsymbol{Q}^{(k)}$, we compute \n",
    "$$\n",
    "\\boldsymbol{A}^{(k)} = [\\boldsymbol{Q}^{(k)}]^T\\boldsymbol{A}\\boldsymbol{Q}^{(k)},\n",
    "$$\n",
    "which is a generalization of the Rayleigh quotient. \n",
    "\n",
    "Here $\\boldsymbol{A}^{(k)}$ is orthogonally similar to $\\boldsymbol{A}$ and converges to an upper-triangular matrix for general matrix $\\boldsymbol{A}$ as $k$ grows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1fa0d82bdabb5288f7efc0788d29c4d5bb5f690328690a3d32d2cd65de760c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
