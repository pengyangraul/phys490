{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Matrices I\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "    html-to-math: katex\n",
    "    html-math-method: katex\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Motivation\n",
    "Linear algebra pops up almost everywhere in physics, so the matrix-related techniques developed below will be used repeatedly in later lectures. As a result, we will spend lots of time on matrices. We will take the time to introduce several numerical techniques in detail. \n",
    "\n",
    "## Examples from Physics\n",
    "We discuss some elementary examples from undergraduate physics.\n",
    "\n",
    "### Rotations in two dimensions\n",
    "Consider a two-dimensional Cartesian coordinate system. A point $\\boldsymbol{r} = (x,y)^T$ can be rotated counter-clockwise through an angle $\\theta$ about the origin, producing a new point $\\boldsymbol{r}' = (x',y')^T$. The two points' coordinates are related as follows:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The $2\\times 2$ matrix appearing here is an example of a _rotation matrix_ in Euclidean space. If you know $\\boldsymbol{r}'$ and wish to calculate $\\boldsymbol{r}$, you need to solve this system of two linear equations. \n",
    "\n",
    "### Electrostatic potentials\n",
    "Assume you have $n$ electric charges $q_j$ (which are unknown) held at the positions $\\boldsymbol{R}_j$ (which are known).  Further assume that you have measured the electric potential $\\phi(r_i)$ at the $n$ known positions $\\boldsymbol{r}_i$. From the definition of the potential (as well as the fact that the potential obeys the principle of superposition), we see that:\n",
    "$$\n",
    "\\phi(\\boldsymbol{r}_i) = \\sum_{j=0}^{n-1}\\left(\\frac{k q_j}{|\\boldsymbol{r}_i - \\boldsymbol{R}_j|}\\right),\n",
    "$$\n",
    "where $i = 0,1,\\dots,n-1$.\n",
    "If you assume you have four charges, the above relation turns into the following $4\\times 4$ linear systems of equations:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_3|\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\phi(\\boldsymbol{r}_0) \\\\ \\phi(\\boldsymbol{r}_1) \\\\ \\phi(\\boldsymbol{r}_2) \\\\ \\phi(\\boldsymbol{r}_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "which needs to be solved for the 4 unknowns $q_0$, $q_1$, $q_2$ and $q_3$.\n",
    "\n",
    "### Principle moments of inertia\n",
    "In study of the rotation of a rigid body about an arbitrary axis in three dimensions, you may have encountered the moment of inertia tensor:\n",
    "$$\n",
    "I_{\\alpha \\beta} = \\int \\rho(\\boldsymbol{r}) \\left(\\delta_{\\alpha \\beta}r^2 - \\boldsymbol{r}_\\alpha \\boldsymbol{r}_\\beta\\right)d^3 r,\n",
    "$$\n",
    "where $\\rho(r)$ is the mass density, $\\alpha$ and $\\beta$ denote Cartesian components, and $\\delta_{\\alpha \\beta}$ is the Kronecker delta. \n",
    "\n",
    "The moment of inertia tensor is represented by a $3\\times 3$ matrix: \n",
    "$$\n",
    "\\boldsymbol{I} = \n",
    "\\begin{pmatrix}\n",
    "I_{xx} & I_{xy} & I_{xz} \\\\\n",
    "I_{yx} & I_{yy} & I_{yz} \\\\\n",
    "I_{zx} & I_{zy} & I_{zz}.\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "This is a symmetric matrix. It is possible to choose a coordinate system such that the off-diagonal elements vanish. \n",
    "This axes of this coordinate system are known as the _principal axes_ for the body at the origin. Then the moment of inertian tensor is represented by a diagonal matrix, with diagonal elements $I_0$, $I_1$, and $I_2$, known as the principal moments. This is an instance of the \"eigenvalue problem\".\n",
    "\n",
    "## The problems to be solved\n",
    "First, we look at the problem where we have $n$ unknowns $x_i$, along with $n\\times n$ coefficients $A_{ij}$ and $n$ constants $b_i$:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A_{00} & A_{01} & \\dots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\dots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\dots & A_{n-1,n-1} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_{n-1}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where we used a comma to separate two indices when this was necessary to avoid confusion. \n",
    "These are $n$ equations linear in $n$ unknowns. \n",
    "\n",
    "In compact matrix form, this problem is written as \n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b},\n",
    "$$\n",
    "where $\\boldsymbol{A}$ is called the _coefficient matrix_. This is a problem that we will spend considerable time solving in this lecture. \n",
    "We will be doing this mainly by using the _augmented coefficient matrix_ which places together the elements of $\\boldsymbol{A}$ and $\\boldsymbol{b}$, i.e.:\n",
    "$$\n",
    "(\\boldsymbol{A}|\\boldsymbol{b})= \\left(\n",
    "\\begin{matrix}\n",
    "A_{00} & A_{01} & \\dots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\dots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\dots & A_{n-1,n-1} \n",
    "\\end{matrix}\\right|\n",
    "\\left.\n",
    "\\begin{matrix}\n",
    "b_0 \\\\ b_1 \\\\ \\vdots \\\\b_{n-1}\n",
    "\\end{matrix}\n",
    "\\right).\n",
    "$$\n",
    "For now we assume the determinant of $\\boldsymbol{A}$ satisfy $|\\boldsymbol{A}| \\neq 0$.\n",
    "\n",
    "In a course on linear algebra you have seen examples of legitimate operations one can carry out while solving the system of linear equations. \n",
    "Such operations change the elements of $\\boldsymbol{A}$ and $\\boldsymbol{b}$, but leave the solution vector $\\boldsymbol{x}$ unchanged. \n",
    "More generally, we are allowed to carry the following elementary row operations:\n",
    "\n",
    "- _Scaling_: each row/equation may be multiplied by a constant (multiplies $|\\boldsymbol{A}|$ by the same constant).\n",
    "- _Pivoting_: two rows/equations may be interchanged (changes sign of $|\\boldsymbol{A}|$).\n",
    "- _Elimination_: a row/equation may be replaced by a linear combination of that row/equation with any other row/equation (doesn't change $|\\boldsymbol{A}|$).\n",
    "\n",
    "Keep in mind that these are operations that are carried out on the augmented coefficient matrix $(\\boldsymbol{A}|\\boldsymbol{b})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we wish to tackle the standard form of the matrix eigenvalue problem:\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{v} = \\lambda \\boldsymbol{v}.\n",
    "$${#eq-eigenvalue}\n",
    "Here, both $\\lambda$ and the column vector $\\boldsymbol{v}$ are unknown. This $\\lambda$ is called an _eigenvalue_ and $\\boldsymbol{v}$ is called an _eigenvector_.\n",
    "\n",
    "Let's sketch one possible approach to solve this problem.  We can move everything to the left-hand side, we have\n",
    "$$\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = \\boldsymbol{0},\n",
    "$$\n",
    "where $\\boldsymbol{I}$ is the $n\\times n$ identity matrix and $\\boldsymbol{0}$ is an $n\\times 1$ column vector made up of $0$s. \n",
    "It is easy to see that we are faced with a system of $n$ linear equations: the coefficient matrix here is $A - \\lambda \\boldsymbol{I}$. \n",
    "\n",
    "The trivial solution is $\\boldsymbol{v} = 0$. In order for a non-trivial solution to exist, we must have vanishing determinant $|\\boldsymbol{A} - \\lambda \\boldsymbol{I}| = 0$.\n",
    "In other words, the matrix $\\boldsymbol{A} - \\lambda \\boldsymbol{I}$ is singular. Expanding the determinant gives us a polynomial equation, known as the _characteristic equation_:\n",
    "$$\n",
    "(-1)^n\\lambda^n + c_{n-1} \\lambda^{n-1} + \\cdots + c_1 \\lambda + c_0 = 0.\n",
    "$$\n",
    "\n",
    "Thus, an $n \\times n $ matrix has at most $n$ distinct eigenvalues, which are the roots of the characteristic polynomial. When a root occurs twice, we say that root has multiplicity $2$. If a root occurs only once, in other words if it has multiplicity 1, we are dealing with a _simple_ eigenvalue.\n",
    "\n",
    "Having calculated the eigenvalues, one way to evaluate the eigenvectors is simply by using @eq-eigenvalue again. \n",
    "\n",
    "- Specifically, for a given/known eigenvalue, $\\lambda_i$, one tries to solve the system of linear equations $(\\boldsymbol{A}-\\lambda_i\\boldsymbol{I})\\boldsymbol{v}_i = 0$ for $\\boldsymbol{v}_i$. \n",
    "- For each value $\\lambda_i$, we will not be able to determine unique values of $\\boldsymbol{v}_i$, so we will limit ourselves to computing the relative values of the components of $\\boldsymbol{v}_i$. \n",
    "- We will in the following use the notation $(v_j)_0$, $(v_j)_1$ etc. to denote the $n$ elements of the column vector $\\boldsymbol{v}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "We now turn to a discussion of practical error estimation in work with matrices. we will provide some general derivations and examples of when a problem is \"well-conditioned\", typically by using matrix perturbation theory (i.e., by checking what happens if there are uncertainties in the input data).\n",
    "\n",
    "After some preliminary comments, examples, and definitions, we will investigate quantitatively how linear systems, eigenvalues, and eigenvectors depend on the input data. We will be examining in each case the simplest scenario but, hopefully, this will be enough to help you grasp the big picture. \n",
    "\n",
    "## From _a posteriori_ to _a priori_ Estimates\n",
    "\n",
    "Let us look at a specific $2\\times 2$ linear system, namely $\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b}$ for the case where\n",
    "$$\n",
    "(\\boldsymbol{A}|\\boldsymbol{b}) = \\left(\n",
    "\\begin{matrix}\n",
    "0.2161 & 0.1441 \\\\\n",
    "1.2969 & 0.8648 \n",
    "\\end{matrix}\\ \\right|\n",
    "\\left.\n",
    "\\begin{matrix}\n",
    "0.1440 \\\\\n",
    "0.8642\n",
    "\\end{matrix}\n",
    "\\right).\n",
    "$${#eq-linear_eq}\n",
    "\n",
    "Simply put, there are two options on how to analyze errors: \n",
    "\n",
    "a. an _a priori_ analysis, in which case we try to see how easy/hard the problem is to solve before we begin solving it.\n",
    "b. an _a posteriori_ analysis, where we have produced a solution, and attempt to see how good it is.\n",
    "\n",
    "Let us start with the latter option, an _a posteriori_ approach.  Say you are provided with the following approximate solution to the problem defined in @eq-linear_eq:\n",
    "$$\n",
    "\\tilde{\\boldsymbol{x}}^T = (0.9911 \\quad -0.4870).\n",
    "$${#eq-approximate_sol}\n",
    "\n",
    "One way of testing how good a solution is, is to evaluate the residue vector:\n",
    "$$\n",
    "\\boldsymbol{r} = \\boldsymbol{b} - \\boldsymbol{A} \\tilde{\\boldsymbol{x}}.\n",
    "$$\n",
    "Plugging in @eq-approximate_sol, we find the residue vector\n",
    "$$\n",
    "\\boldsymbol{r}^T = (-10^{-8} \\quad 10^{-8})\n",
    "$$\n",
    "which might naturally lead you to the conclusion that our approximate solution $\\tilde{\\boldsymbol{x}}$ is pretty good!\n",
    "\n",
    "However, here’s the thing: the exact solution to our problem is actually:\n",
    "$$\n",
    "\\boldsymbol{x}^T = (-2 \\quad 2).\n",
    "$$\n",
    "The approximate solution $\\tilde{\\boldsymbol{x}}$ doesn't contain even a single correct significant figure!\n",
    "\n",
    "With the disclaimer that there’s much more that could be said at the a posteriori level, we now drop this line of attack and turn to an _a priori_ analysis: could we have realized that solving the problem in @eq-linear_eq was difficult? How could we know that there's something pathological about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude of Determinant?\n",
    "\n",
    "### Example 1\n",
    "In an attempt to see what is wrong with our previous example in @eq-linear_eq, we start to make small perturbation to the input data. \n",
    "Imagine we didn't know the values of the coefficients in $\\boldsymbol{A}$ all that precisely. Would anything change? \n",
    "\n",
    "Let us take\n",
    "$$\n",
    "\\Delta \\boldsymbol{A} = \n",
    "\\begin{pmatrix}\n",
    "0.0001 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We want to study the effect of this perturbation on the solution, namely\n",
    "$$\n",
    "(\\boldsymbol{A} + \\Delta \\boldsymbol{A})(\\boldsymbol{x} + \\Delta \\boldsymbol{x}) = \\boldsymbol{b},\n",
    "$$\n",
    "where $\\boldsymbol{b}$ is kept fixed/unperturbed. \n",
    "For the specific case studied here, we find (using the following program)\n",
    "$$\n",
    "(\\boldsymbol{x} + \\Delta \\boldsymbol{x})^T  = (-2.31294091\\times 10^{-4} \\quad 0.99653059\\times 10^{-1}).\n",
    "$$\n",
    "\n",
    "We see that this is not a \"small\" effect. Our perturbation amounted to changing only one element of $\\boldsymbol{A}$ by less than $0.1\\%$, and \n",
    "had a dramatic impact on the solution to our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. -2.]\n",
      "[-2.31294091e-04  9.99653059e-01]\n",
      "determinant of A is:  -9.999999998544968e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[0.2161, 0.1441],[1.2969, 0.8648]])\n",
    "deltaA = np.array([[0.0001, 0],[0, 0]])\n",
    "b = np.array([0.1440, 0.8642])\n",
    "# we use np.linalg.solve(A,b) to solve the equation Ax = b.\n",
    "x = np.linalg.solve(A,b)\n",
    "x_dx = np.linalg.solve(A+deltaA,b)\n",
    "\n",
    "print(x)\n",
    "print(x_dx)\n",
    "\n",
    "print(\"determinant of A is: \", np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "Let us look at the following example\n",
    "$$\n",
    "(\\boldsymbol{A} | \\boldsymbol{b}) = \n",
    "\\left(\n",
    "    \\begin{matrix}\n",
    "    1 & 1 \\\\\n",
    "    1 & 1.001\n",
    "    \\end{matrix}\\ \n",
    "\\right|\n",
    "\\left.\n",
    "    \\begin{matrix}\n",
    "    2 \\\\\n",
    "    2.001\n",
    "    \\end{matrix}\n",
    "\\right).\n",
    "$$\n",
    "The exact solution is (from the following program)\n",
    "$$\n",
    "\\boldsymbol{x}^T = (1 \\quad 1).\n",
    "$$\n",
    "We can then add a perturbation\n",
    "$$\n",
    "\\Delta\\boldsymbol{A} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0.001\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Then, the perturbed solution is\n",
    "$$\n",
    "(\\boldsymbol{x} + \\Delta\\boldsymbol{x})^T = (1.5 \\quad 0.5).\n",
    "$$\n",
    "\n",
    "Instead of adding a perturbation to $\\boldsymbol{A}$, one can also add a perturbation to $\\boldsymbol{b}$, with \n",
    "$$\n",
    "\\Delta \\boldsymbol{b}^T = (0 \\quad 0.001).\n",
    "$$\n",
    "We find (in the following program)\n",
    "$$\n",
    "(\\boldsymbol{x} + \\Delta \\boldsymbol{x})^T = (0 \\quad 2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n",
      "[1.5 0.5]\n",
      "[0. 2.]\n",
      "determinant of A is:  0.00099999999999989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 1],[1, 1.001]])\n",
    "deltaA = np.array([[0, 0],[0, 0.001]])\n",
    "b = np.array([2, 2.001])\n",
    "deltab = np.array([0, 0.001])\n",
    "# we use np.linalg.solve(A,b) to solve the equation Ax = b.\n",
    "x = np.linalg.solve(A,b)\n",
    "x_dx = np.linalg.solve(A+deltaA,b)\n",
    "x_dx2 = np.linalg.solve(A,b+deltab)\n",
    "\n",
    "print(x)\n",
    "print(x_dx)\n",
    "print(x_dx2)\n",
    "\n",
    "\n",
    "print(\"determinant of A is: \", np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "There are also cases where small perturbations won't lead to dramatic consequences in the solutions. See the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  4.]\n",
      "[-1.00334448  4.00668896]\n",
      "[-0.99333333  3.99666667]\n",
      "determinant of A is:  2.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# A|b: 2   1 | 2\n",
    "#      1   2 | 7\n",
    "A = np.array([[2, 1],[1, 2]])\n",
    "b = np.array([2,7])\n",
    "# Delta A: 0    0\n",
    "#          0.01 0\n",
    "deltaA = np.array([[0, 0],[0.01, 0]])\n",
    "# Delta b: 0.01\n",
    "#          0\n",
    "deltab = np.array([0.01, 0])\n",
    "# we use np.linalg.solve(A,b) to solve the equation Ax = b.\n",
    "x = np.linalg.solve(A,b)\n",
    "x_dx = np.linalg.solve(A+deltaA,b)\n",
    "x_dx2 = np.linalg.solve(A,b+deltab)\n",
    "\n",
    "print(x)\n",
    "print(x_dx)\n",
    "print(x_dx2)\n",
    "\n",
    "\n",
    "print(\"determinant of A is: \", np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms for Matrices and Vectors\n",
    "### Example 4\n",
    "Consider the following question: what does \"small determinant\" mean? If the definition is \"much less than 1\", then one might counter-argue: what about the following matrix:\n",
    "$$\n",
    "\\boldsymbol{A} = \n",
    "\\begin{pmatrix}\n",
    "0.2 & 0.1 \\\\\n",
    "0.1 & 0.2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which is just the matrix $\\boldsymbol{A}$ in Example 3 multiplied by 0.1. This matrix has a determinant $\\det(\\boldsymbol{A}) = 0.03$, which is certainly much less than $1$.\n",
    "If you also multiply each element of $\\boldsymbol{b}$ in the previsou example by $0.1$, you should get the same answer. What's more, this linear system of equations should be equally sensitive to perturbations. \n",
    "\n",
    "Thus, the value of the determinant should be compared with the magnitude of the relevant matrix elements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and Properties for Matrices\n",
    "Let us provide our intuitions with quantitative backing. \n",
    "We shall introduce the _matrix norm_, which measures the magnitude of $\\boldsymbol{A}$.\n",
    "There are several possible definitions of a norm, but we will employ two possibilities. \n",
    "\n",
    "1. **Euclidean norm**:\n",
    "   $$\n",
    "   \\|\\boldsymbol{A} \\|_E = \\sqrt{\\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1} |A_{ij}|^2},\n",
    "   $$\n",
    "   which is sometimes also called the _Frobenius norm_. \n",
    "2. **Infinity norm**:\n",
    "   $$\n",
    "   \\| \\boldsymbol{A} \\|_{\\infty} = \\max_{0\\leq i \\leq n-1} \\sum_{j=0}^{n-1} |A_{ij}|,\n",
    "   $$\n",
    "   which is also known as the _maximum row-sum norm_.\n",
    "\n",
    "Regardless of the specific norm definitions, all matrix norms for square matrices obey:\n",
    "\n",
    "- $\\|\\boldsymbol{A} \\| \\geq 0$\n",
    "- $\\| \\boldsymbol{A} \\| = 0$ if and only if all $A_{ij} = 0$\n",
    "- $\\| k \\boldsymbol{A} \\| = |k| \\|\\boldsymbol{A}\\|$\n",
    "- $\\| \\boldsymbol{A} + \\boldsymbol{B} \\| \\leq \\| \\boldsymbol{A} \\| + \\| \\boldsymbol{B} \\|$\n",
    "- $\\| \\boldsymbol{A B} \\| \\leq \\|\\boldsymbol{A}\\|\\|\\boldsymbol{B}\\|$\n",
    "\n",
    "Now, we return to the question when the determinant is \"small\". A reasonable definition would be $|\\det(\\boldsymbol{A})| \\ll \\|\\boldsymbol{A}\\|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 : det(A) = -9.999999998544968e-09 , Euclidean norm = 1.5802824652573981\n",
      "Example 2 : det(A) = 0.00099999999999989 , Euclidean norm = 2.000500187453128\n",
      "Example 3 : det(A) = 2.9999999999999996 , Euclidean norm = 3.1622776601683795\n",
      "Example 4 : det(A) = 0.03000000000000001 , Euclidean norm = 0.31622776601683794\n"
     ]
    }
   ],
   "source": [
    "A1 = np.array([[0.2161, 0.1441],[1.2969, 0.8648]])\n",
    "A2 = np.array([[1, 1],[1, 1.001]])\n",
    "A3 = np.array([[2, 1],[1, 2]])\n",
    "A4= np.array([[0.2, 0.1],[0.1, 0.2]])\n",
    "\n",
    "Alist = [A1, A2, A3, A4]\n",
    "\n",
    "for ii,A in enumerate(Alist):\n",
    "    print(\"Example\", ii+1, \": det(A) =\", np.linalg.det(A), \", Euclidean norm =\", np.linalg.norm(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem to be consistent with what we had seen above: \n",
    "\n",
    "- Examples 1 and 2 are near-singular, while Example 3 is not singular. \n",
    "- For Example 4, this criterion claims that our matrix is not quite singular (though it's getting there). \n",
    "\n",
    "Our introduction of the concept of the matrix norm seems to have served its purpose: a small determinant needs to be compared to the matrix norm, so Example 4 (despite having a small determinant) is not singular, given that its matrix elements are small, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions for Vectors\n",
    "Let us also introduce norms for vector norms:\n",
    "$$\n",
    "\\|\\boldsymbol{x}\\|_E = \\sqrt{\\sum_{i = 0}^{n-1} |x_i|^2}, \\quad \\|\\boldsymbol{x} \\|_{\\infty} = \\max_{0 \\leq i \\leq n-1} |x_i|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Number for Linear Systems\n",
    "\n",
    "Unfortunately, our criterion $|\\det(\\boldsymbol{A})|\\ll \\| \\boldsymbol{A} \\|$ is flawed, though this appears in many textbooks. \n",
    "We will look at two examples. \n",
    "\n",
    "### Example 5\n",
    "We shall consider\n",
    "$$\n",
    "\\boldsymbol{A} = \n",
    "\\begin{pmatrix}\n",
    "2 \\times 10^{-10} & 1 \\times 10^{-10} \\\\\n",
    "1 \\times 10^{-10} & 2 \\times 10^{-10}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which is the matrix in Example 3 multiplied by $10^{-10}$. \n",
    "Here, we have $|\\det(\\boldsymbol{A})| = 3 \\times 10^{-20}$, and $\\|\\boldsymbol{A} \\|_E \\simeq 3.16 \\times 10^{-10}$, so \n",
    "$|\\det(\\boldsymbol{A})| \\ll \\|\\boldsymbol{A} \\|_E$ holds.\n",
    "\n",
    "But isn't this strange? Simply multiplying a set of equations with a small number cannot be enough to make the problem near-singular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6\n",
    "Let us look at the following $8 \\times 8$ problem: \n",
    "$$\n",
    "\\boldsymbol{A} = \n",
    "\\begin{pmatrix*}[r]\n",
    "2 & -2 & -2 & \\cdots & -2 \\\\\n",
    "0 & 2 & -2 & \\cdots & -2 \\\\\n",
    "0 & 0 & 2 & \\cdots & -2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 2\n",
    "\\end{pmatrix*}\n",
    "$$\n",
    "The corresponding results are \n",
    "$|\\det(\\boldsymbol{A})| = 256$, and $\\|\\boldsymbol{A} \\|_E = 12$, so $|\\det(\\boldsymbol{A})| \\gg \\|\\boldsymbol{A} \\|_E$ holds.\n",
    "\n",
    "Now, take a look at the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-21.  -11.   -5.   -3.   -1.   -1.    0.   -0.5]\n",
      "[-30.88235294 -15.94117647  -7.47058824  -4.23529412  -1.61764706\n",
      "  -1.30882353  -0.15441176  -0.65441176]\n",
      "determinant of A is:  255.99999999999994\n",
      "norm of A is:  12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[2, -2, -2, -2, -2, -2, -2, -2],\n",
    "              [0,  2, -2, -2, -2, -2, -2, -2],\n",
    "              [0,  0,  2, -2, -2, -2, -2, -2],\n",
    "              [0,  0,  0,  2, -2, -2, -2, -2],\n",
    "              [0,  0,  0,  0,  2, -2, -2, -2],\n",
    "              [0,  0,  0,  0,  0,  2, -2, -2],\n",
    "              [0,  0,  0,  0,  0,  0,  2, -2],\n",
    "              [0,  0,  0,  0,  0,  0,  0,  2]])\n",
    "b = np.array([1, -1, 1, -1, 1, -1, 1, -1])\n",
    "\n",
    "\n",
    "deltaA = np.zeros((8,8))\n",
    "deltaA[-1,0] = -0.01  # bottom-left elememnt is -0.01\n",
    "\n",
    "# Delta b: 0.01\n",
    "#          0\n",
    "deltab = np.array([0.01, 0])\n",
    "# we use np.linalg.solve(A,b) to solve the equation Ax = b.\n",
    "x = np.linalg.solve(A,b)\n",
    "x_dx = np.linalg.solve(A+deltaA,b)\n",
    "\n",
    "print(x)\n",
    "print(x_dx)\n",
    "\n",
    "\n",
    "print(\"determinant of A is: \", np.linalg.det(A))\n",
    "print(\"norm of A is: \", np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation\n",
    "\n",
    "In the present subsection, we will carry out an informal derivation that will point us toward a quantitative measure of ill-conditioning. This measure of the sensitivity of our problem to small changes in its elements will be called the condition number.\n",
    "\n",
    "Let us start with the unperturbed problem\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b}\n",
    "$$\n",
    "and the perturbed one\n",
    "$$\n",
    "(\\boldsymbol{A} + \\Delta \\boldsymbol{A})(\\boldsymbol{x} + \\Delta \\boldsymbol{x}) = \\boldsymbol{b}.\n",
    "$$\n",
    "\n",
    "Combining the above two equations, we have\n",
    "$$\n",
    "\\boldsymbol{A}\\Delta \\boldsymbol{x} = -\\Delta \\boldsymbol{A}(\\boldsymbol{x} + \\Delta \\boldsymbol{x}).\n",
    "$$\n",
    "Assuming $\\boldsymbol{A}$ is nonsingular (so you can invert it), we get\n",
    "$$\n",
    "\\Delta \\boldsymbol{x} = -\\boldsymbol{A}^{-1}\\Delta \\boldsymbol{A} (\\boldsymbol{x} + \\Delta \\boldsymbol{x}).\n",
    "$$\n",
    "Now, we take the norm on both sides, we obtain\n",
    "$$\n",
    "\\| \\Delta \\boldsymbol{x}\\| = \\| \\boldsymbol{A}^{-1}\\Delta \\boldsymbol{A} (\\boldsymbol{x} + \\Delta \\boldsymbol{x})\\|\n",
    "\\leq \\| \\boldsymbol{A}^{-1} \\| \\| \\Delta \\boldsymbol{A} \\| \\| \\boldsymbol{x}+ \\Delta \\boldsymbol{x} \\|.\n",
    "$$\n",
    "\n",
    "This means\n",
    "$$\n",
    "\\frac{\\| \\Delta \\boldsymbol{x} \\|}{\\|\\boldsymbol{x} + \\Delta \\boldsymbol{x} \\|}\n",
    "\\leq \\| \\boldsymbol{A}^{-1} \\| \\|\\Delta \\boldsymbol{A} \\| =\n",
    "\\| \\boldsymbol{A}\\|\\| \\boldsymbol{A}^{-1} \\| \\frac{\\|\\Delta \\boldsymbol{A} \\|}{\\| \\boldsymbol{A}\\|}.\n",
    "$$\n",
    "\n",
    "In other words, if you know an error bound on $\\|\\Delta \\boldsymbol{A} \\|/ \\|\\boldsymbol{A} \\|$\n",
    "then translates to an error bound on $\\|\\Delta \\boldsymbol{x}\\|/\\|\\boldsymbol{x}\\| \\simeq \\|\\Delta \\boldsymbol{x}\\|/\\|\\boldsymbol{x} + \\Delta \\boldsymbol{x}\\|$.\n",
    "\n",
    "This leads to the introduction of the _condition number_:\n",
    "$$\n",
    "\\kappa(\\boldsymbol{A}) = \\| \\boldsymbol{A}\\|\\| \\boldsymbol{A}^{-1} \\|,\n",
    "$$\n",
    "which determines if a small perturbation gets amplified when solving for $\\boldsymbol{x}$ or not. \n",
    "\n",
    "A large condition number leads to an amplification of a small perturbation: we say we are dealing with an _ill-conditioned_ problem. If the condition number is of order unity, then a small perturbation is not amplified, so we are dealing with a _well-conditioned_ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "- Example 1: $\\kappa(\\boldsymbol{A}) = 249729267.388$, ill-conditioned.\n",
    "- Example 2: $\\kappa(\\boldsymbol{A}) = 4002.001$, ill-conditioned.\n",
    "- Example 3: $\\kappa(\\boldsymbol{A}) = 3.33$, well-conditioned\n",
    "- Example 4: $\\kappa(\\boldsymbol{A}) = 3.33$, well-conditioned\n",
    "- Example 5: $\\kappa(\\boldsymbol{A}) = 3.33$, well-conditioned\n",
    "- Example 6: $\\kappa(\\boldsymbol{A}) = 512.18$, ill-conditioned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1fa0d82bdabb5288f7efc0788d29c4d5bb5f690328690a3d32d2cd65de760c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
