{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Matrices I\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Linear algebra pops up almost everywhere in physics, so the matrix-related techniques developed below will be used repeatedly in later lectures. As a result, we will spend lots of time on matrices. We will take the time to introduce several numerical techniques in detail. \n",
    "\n",
    "## Examples from Physics\n",
    "We discuss some elementary examples from undergraduate physics.\n",
    "\n",
    "### Rotations in two dimensions\n",
    "Consider a two-dimensional Cartesian coordinate system. A point $\\boldsymbol{r} = (x,y)^T$ can be rotated counter-clockwise through an angle $\\theta$ about the origin, producing a new point $\\boldsymbol{r}' = (x',y')^T$. The two points' coordinates are related as follows:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The $2\\times 2$ matrix appearing here is an example of a _rotation matrix_ in Euclidean space. If you know $\\boldsymbol{r}'$ and wish to calculate $\\boldsymbol{r}$, you need to solve this system of two linear equations. \n",
    "\n",
    "### Electrostatic potentials\n",
    "Assume you have $n$ electric charges $q_j$ (which are unknown) held at the positions $\\boldsymbol{R}_j$ (which are known).  Further assume that you have measured the electric potential $\\phi(r_i)$ at the $n$ known positions $\\boldsymbol{r}_i$. From the definition of the potential (as well as the fact that the potential obeys the principle of superposition), we see that:\n",
    "$$\n",
    "\\phi(\\boldsymbol{r}_i) = \\sum_{j=0}^{n-1}\\left(\\frac{k q_j}{|\\boldsymbol{r}_i - \\boldsymbol{R}_j|}\\right),\n",
    "$$\n",
    "where $i = 0,1,\\dots,n-1$.\n",
    "If you assume you have four charges, the above relation turns into the following $4\\times 4$ linear systems of equations:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_0 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_1 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_2 - \\boldsymbol{R}_3| \\\\\n",
    "k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_0| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_1| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_2| &k/|\\boldsymbol{r}_3 - \\boldsymbol{R}_3|\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\phi(\\boldsymbol{r}_0) \\\\ \\phi(\\boldsymbol{r}_1) \\\\ \\phi(\\boldsymbol{r}_2) \\\\ \\phi(\\boldsymbol{r}_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "which needs to be solved for the 4 unknowns $q_0$, $q_1$, $q_2$ and $q_3$.\n",
    "\n",
    "### Principle moments of inertia\n",
    "In study of the rotation of a rigid body about an arbitrary axis in three dimensions, you may have encountered the moment of inertia tensor:\n",
    "$$\n",
    "I_{\\alpha \\beta} = \\int \\rho(\\boldsymbol{r}) \\left(\\delta_{\\alpha \\beta}r^2 - \\boldsymbol{r}_\\alpha \\boldsymbol{r}_\\beta\\right)d^3 r,\n",
    "$$\n",
    "where $\\rho(r)$ is the mass density, $\\alpha$ and $\\beta$ denote Cartesian components, and $\\delta_{\\alpha \\beta}$ is the Kronecker delta. \n",
    "\n",
    "The moment of inertia tensor is represented by a $3\\times 3$ matrix: \n",
    "$$\n",
    "\\boldsymbol{I} = \n",
    "\\begin{pmatrix}\n",
    "I_{xx} & I_{xy} & I_{xz} \\\\\n",
    "I_{yx} & I_{yy} & I_{yz} \\\\\n",
    "I_{zx} & I_{zy} & I_{zz}.\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "This is a symmetric matrix. It is possible to choose a coordinate system such that the off-diagonal elements vanish. \n",
    "This axes of this coordinate system are known as the _principal axes_ for the body at the origin. Then the moment of inertian tensor is represented by a diagonal matrix, with diagonal elements $I_0$, $I_1$, and $I_2$, known as the principal moments. This is an instance of the \"eigenvalue problem\".\n",
    "\n",
    "## The problems to be solved\n",
    "First, we look at the problem where we have $n$ unknowns $x_i$, along with $n\\times n$ coefficients $A_{ij}$ and $n$ constants $b_i$:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A_{00} & A_{01} & \\dots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\dots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\dots & A_{n-1,n-1} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_{n-1}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where we used a comma to separate two indices when this was necessary to avoid confusion. \n",
    "These are $n$ equations linear in $n$ unknowns. \n",
    "\n",
    "In compact matrix form, this problem is written as \n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b},\n",
    "$$\n",
    "where $\\boldsymbol{A}$ is called the _coefficient matrix_. This is a problem that we will spend considerable time solving in this lecture. \n",
    "We will be doing this mainly by using the _augmented coefficient matrix_ which places together the elements of $\\boldsymbol{A}$ and $\\boldsymbol{b}$, i.e.:\n",
    "$$\n",
    "(\\boldsymbol{A}|\\boldsymbol{b})= \\left(\n",
    "\\begin{matrix}\n",
    "A_{00} & A_{01} & \\dots & A_{0,n-1} \\\\\n",
    "A_{10} & A_{11} & \\dots & A_{1,n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n-1,0} & A_{n-1,1} & \\dots & A_{n-1,n-1} \n",
    "\\end{matrix}\\right|\n",
    "\\left.\n",
    "\\begin{matrix}\n",
    "b_0 \\\\ b_1 \\\\ \\vdots \\\\b_{n-1}\n",
    "\\end{matrix}\n",
    "\\right).\n",
    "$$\n",
    "For now we assume the determinant of $\\boldsymbol{A}$ satisfy $|\\boldsymbol{A}| \\neq 0$.\n",
    "\n",
    "In a course on linear algebra you have seen examples of legitimate operations one can carry out while solving the system of linear equations. \n",
    "Such operations change the elements of $\\boldsymbol{A}$ and $\\boldsymbol{b}$, but leave the solution vector $\\boldsymbol{x}$ unchanged. \n",
    "More generally, we are allowed to carry the following elementary row operations:\n",
    "- _Scaling_: each row/equation may be multiplied by a constant (multiplies $|\\boldsymbol{A}|$ by the same constant).\n",
    "- _Pivoting_: two rows/equations may be interchanged (changes sign of $|\\boldsymbol{A}|$).\n",
    "- _Elimination_: a row/equation may be replaced by a linear combination of that row/equation with any other row/equation (doesn't change $|\\boldsymbol{A}|$).\n",
    "\n",
    "Keep in mind that these are operations that are carried out on the augmented coefficient matrix $(\\boldsymbol{A}|\\boldsymbol{b})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we wish to tackle the standard form of the matrix eigenvalue problem:\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{v} = \\lambda \\boldsymbol{v}.\n",
    "$${#eq-eigenvalue}\n",
    "Here, both $\\lambda$ and the column vector $\\boldsymbol{v}$ are unknown. This $\\lambda$ is called an _eigenvalue_ and $\\boldsymbol{v}$ is called an _eigenvector_.\n",
    "\n",
    "Let's sketch one possible approach to solve this problem.  We can move everything to the left-hand side, we have\n",
    "$$\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = \\boldsymbol{0},\n",
    "$$\n",
    "where $\\boldsymbol{I}$ is the $n\\times n$ identity matrix and $\\boldsymbol{0}$ is an $n\\times 1$ column vector made up of $0$s. \n",
    "It is easy to see that we are faced with a system of $n$ linear equations: the coefficient matrix here is $A - \\lambda \\boldsymbol{I}$. \n",
    "\n",
    "The trivial solution is $\\boldsymbol{v} = 0$. In order for a non-trivial solution to exist, we must have vanishing determinant $|\\boldsymbol{A} - \\lambda \\boldsymbol{I}| = 0$.\n",
    "In other words, the matrix $\\boldsymbol{A} - \\lambda \\boldsymbol{I}$ is singular. Expanding the determinant gives us a polynomial equation, known as the _characteristic equation_:\n",
    "$$\n",
    "(-1)^n\\lambda^n + c_{n-1} \\lambda^{n-1} + \\cdots + c_1 \\lambda + c_0 = 0.\n",
    "$$\n",
    "\n",
    "Thus, an $n \\times n $ matrix has at most $n$ distinct eigenvalues, which are the roots of the characteristic polynomial. When a root occurs twice, we say that root has multiplicity $2$. If a root occurs only once, in other words if it has multiplicity 1, we are dealing with a _simple_ eigenvalue.\n",
    "\n",
    "Having calculated the eigenvalues, one way to evaluate the eigenvectors is simply by using @eq-eigenvalue again. \n",
    "- Specifically, for a given/known eigenvalue, $\\lambda_i$, one tries to solve the system of linear equations $(\\boldsymbol{A} - \\lambda_i\\boldsymbol{I})\\boldsymbol{v}_i = 0$ for $\\boldsymbol{v}_i$. \n",
    "- For each value $\\lambda_i$, we will not be able to determine unique values of $\\boldsymbol{v}_i$, so we will limit ourselves to computing the relative values of the components of $\\boldsymbol{v}_i$. \n",
    "- We will in the following use the notation $(v_j)_0$, $(v_j)_1$ etc. to denote the $n$ elements of the column vector $\\boldsymbol{v}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df1fa0d82bdabb5288f7efc0788d29c4d5bb5f690328690a3d32d2cd65de760c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
