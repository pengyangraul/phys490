{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Numerical Integration III\"\n",
    "subtitle: \"\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "    html-to-math: katex\n",
    "    html-math-method: katex\n",
    "    callout-appearance: minimal\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo techniques\n",
    "_Monte Carlo techniques_ employ random numbers to tackle either naturally stochastic processes or nonprobabilistic problems.In keeping with the theme of the present chapter, we will focus on the latter, namely numerical integration. We start by discussing what \"random\" numbers are;  we then turn to a detailed discussion of one-dimensional Monte Carlo quadrature, before addressing the real-world problem of multidimensional integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random Numbers\n",
    "Let us consider a nice example of patterns is the decimal digits of the number $\\pi$:\n",
    "$$\n",
    "\\pi = 3.14159265358979323846264338327950\\dots\n",
    "$$\n",
    "Random simply means that you can not predict the following digit based on the previous digits. In other words, probabbilities for the appearance of any number ($0-9$) in the following digit are equally likely. \n",
    "\n",
    "On the other hand, computers produce what are known as _pseudorandom numbers_: the use of the modifier \"pseudo\" is due to the fact that computers are (supposed to be) deterministic systems. Thus, they produce sequences where each number is completely determined by its predecessor(s). However, if someone who does not have access to the random-number generation algorithm is led to believe that the sequence is truly random, then we have a \"good\" random-number generator. Thus, when dealing with good pseudorandom number sequences we tend to simply drop the \"pseudo\" and speak simply of random-number sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Numbers in Python\n",
    "Python itself includes a high-quality generator in the random module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19236379321481523\n",
      "0.2868424512347926\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(314159)\n",
    "print(random.random())\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first call the `random.seed()` function to provide the seed (which can be thought as the starting point for a (pseudo-) random number generator). We then see that repeated invocations to the `random.random()` function lead to new random numbers, uniformly distributed in $[0, 1)$ (meaning any numbers in between is equally likely to appear).\n",
    "\n",
    "If you need several random numbers stored in an array, you could hand-roll a solution using `random.random()`. However, it's probably best to directly employ the functionality contained in `numpy.random.uniform()`: this function takes in three parameters: the first two determine the interval $[a,b)$ while the third one contains the shape of the output array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5792603  -0.68717081  0.3347907  -0.88203901]\n",
      "[[-0.60110989  0.48781171  0.91841034]\n",
      " [-0.85697526  0.65842403  0.7414521 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(313159)\n",
    "print(np.random.uniform(-1,1,4))\n",
    "print(np.random.uniform(-1,1,(2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Quadrature\n",
    "We now turn to the question of how random numbers can be used to compute integrals, starting from the one-dimensional case for simplicity.\n",
    "\n",
    "### Probability Summary\n",
    "#### Discrete Random Variables\n",
    "Consider a discrete random variable $X$: its possible values are $x_i$, each one appearing with the corresponding probability $p_i^X$. Observe that we are using an upper-case symbol for the random variable and a lower-case symbol for its possible values.\n",
    "\n",
    "**Mean and variance** The _expectation_ of this random variable (also known as the mean value or expected value) is simply:\n",
    "$$\n",
    "\\braket{X} = \\sum_{i}p_i^X x_i.\n",
    "$$\n",
    "One can take the expected value of other quantities, for example the random variable $X^2$. This is called the second moment of $X$ and is simply:\n",
    "$$\n",
    "\\braket{X^2} = \\sum_i p_i^X x_i^2.\n",
    "$$\n",
    "This helps us calculate another useful quantity, known as the _variance_, $\\mathrm{var}(X)$. The variance is the expectation of the random variable $(X -\\braket{X})^2$:\n",
    "$$\n",
    "\\mathrm{var}(X) = \\braket{[ X - \\braket{X}]^2}.\n",
    "$$\n",
    "\n",
    "\n",
    "A simple calculation leads to an alternative expression for the variance:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{var}(X) &= \\braket{X^2 + \\braket{X}^2 - 2X\\braket{X}} = \\braket{X^2} + \\braket{\\braket{X}^2} - \\braket{2X\\braket{X}} \\\\\n",
    "& = \\braket{X^2} + \\braket{X}^2 - 2\\braket{X}^2 =\\braket{X^2} - \\braket{X}^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "where we have used the property that \n",
    "$$\n",
    "\\braket{aX + b X^2} = a\\braket{X} + b \\braket{X^2},\n",
    "$$\n",
    "namely the $\\braket{\\cdot}$ operation is linear.\n",
    "\n",
    "Another concept that is often used is that of the _standard deviation_; this is simply the square root of the variance, $\\mathrm{var}(X)$.\n",
    "\n",
    "If $X$ is a random variable, then $f(X)$ will also be a random variable: taking possible values $f(x_i)$ with probability $p_i^X$.\n",
    "Thus, it has expectation\n",
    "$$\n",
    "\\braket{f(X)} = \\sum_{i}p_i^X f(x_i)\n",
    "$$\n",
    "and, similarly, its variance is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{var}[f(X)] &= \\braket{[f(X) - \\braket{f(X)}]^2} = \\braket{f^2(X)} - \\braket{f(X)}^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "####  Continuous Random Variables\n",
    "For the case of a continuous random variable $X$ we are faced with a _probability density function_, $p(x)$, which plays the role $p_i^X$ played in the discrete case. We typically assume\n",
    "that the probability density function is normalized, i.e.:\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} p(x)dx = 1\n",
    "$$\n",
    "holds. The definition of the expectation is:\n",
    "$$\n",
    "\\braket{X} = \\int_{-\\infty}^{\\infty}x p(x) dx.\n",
    "$$\n",
    "\n",
    "Similarly, we have for the _variance_:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{var}(X) &= \\braket{[X - \\braket{X}]^2} = \\int_{-\\infty}^{\\infty} (x - \\braket{X})^2 p(x) dx \\\\\n",
    "& = \\braket{X^2} - \\braket{X}^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similar to the case for discrete random variable, if $X$ is a continuous variable, the $f(X)$ is also a continuous variable. \n",
    "\n",
    "\n",
    "### Population Mean and Population Variance\n",
    "From probability theory, we know that the expectation of a function f of a continuous random variable $X$ is\n",
    "$$\n",
    "f(X) = \\int_{-\\infty}^{\\infty}p(x) f(x)dx.\n",
    "$$\n",
    "\n",
    "For now, we take $p(x)$, the probability density function, to be uniform from $a$ to $b$, and zero otherwise. This leads to the following result:\n",
    "$$\n",
    "\\braket{f(X)} = \\frac{1}{b-a} \\int_{a}^{b} f(x) dx.\n",
    "$$\n",
    "\n",
    "Except for the $1/(b-a)$ prefactor, the right-hand side is exactly the numerical integration problem we would like to solve. \n",
    "\n",
    "To keep the terminology straight, we will call this $\\braket{f(X)}$ the _population mean_. \n",
    "\n",
    "Similarly, we introduce the _variance_ of a function of a random variable, in terms of an integral:\n",
    "$$\n",
    "\\mathrm{var}[f(X)] = \\braket{[f(X) - \\braket{f(X)}]^2}\n",
    "$$\n",
    "and specialize to the case where $p(x) = 1/(b - a)$, to find\n",
    "$$\n",
    "\\mathrm{var}[f(X)] = \\frac{1}{b-a}\\int_{a}^b f^2(x)dx \n",
    "- \\left(\\frac{1}{b-a}\\int_a^b f(x) dx \\right)^2\n",
    "$$\n",
    "We call this the _population variance_, and its\n",
    "square root the _population standard deviation_,$\\mathrm{var}[f(X)]$.\n",
    "\n",
    "Crucially, both the population mean and the population variance, $\\braket{f(X)}$ and $\\mathrm{var}[f(X)]$, are unknown, i.e., we don't know the value of either $\\int_a^b f(x)dx$ or $\\int_a^b f^2(x)dx$; this is precisely why we wish to employ Monte Carlo integration. In what follows, we will learn how to estimate both $\\braket{f(X)}$ and $\\mathrm{var}[f(X)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Mean and Its Variance\n",
    "Assume that the random variables $X_0, X_1, X_2, \\dots, X_{n-1}$ are drawn randomly from $p(x)$, which for us is uniform from $a$ to $b$. For each of these random variables $X_i$, we act with the function $f$, leading to $n$ new random variables, $f(X_0), f(X_1), f(X_2),\\dots, f(X_{nâˆ’1})$. \n",
    "\n",
    "The _arithmetic average_ of several random variables is also a random variable. We define:\n",
    "$$\n",
    "\\overline{f} = \\frac{1}{n}\\sum_{i=0}^{n-1} f(X_i)\n",
    "$$\n",
    "which we call the _sample mean_. It is the arithmetic average of the function value over $n$ samples. Crucially, this $\\overline{f}$ is a quantity computed from a finite number of samples, $n$; this means it is quite different from the population mean, $\\braket{f(X)}$.\n",
    "\n",
    "We will now see that the sample mean, $\\overline{f}$, can be used to estimate the population mean, $\\braket{f(X)}$.  Let us examine the expectation of the sample mean\n",
    "$$\n",
    "\\braket{\\overline{f}} = \\braket{\\frac{1}{n} \\sum_{i=0}^{n-1} f(X_i)} = \\frac{1}{n}\\sum_{i=0}^{n-1}\\braket{f(X)} = \\braket{f(X)}.\n",
    "$$\n",
    "\n",
    "In words, our result is that the _expectation of the sample mean is equal to the population mean_. This motivates our choice to use $\\overline{f}$ as an estimator of $f(X)$; for us an estimator is a useful approximation of a given\n",
    "quantity. It can be shown that as $n\\to\\infty$, $\\overline{f}\\to\\braket{f(X)}$.\n",
    "\n",
    "In order to quantify how fast the sample mean approaches the population mean, we turn to the _variance of the sample mean_:\n",
    "$$\n",
    "\\mathrm{var}(\\overline{f}) = \\mathrm{var}\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}f(X_i)\\right) = \\frac{1}{n^2} \\sum_{i=0}^{n-1} \\mathrm{var}[f(X)] = \\frac{1}{n}\\mathrm{var}[f(X)],\n",
    "$${#eq-var-f}\n",
    "where  we have used the property of variance\n",
    "$$\n",
    "\\mathrm{var}(\\lambda_1 X_1 + \\lambda_2 X_2)  = \\lambda_1^2 \\mathrm{var}(X_1) \n",
    "+ \\lambda_2^2 \\mathrm{var}(X_2).\n",
    "$$\n",
    "\n",
    "Thus, our result states that _the variance of the sample mean decreases as $1/n$._ The standard deviation of the sample means goes as $1/\\sqrt{n}$, because\n",
    "$$\n",
    "\\sqrt{\\mathrm{var}(\\overline{f})} = \\frac{1}{\\sqrt{n}} \\sqrt{\\mathrm{var}[f(X)]}.\n",
    "$$\n",
    "\n",
    "Notice that on the right hand side the quantity $\\mathrm{var}[f(x)]$ is not known, we need to come up with an estimator for $\\mathrm{var}[f(X)]$. We propose an estimator\n",
    "$$\n",
    "e_{var} = \\overline{f^2} - \\overline{f}^2 = \\frac{1}{n}\\sum_{i=0}^{n-1}f^2(X_i) - \\left[\\frac{1}{n}\\sum_{i=0}^{n-1} f(X_i)\\right]^2\n",
    "$$\n",
    "It is important to note that this expression makes use of the already observed values of $f(X_i)$; this means that it can readily be evaluated.\n",
    "\n",
    "One can expect its expectation, \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\braket{e_{var}}  &= \\frac{1}{n}\\sum_{i=0}^{n-1} \\braket{f^2(X_i)} - \\frac{1}{n^2}\\sum_{i,j=0}^{n-1}\\braket{f(X_i)f(X_j)} \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=0}^{n-1} \\braket{f^2(X_i)} - \\frac{1}{n^2}\\left[\\sum_{i=0}^{n-1} \\braket{f^2(X_i)} + \\sum_{i\\neq j}\\braket{f(X_i)}\\braket{f(X_j)}\\right] \\\\\n",
    "&=\\frac{n-1}{n^2}n\\braket{f^2(X)} - \\frac{1}{n^2}n(n-1)\\braket{f(X)}^2\\\\\n",
    "&=\\frac{n-1}{n}\\left[\\braket{f^2(X) - \\braket{f(X)}^2}\\right] = \\frac{n-1}{n}\\mathrm{var}[f(X)].\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You see that this estimator $e_{var}$ is close to, but not quite the same as the population variance $\\mathrm{var}[f(X)]$.\n",
    "This implies that it is not an _unbiased estimator_ (i.e., it is a _biased estimator_). . An unbiased estimator $g$ of a quantity $G$ is one for which the mean is equal to the quantity you're trying to estimate, $\\braket{g} = G$. But one can trivially define an unbiased estimator $n e_{var}/(n-1)$, since\n",
    "$$\n",
    "\\braket{\\frac{n}{n-1}e_{var}} = \\mathrm{var}[f(X)].\n",
    "$$\n",
    "\n",
    "With this, we can write @eq-var-f as\n",
    "$$\n",
    "\\mathrm{var}(\\overline{f}) = \\frac{1}{n}\\mathrm{var}[f(X)] \\simeq \\frac{1}{n} \\frac{n}{n-1} e_{var} = \\frac{1}{n-1}(\\overline{f^2} - \\overline{f}^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Prescription\n",
    "Let us recall that our goal is to approximate the integral\n",
    "$$\n",
    "\\int_{a}^b f(x) dx = (b-a)\\braket{f(X)}.\n",
    "$$\n",
    "Using the estimation described in the previous section, we can write\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int_{a}^b f(x) dx &\\simeq (b - a)\\overline{f} \\pm (b-a)\\sqrt{\\mathrm{var}(\\overline{f})} \\\\\n",
    "&= (b - a)\\overline{f} \\pm (b-a)\\sqrt{\\frac{\\overline{f^2} - \\overline{f}^2}{n-1}} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We took the square root of the variance of the sample mean here to produce the standard deviation of the sample mean; in order to interpret that in the usual (Gaussian) sense of $\\pm$, one also needs to make the assumption of a finite variance, which will always be true for us. In other words, we employed the _central limit theorem_ which tells us that asymptotically the $\\overline{f}$ obeys a normal/Gaussian distribution regardless of which $p(x)$ was employed to draw the samples (e.g., a uniform one in our case).\n",
    "\n",
    "We thus have the formula for the _Monte Carlo integration_\n",
    "$$\n",
    "\\int_a^b f(x) dx \\simeq \\frac{b - a}{n} \\sum_{i=0}^{n-1} f(X_i) \\pm \\frac{b-a}{\\sqrt{n-1}}\\sqrt{\\frac{1}{n}\\sum_{i=0}^{n-1}f^2(X_i) - \\left[\\frac{1}{n}\\sum_{i=0}^{n-1}f(X_i)\\right]^2}.\n",
    "$$\n",
    "\n",
    "Note that the $\\sqrt{n-1}$ i the denominator decreases the standard deviation of the sample mean. The first term $(b-a) f(X_i)$ is similar to the rectangle or midpoint methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo beyond the Uniform Distribution\n",
    "Having understood how and why Monte Carlo quadrature works, we now see if we can solve harder problems, or solve the same problems better. To give the punchline ahead of time, this will involve non-uniformly distributed random number.\n",
    "\n",
    "### Generalizing to Weight Functions\n",
    "In our previous discussion we see that the expectation of a function of a continuous random variable:\n",
    "$$\n",
    "\\braket{f(X)} = \\int_{-\\infty}^\\infty p(x)f(x) dx.\n",
    "$$\n",
    "\n",
    "This time, we take the probability density function to be $w(x)/(b-a)$ from $a$ to $b$ and zero elsewhere. This $w(x)$ is known as the _weight function_, which is positive but general. This leads to \n",
    "$$\n",
    "\\braket{f(X)} = \\frac{1}{b-a} \\int_a^b w(x) f(x) dx. \n",
    "$$\n",
    "\n",
    "Notice that the right hand side is precisely of the form we encounted last week in our discussion of general Gaussian quadrature.\n",
    "For this new population mean, we can write down the corresponding population variance. \n",
    "\n",
    "We could now introduce a new sample mean, \n",
    "$$\n",
    "\\overline{f} = \\frac{1}{n} \\sum_{i=0}^{n-1} f(X_i)\n",
    "$$\n",
    "where crucially this time the $X_0, X_1, \\dots,  X_{n-1}$ are drawn randomly from the probability density function $p(x) = w(x)/(b-a)$, i.e. _not_ from a uniform distribution.  \n",
    "\n",
    "We can follow the derivation in the previous section, and obtain the formula for _Monte Carlo integration with a general weight function_:\n",
    "$$\n",
    "\\int_a^b w(x)f(x) dx \\simeq \\frac{b - a}{n} \\sum_{i=0}^{n-1} f(X_i) \\pm \\frac{b-a}{\\sqrt{n-1}}\\sqrt{\\frac{1}{n}\\sum_{i=0}^{n-1}f^2(X_i) - \\left[\\frac{1}{n}\\sum_{i=0}^{n-1}f(X_i)\\right]^2}.\n",
    "$$\n",
    "Here the formula looks exactly the same as the previous one, except that $X_i$s are drawn from a different distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Transform Sampling\n",
    "You may wonder how to draw variables $X_i$s from the probability density function $w(x)/(b-a)$, as you only learned the generation of uniformly distributed random numbers. We now go over a specific technique that helps you accomplish this task, known as _inverse transform sampling_ or, sometimes, simply _inverse sampling_. \n",
    "\n",
    "The main idea is to somehow manage to convert the integrand $wf$ to $f$, via an appropriate change of variables. We make the following change of variables\n",
    "$$\n",
    "du = w(x)dx,\n",
    "$$\n",
    "which can be integrated\n",
    "$$\n",
    "u(x) = \\int_a^x w(x')dx'.\n",
    "$$\n",
    "This $u(x)$ is known as _cumulative distribution function_.\n",
    "\n",
    "We now realize that when $x$ goes from $a$ to $b$, $u$ goes from $0$ to $b-a$, i.e., $u(a) = 0$ and $u(b) = b-a$.\n",
    "We thus transform the original integral\n",
    "$$\n",
    "\\int_{a}^b w(x)f(x)dx = \\int_0^{b-a} f(x(u)) du\n",
    "$$\n",
    "where $f(x(u))$ means that we've expressed $x$ in terms of $u$. Note that the right hand side is in the form of an _unweighted_ integral over $u$. Thus, we can write\n",
    "$$\n",
    "\\int_a^b w(x)f(x) dx = \\int_0^{b-a}f(x(u))du  \\simeq  \\frac{b-a}{n} \\sum_{i=0}^{n-1} f(X(U_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate, the $U_i$s are uniformly distributed from $0$ to $b-a$ and as a result the $X_i$s (which are produced by $x(u)$ which is inverted from $u(x)$) are distributed according to $w(x)$ from $a$ to $b$. In summary, we've managed to employ uniform numbers, together with a transformation, to apply Monte Carlo to a weighted integral.\n",
    "\n",
    "Let us look at a specific example:\n",
    "$$\n",
    "I = \\int_0^1 e^{-x} \\cos(x) dx.\n",
    "$$\n",
    "\n",
    "You can definitely choose $f(x) = e^{-x}\\cos(x)$ and sample $x$ uniformly in the interval between $0$ and $1$. However, since $e^{-x}$ makes the integrand $f(x)$ decrease as $x$ is increasing, you are not taking into the consideration the fact the most of the contribution to the integral comes from small values of $x$.\n",
    "\n",
    "Instead, we can take the weight function $w(x) = ce^{âˆ’x}$, where we normalize the weight function by choosing $c = e/(e-1)$, namely\n",
    "$$\n",
    "\\int_0^1 w(x)dx = \\int_0^1 \\frac{1}{e-1} e^{-x}dx = 1.\n",
    "$$\n",
    "We then choose the function $f(x) = \\cos(x)/c$, such that the normalization constant get canceled.\n",
    "\n",
    "The inverse transform method then tells you to evaluate $u(x)$ as\n",
    "$$\n",
    "u(x) = c\\int_0^x e^{-x'} dx' = c(1-e^{-x})\n",
    "$$\n",
    "which leads to \n",
    "$$\n",
    "x(u) = -\\ln\\left( 1 - \\frac{u}{c}\\right).\n",
    "$$\n",
    "\n",
    "Thus, we can write \n",
    "$$\n",
    "\\begin{align*}\n",
    "I &= \\int_0^1 c e^{-x} \\frac{\\cos x}{c} dx = \\int_0^1 \\frac{1}{c} \\cos\\left[-\\ln \\left( 1 - \\frac{u}{c}\\right) \\right]du \\\\\n",
    "& \\simeq \\frac{1}{n} \\sum_{i=0}^{n-1} \\frac{1}{c}  \\cos\\left[-\\ln \\left( 1 - \\frac{U_i}{c}\\right) \\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "where $U_i$s are sampled uniformly from $0$ to $1$. \n",
    "\n",
    "You see that you have managed to sample from an exponential distribution, even though your input random-number generator was for a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "In this section, we will show that we can even introduce weight functions by hand even the integrand appears to be a single (i.e. unweighted) function:\n",
    "$$\n",
    "\\int_a^b f(x) dx = \\int_a^b w(x) \\frac{f(x)}{w(x)} dx = \\int_0^{b-a} \\frac{f(x(u))}{w(x(u))}du \\simeq \\frac{b-a}{n}\\sum_{i=0}^{n-1} \\frac{f(X(U_i))}{w(X(U_i))}.\n",
    "$$\n",
    "\n",
    "- In the first equality we multiplied and divided with a positive weight function of our choosing. \n",
    "- In the second equality we used the change of variables as we did previously.\n",
    "- In the thrid equality we treated $f/w$ as our (\"unweighted\") integrand and therefore used $U_i$s which are uniformly distributed from $0$ to $b-a$. \n",
    "\n",
    "The entire process is known as _importance sampling_.\n",
    "\n",
    "You may be wondering why we went through the trouble of doing this. The answer is as follows: if you choose a $w(x)$ that behaves approximately the same way that $f(x)$ does,then your random numbers will be distributed in the most \"important\" regions, instead of uniformly. To put the same idea differently: since our (unweighted) integrand is $f/w$, the variance will be computed with $f/w$ in the place of $f$. Since $w(x)$ is chosen to be similar to $f(x)$, we see that $f/w$ will be less varying than f itself was; this will lead to a reduction of the variance, i.e., a better overall estimate of the integral we are trying to compute.\n",
    "\n",
    "To see this in action, we turn to the example we've been revisiting throughout numerical integration, namely we integrate the following function\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{x^2 + 1}}\n",
    "$$\n",
    "from 0 to 1. We can plot it in the following (blue solid curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0,1,50)\n",
    "fx = 1/np.sqrt(x**2+1)\n",
    "c0 = 4-2*np.sqrt(2)\n",
    "c1 = -6+4*np.sqrt(2)\n",
    "wx = c0 + c1*x\n",
    "plt.plot(x, fx,'b-', label=r'$\\frac{1}{\\sqrt{x^2 + 1}}$')\n",
    "plt.plot(x, fx/wx,'r--', label=r'$\\frac{1}{w(x)\\sqrt{x^2 + 1}}$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $f(x)$ is decreasing from 1 to 0.7 in our interval; with that in mind, we decide to employ a linear weight function:\n",
    "$$\n",
    "w(x) = c_0 + c_1 x.\n",
    "$$\n",
    "We wish $w(x)$ to roughly track the behavior of $f(x)$ in our interval; one way to do this is to ensure that $f(0)/w(0) = f(1)/w(1)$.\n",
    "This gives one equation relating $c_0$ and $c_1$. If we then also impose the normalization condition\n",
    "$$\n",
    "\\int_0^1 w(x)dx = 1\n",
    "$$\n",
    "we get another relation. Thus, we are able to determine both parameters:\n",
    "$$\n",
    "c_0 = 4 - 2\\sqrt{2}, \\quad c_1 = -6 + 4\\sqrt{2}. \n",
    "$$\n",
    "\n",
    "In the above figure, the red dashed line corresponds to the value of $f(x)/w(x)$, which varies between 0.85 and 0.9, which is considerably smaller than the variation of our original integrand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the importance-sampling prescription. We first compute\n",
    "$$\n",
    "u(x) = \\int_{0}^x (c_0 + c_1x') dx' = c_0 x + \\frac{1}{2}c_1x^2,\n",
    "$$\n",
    "which can be inverted to obtain\n",
    "$$\n",
    "x(u) = \\frac{-c_0 + \\sqrt{2c_1 u + c_0^2}}{c_1}.\n",
    "$$\n",
    "\n",
    "Note that we picked the root which leads to $x$ from 0 to 1 for $u$ in $[0,1]$. We can now compute the integral as\n",
    "$$\n",
    "\\int_0^1 \\frac{1}{\\sqrt{x^2+1}}dx  \\simeq \\frac{1}{n} \\sum_{i=0}^{n-1} \\frac{1}{\\sqrt{X(U_i)^2 + 1}(c_0 + c_1 X(U_i))}\n",
    "$$\n",
    "in which $U_i$ are uniformly distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We now implement Monte Carlo quadrature for the case of a one-dimensional integral. This is done in `montecarlo()`, which addresses both the case of uniform sampling and that of importance sampling. Crucially, uniform sampling is the default parameter value, which means that you can call this function with `montecarlo(f,a,b,n)`, i.e., with precisely the same interface as our three integrators in the Newton-Cotes integration and Gauss-Legendre integration. \n",
    "\n",
    "Also, we wrote a different function `stats(fs)`, which compute the mean and standard deviation for a given array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    100   0.873135430 0.009827018   0.880184046 0.001397861\n",
      "   1000   0.878313494 0.003014040   0.880653976 0.000439206\n",
      "  10000   0.879343920 0.000933506   0.881029489 0.000139055\n",
      " 100000   0.881289768 0.000292906   0.881400087 0.000043577\n",
      "1000000   0.881433836 0.000092589   0.881389786 0.000013775\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 1/np.sqrt(x**2 + 1)\n",
    "    \n",
    "def montecarlo(f,a,b,n,option=\"uniform\"):\n",
    "    np.random.seed(314159)\n",
    "    us = np.random.uniform(a, b, n)\n",
    "\n",
    "    if option==\"uniform\":\n",
    "        fs = f(us)\n",
    "    else:\n",
    "        c0 = 4 - 2*np.sqrt(2)\n",
    "        c1 = -6 + 4*np.sqrt(2)\n",
    "        xs = (-c0 + np.sqrt(2*c1*us + c0**2))/c1\n",
    "        fs = f(xs)/(c0 + c1*xs)\n",
    "\n",
    "    fbar, err = stats(fs)\n",
    "    return (b-a)*fbar, (b-a)*err\n",
    "\n",
    "def stats(fs):\n",
    "    n = fs.size\n",
    "    fbar = np.sum(fs)/n\n",
    "    fsq = np.sum(fs**2)/n\n",
    "    varfbar = (fsq - fbar**2)/(n - 1)\n",
    "    return fbar, np.sqrt(varfbar)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for n in 10**np.arange(2,7):\n",
    "        avu, erru = montecarlo(f, 0., 1., n)\n",
    "        avi, erri = montecarlo(f, 0., 1., n, option=\"is\")\n",
    "        rowf = \"{0:7d}   {1:1.9f} {2:1.9f}   {3:1.9f} {4:1.9f}\"\n",
    "        print(rowf.format(n, avu, erru, avi, erri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that importance sampling consistently helps us reduce the standard deviation by an order of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo in Many Dimensions\n",
    "We now turn to a truly relevant application of the Monte Carlo approach: multidimensional integration. As in the one-dimensional case, we start from uniform sampling, but then try to do a better job: we explain how to carry out weighted sampling via the _Metropolis-Hastings_ algorithm, which is one of the most successful methods ever.\n",
    "\n",
    "### Uniform Sampling\n",
    "Notations: we bundle together the variables $x_0, x_1, \\dots, x_{d-1}$ into $\\boldsymbol{x}$. We will be dealing with a scalar function of many variables, $f(\\boldsymbol{x})$. We define the _population mean_\n",
    "$$\n",
    "\\braket{f(\\boldsymbol{X})} = \\frac{1}{V}\\int f(\\boldsymbol{x}) d^dx\n",
    "$$\n",
    "where the left hand side is of a function of a multidimensional random variable $\\boldsymbol{X}$. The volume $V$ is the generalization of the interval length $b-a$.\n",
    "\n",
    "The practical formula for _multidimensional Monte Carlo integration_ is\n",
    "$$\n",
    "\\int f(\\boldsymbol{x})d^d x \\simeq \\frac{V}{\\mathcal{N}}\\sum_{i=0}^{\\mathcal{N} - 1} f(\\boldsymbol{X}_i) \\pm \n",
    "\\frac{V}{\\mathcal{N} - 1} \\sqrt{\\frac{1}{\\mathcal{N}}\\sum_{i=0}^{\\mathcal{N}-1}f^{2}(\\boldsymbol{X}_i) - \\left[\\frac{1}{\\mathcal{N}} \\sum_{i=0}^{\\mathcal{N}-1}\n",
    "f(\\boldsymbol{X}_i)\\right]^2} \n",
    "$$\n",
    "where $\\mathcal{N}$ is the number of samples, and $\\boldsymbol{X}_i$ are sampled uniformly.\n",
    "\n",
    "### Weighted Sampling via the Metropolisâ€“Hastings Algorithm\n",
    "We can also introduce a weight for the multidimensional problem, and obtain\n",
    "$$\n",
    "\\int w(\\boldsymbol{x}) f(\\boldsymbol{x})d^d x \\simeq \\frac{V}{\\mathcal{N}}\\sum_{i=0}^{\\mathcal{N} - 1} f(\\boldsymbol{X}_i) \\pm \n",
    "\\frac{V}{\\mathcal{N} - 1} \\sqrt{\\frac{1}{\\mathcal{N}}\\sum_{i=0}^{\\mathcal{N}-1}f^{2}(\\boldsymbol{X}_i) - \\left[\\frac{1}{\\mathcal{N}} \\sum_{i=0}^{\\mathcal{N}-1}\n",
    "f(\\boldsymbol{X}_i)\\right]^2} \n",
    "$$\n",
    "where $\\boldsymbol{X}_i$ are drawn from the distribution $w(\\boldsymbol{x})$.\n",
    "\n",
    "We can in principle derive formulas for inverse sampling, however, the multidimensional case would be much more complicated and practically this is very difficult to perform. In the following, we will construct a way to sample $\\boldsymbol{X}_i$ directly from distribution $w(\\boldsymbol{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Chains\n",
    "We shall introduce the so-called _Markov chain Monte Carlo (MCMC)_. The main new concept involved here is that of a _Markov chain_: imagine you have a sequence of random samples $\\boldsymbol{X}_0,\\boldsymbol{X}_1,\\dots,\\boldsymbol{X}_{\\mathcal{N}-1}$ for which a given sample $\\boldsymbol{X}_i$ depends only on the previous one $\\boldsymbol{X}_{i-1}$, but not on any of the earlier ones. In other words, one starts from a random sample $\\boldsymbol{X}_0$, uses that to produce sample $\\boldsymbol{X}_1$, then uses that in its turn to produce sample $\\boldsymbol{X}_2$, and so on. This sequence of samples $\\boldsymbol{X}_0,\\boldsymbol{X}_1,\\dots,\\boldsymbol{X}_{\\mathcal{N}-1}$ is known as a _random walk_. \n",
    "\n",
    "Note that this is quite different from what we were doing in earlier sections: there the $X_i$ were independent from one another, whereas now we use a given $\\boldsymbol{X}_{i-1}$ to produce the next one $\\boldsymbol{X}_{i}$. The reason Markov chains are so useful is that they can be produced such that they asymptotically (i.e., as $\\mathcal{N}\\to \\infty$) have the distribution we would like them to, which in our case would be $w(\\boldsymbol{x})$. One could therefore do an increasingly better job at computing a $d$-dimensional integral by continuing the Markov chain for larger values of $\\mathcal{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Balance\n",
    "We wish to produce a Markov chain with an asymptotic distribution of our choosing, which would therefore be the stationary distribution of the chain. Thus, we can borrow ideas from the statistical mechanics of systems in equilibrium. A sufficient (but not necessary) condition of evolving toward equilibrium and staying there is the _principle of detailed balance_:\n",
    "$$\n",
    "w(\\boldsymbol{X}) T(\\boldsymbol{X} \\to \\boldsymbol{Y}) = w(\\boldsymbol{Y}) T(\\boldsymbol{Y} \\to \\boldsymbol{X}).\n",
    "$$\n",
    "Here $T(\\boldsymbol{X}\\to \\boldsymbol{Y})$ is the (conditional) probability density that you will move to $\\boldsymbol{Y}$ if you start at $\\boldsymbol{X}$; it is often called the _transition probability_. \n",
    "\n",
    "Since we're dealing with a Markov chain, we need to know how to go from one sample to the next; this is precisely what the transition probability will allow us to do. Since $w(\\boldsymbol{X})$ is the probability density of being near $\\boldsymbol{X}$, $w(\\boldsymbol{X})T(\\boldsymbol{X}\\to\\boldsymbol{Y})$ quantifies how likely it is to start at $\\boldsymbol{X}$ and move to $\\boldsymbol{Y}$. Similarly, $W(\\boldsymbol{Y})T(\\boldsymbol{Y}\\to\\boldsymbol{X})$ tells us how likely it is to start at $\\boldsymbol{Y}$ and move to $\\boldsymbol{X}$. \n",
    "\n",
    "In words, the principle of detailed balence says that it is equally likely that we will go in one direction as in the reverse direction. The principle of detailed balance is sometimes known as the reversibility condition, due to the fact that the reverse process would result if everything went backward in time. Intuitively, detailed balance tells us that if you're in equilibrium then effectively not much is changing: you could go somewhere, but you're just as likely to come back.\n",
    "\n",
    "At this stage, detailed balance is just a condition: we haven't shown how to actually produce a Markov chain that obeys it. Even so, we will now spend some time seeing exactly\n",
    "how the detailed-balance condition can help us accomplish our goal. Instead of thinking about moving from an individual sample to another one, it can be helpful to think in terms of going from one probability density function to another. \n",
    "\n",
    "Assume that $p_{i-1}(\\boldsymbol{X})$ is the distribution of values of the random variable $\\boldsymbol{X}_{i-1}$ and, similarly, $p_i(\\boldsymbol{X})$ is the distribution of $\\boldsymbol{X}_i$. We can straightforwardly relate $p_i(\\boldsymbol{X})$ to $p_{i-1}(\\boldsymbol{X})$ as follows:\n",
    "$$\n",
    "p_i(\\boldsymbol{X}) = p_{i-1}(\\boldsymbol{X}) + \\int [p_{i-1}(\\boldsymbol{Y})T(\\boldsymbol{Y}\\to \\boldsymbol{X}) - p_{i-1}(\\boldsymbol{X})T(\\boldsymbol{X}\\to\\boldsymbol{Y})]d^d Y.\n",
    "$$\n",
    "In words, what this is saying is that the probability of being near $\\boldsymbol{X}$ at step $i$ is equal to the probability of being near $\\boldsymbol{X}$ at step $i-1$, plus the probability of leaving all other configurations $\\boldsymbol{Y}$ and coming to $\\boldsymbol{X}$, minus the probability of leaving $\\boldsymbol{X}$ and going to any other configurations $\\boldsymbol{Y}$. \n",
    "\n",
    "We can first show that $w(\\boldsymbol{X})$ is a fixed point of the iteration: for $p_{i-1}(\\boldsymbol{X})=w(\\boldsymbol{X})$, we have\n",
    "$$\n",
    "p_i(\\boldsymbol{X})  = w(\\boldsymbol{X}) + \\int [w(\\boldsymbol{Y})T(\\boldsymbol{Y}\\to\\boldsymbol{X}) - w(\\boldsymbol{X})T(\\boldsymbol{X}\\to\\boldsymbol{Y})]d^d Y = w(\\boldsymbol{X}),\n",
    "$$\n",
    "where in the last step we have used th detailed-balence condition. \n",
    "\n",
    "Second, we would like to know that we are actually approaching that stationary distribution: it wouldn't do us much good if a fixed point existed but we could never reach it. To see this, we can write\n",
    "$$\n",
    "\\frac{p_i(\\boldsymbol{X})}{w(\\boldsymbol{X})} = \\frac{p_{i-1}(\\boldsymbol{X})}{w(\\boldsymbol{X})} + \\int [p_{i-1}(\\boldsymbol{Y})\\frac{T(\\boldsymbol{Y}\\to \\boldsymbol{X})}{w(\\boldsymbol{X})} - p_{i-1}(\\boldsymbol{X})\\frac{T(\\boldsymbol{X}\\to\\boldsymbol{Y})}{w(\\boldsymbol{X})}]d^d Y.\n",
    "$$\n",
    "Using the detailed-balence condition in this form:\n",
    "$$\n",
    "\\frac{T(\\boldsymbol{X} \\to \\boldsymbol{Y})}{w(\\boldsymbol{Y})}  = \\frac{T(\\boldsymbol{Y}\\to\\boldsymbol{X})}{w(\\boldsymbol{X})},\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "\\frac{p_i(\\boldsymbol{X})}{w(\\boldsymbol{X})} = \\frac{p_{i-1}(\\boldsymbol{X})}{w(\\boldsymbol{X})} + \\int T(\\boldsymbol{X}\\to \\boldsymbol{Y})\\left[\\frac{p_{i-1}(\\boldsymbol{Y})}{w(\\boldsymbol{Y})} - \\frac{p_{i-1}(\\boldsymbol{X})}{w(\\boldsymbol{X})}\\right]d^d Y.\n",
    "$$\n",
    "\n",
    "Note that here $T(\\boldsymbol{X}\\to \\boldsymbol{Y})$ is the transition probability and thus is positive. To understand the above expression:\n",
    "\n",
    "- Let's say a $\\frac{p_{i-1}(\\boldsymbol{X})}{w(\\boldsymbol{X})}$ is near a maximum, i.e., is larger than other ratios, $\\frac{p_{i-1}(\\boldsymbol{Y})}{w(\\boldsymbol{Y})}$. Then, the integral will become nagative and thus $\\frac{p_{i}(\\boldsymbol{X})}{w(\\boldsymbol{X})}$ will become smaller. \n",
    "- Let's say a $\\frac{p_{i-1}(\\boldsymbol{X})}{w(\\boldsymbol{X})}$ is near a minimum, i.e., is smaller than other ratios, $\\frac{p_{i-1}(\\boldsymbol{Y})}{w(\\boldsymbol{Y})}$. Then, the integral will become positive and thus $\\frac{p_{i}(\\boldsymbol{X})}{w(\\boldsymbol{X})}$ will become larger. \n",
    "\n",
    "In both cases, the ratio  $\\frac{p_{i}(\\boldsymbol{X})}{w(\\boldsymbol{X})}$ will be closer to 1 than $\\frac{p_{i-1}(\\boldsymbol{X})}{w(\\boldsymbol{X})}$ was.\n",
    "\n",
    "While we still haven't shown how to produce a Markov chain that obeys detailed balance, our two results are that if you have a Markov chain that obeys detailed balance then:\n",
    "\n",
    "a. $w(\\boldsymbol{X})$ is a _stationary distribution_, \n",
    "b. $p_i(\\boldsymbol{X})$ _asymptotically approaches that stationary distribution_. \n",
    "\n",
    "In other words, our Markov chain will approach a $d$-dimensional equilibrium distribution of our choosing. We will now introduce an elegant trick that is able to produce a Markov chain obeying detailed balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metropolisâ€“Hastings Algorithm\n",
    "The _Metropolisâ€“Hastings algorithm_ starts by splitting the transition probability:\n",
    "$$\n",
    "T(\\boldsymbol{X}\\to\\boldsymbol{Y}) = \\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})\\alpha(\\boldsymbol{X}\\to\\boldsymbol{Y})\n",
    "$$\n",
    "where $\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})$ is the probability of making a _proposed_ step from $\\boldsymbol{X}$ to $\\boldsymbol{Y}$ and $\\alpha(\\boldsymbol{X}\\to\\boldsymbol{Y})$ is the probability of _accepting_ that move.\n",
    "\n",
    "Note that since we are dealing with an _acceptance_ probability $\\alpha(\\boldsymbol{X}\\to\\boldsymbol{Y})$, this means that some moves will be accepted (i.e., the system moves from $\\boldsymbol{X}$ to $\\boldsymbol{Y}$) and some moves will be rejected (i.e., the system will stay at $\\boldsymbol{X}$). The proposal probability $\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})$ is not unique, and several choices are discussed in the literature. The acceptance probability will be chosen in such a way that detailed balance is obeyed. \n",
    "\n",
    "The Metropolisâ€“Hastings algorithm proceeds by evaluating the following quantity\n",
    "$$\n",
    "R(\\boldsymbol{X}\\to\\boldsymbol{Y}) = \\frac{w(\\boldsymbol{Y})\\pi(\\boldsymbol{Y}\\to\\boldsymbol{X})}{w(\\boldsymbol{X})\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})}\n",
    "$$\n",
    "known as the _Metropolis-Hasting ratio_. \n",
    "Here everything on the right-hand side is known: the desired distribution $w$ is of our choosing, as is also true of the proposal distribution $\\pi$. As a matter of fact, a simpler version of the Metropolis-Hastings algorithm, known as the _Metropolis algorithm_ since that's how it was originally put forward, employs a symmetric proposal distribution; when $\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})=\\pi(\\boldsymbol{Y}\\to\\boldsymbol{X})$ you can see that the ratio is simply $R(\\boldsymbol{X}\\to\\boldsymbol{Y}) = w(\\boldsymbol{Y})/w(\\boldsymbol{X})$, namely the ratio of the (analytical known) desired weight at the configuration $\\boldsymbol{Y}$ and at the configuration $\\boldsymbol{X}$.\n",
    "\n",
    "The next part of the Metropolis-Hastings algorithm is to use the ratio $R(\\boldsymbol{X}\\to\\boldsymbol{Y})$ to determine the acceptance probability as follows:\n",
    "$$\n",
    "\\alpha(\\boldsymbol{X}\\to\\boldsymbol{Y}) = \\min[1, R(\\boldsymbol{X}\\to\\boldsymbol{Y})].\n",
    "$$\n",
    "\n",
    "We already know that $R(\\boldsymbol{X}\\to\\boldsymbol{Y})$ is non-negative. What the above equation does is to account for the possibility that the ratio $R(\\boldsymbol{X}\\to\\boldsymbol{Y})$ is larger than 1: in that case, the acceptance probability is taken to be 1. If $R$ is less than 1, then the proposed step is taken with probability $R$.\n",
    "\n",
    "We can show that with this choice of $\\alpha(\\boldsymbol{X}\\to\\boldsymbol{Y})$, detailed balance is satisfied. \n",
    "Let us first assume $R(\\boldsymbol{X}\\to\\boldsymbol{Y})<1$, then \n",
    "$$\n",
    "\\begin{align*}\n",
    "w(\\boldsymbol{X})T(\\boldsymbol{X}\\to\\boldsymbol{Y}) &= w(\\boldsymbol{X})\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})R(\\boldsymbol{X}\\to\\boldsymbol{Y})\n",
    "= w(\\boldsymbol{X})\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})\\frac{w(\\boldsymbol{Y})\\pi(\\boldsymbol{Y}\\to\\boldsymbol{X})}{w(\\boldsymbol{X})\\pi(\\boldsymbol{X}\\to\\boldsymbol{Y})} \\\\\n",
    "&=w(\\boldsymbol{Y})\\pi(\\boldsymbol{Y}\\to\\boldsymbol{X})\n",
    "\\end{align*}\n",
    "$$\n",
    "Since $R(\\boldsymbol{X}\\to\\boldsymbol{Y})R(\\boldsymbol{Y}\\to\\boldsymbol{X})=1$, we have $R(\\boldsymbol{Y}\\to\\boldsymbol{X})>1$ if $R(\\boldsymbol{X}\\to\\boldsymbol{Y})<1$. \n",
    "Thus, we have $\\alpha(\\boldsymbol{Y}\\to\\boldsymbol{X}) = 1$. We can thus write\n",
    "$$\n",
    "w(\\boldsymbol{X})T(\\boldsymbol{X}\\to\\boldsymbol{Y}) = w(\\boldsymbol{Y})\\pi(\\boldsymbol{Y}\\to\\boldsymbol{X})\\alpha(\\boldsymbol{Y}\\to\\boldsymbol{X}) = w(\\boldsymbol{Y})T(\\boldsymbol{Y}\\to\\boldsymbol{X}).\n",
    "$$\n",
    "This proves the detailed balance condition when $R(\\boldsymbol{X}\\to\\boldsymbol{Y})<1$. In your homework, you will show the detailed balance condition if $R(\\boldsymbol{X}\\to\\boldsymbol{Y})> 1$.\n",
    "\n",
    "In summary, we have shown that _the Metropolisâ€“Hastings algorithm_ satisfies detailed balance.  As we discussed, detailed balance means that $w(\\boldsymbol{X})$ is a stationary distribution and our random walk will be asymptotically approaching that stationary distribution. Thus, we have managed to draw random samples from a $d$-dimensional distribution, which was our goal all along.\n",
    "\n",
    "It is important to emphasize that the Metropolisâ€“Hastings algorithm has allowed us to draw random samples from a $d$-dimensional $w(\\boldsymbol{X})$ simply by calculating ratios of $w$ (and perhaps also of $\\pi$) at two configurations each time. There was no need to worry about a change of variables, no numerical multidimensional inversion, and so on. Simply by evaluating known quantities at a given and a trial configuration, we managed to solve a complicated sampling problem. This is why the Metropolisâ€“Hastings prescription is routinely listed among the most important algorithms of the twentieth century.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Implement the Metropolis Algorithm\n",
    "We now give a practical step-by-step summary of the Metropolis algorithm, which allows us to sample from the $d$-dimensional distribution $w(\\boldsymbol{X})$.\n",
    "\n",
    "1. Start at a random location, $\\boldsymbol{X}_0$. It shouldn't matter where you start, since the Markov chain will \"equilibrate\", reaching the same stationary distribution, anyway. You could account for this \"burn-in\" time by discarding some early iterations or you could start from an $\\boldsymbol{X}_0$ that is not highly unlikely, i.e., pick X0 such that $w(\\boldsymbol{X}_0)$ is not too small.\n",
    "2. Take $\\boldsymbol{X}_{iâˆ’1}$ as given (this is $\\boldsymbol{X}_{0}$ the first time around) and produce a uniformly distributed proposed step according to: \n",
    "   $$\n",
    "   \\boldsymbol{Y}_i = \\boldsymbol{X}_{i-1} + \\theta \\times \\boldsymbol{U}_i\n",
    "   $$\n",
    "   where $\\boldsymbol{U}_i$ is a $d$-dimensional sample of uniformly distributed random numbers from $-1$ to $1$; here $\\theta$ is a number that controls the \"step size\" and $\\boldsymbol{Y}_i$ is the proposed walker configuration. This step, being uniformly distributed in a multidimensional cube of side $2\\theta$, is in fact employing a proposal distribution $\\pi$ that is symmetric; this means that we are actually dealing with the simple Metropolis algorithm. The value of $\\theta$ is chosen (by trying) such that roughly $15\\%$ to $50\\%$ of the proposed steps are accepted.\n",
    "3. For Metropolis algorithm, we can compute the acceptance rate\n",
    "   $$\n",
    "   \\alpha(\\boldsymbol{X}_{i-1}\\to\\boldsymbol{Y}_i) = \\min\\left[1,\\frac{w(\\boldsymbol{Y}_i)}{w(\\boldsymbol{X}_{i-1})}\\right].\n",
    "   $$\n",
    "   Here you see only the ratio $\\frac{w(\\boldsymbol{Y}_i)}{w(\\boldsymbol{X}_{i-1})}$ matters and thus we do not need to normalize $w(\\boldsymbol{X})$.\n",
    "4. With probability $\\alpha(\\boldsymbol{X}_{i-1}\\to\\boldsymbol{Y}_i)$, set $\\boldsymbol{X}_i=\\boldsymbol{Y}_i$ (the proposed step is accepted), otherwise set $\\boldsymbol{X}_i=\\boldsymbol{X}_{i-1}$ (the proposed step is rejected). In practice, this is done as follows: generate a random number $\\xi_i$ uniformly distributed from 0 to 1. Then set\n",
    "   $$\n",
    "   \\boldsymbol{X}_{i}=\n",
    "   \\begin{cases}\n",
    "   \\boldsymbol{Y}_{i}, & {\\rm if\\ }\\alpha(\\boldsymbol{X}_{i-1}\\to\\boldsymbol{Y}_{i})\\geq\\xi_{i}\\\\\n",
    "   \\boldsymbol{X}_{i}, & {\\rm if\\ }\\alpha(\\boldsymbol{X}_{i-1}\\to\\boldsymbol{Y}_{i})<\\xi_{i}\n",
    "   \\end{cases}.\n",
    "   $$\n",
    "Note that random numbers appear in two distinct roles: first, they allow us to produce the proposed walker configuration, $\\boldsymbol{Y}_i$. Second, they help us decide whether to accept or reject the proposed step. Also, note that $\\theta$ affects the overall acceptance rate, which we prefer to have it between 15\\% to 50\\%. \n",
    "1.  Every $n_m$ steps, make a \"measurement\", i.e., evaluate $f(\\boldsymbol{X}_i)$. Because the Markov chain samples are not statistically independent, we are computing the sample mean (and its variance) using not every single sample, but every $n_m$-th one. This is done in order to eliminate the correlation between the samples.\n",
    "2.  Increment $i$ by 1 and go back to step 1. Terminate the entire process when you've generated sufficiently many samples/measurements ($\\mathcal{N}$) that you are comfortable with the variance of the sample mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. In the lecture note, you have seen that the detailed balance is satisfied by taking $R(\\boldsymbol{X}\\to\\boldsymbol{Y})<1$. Now, please show that the detailed balance condition is also satisfied if we have $R(\\boldsymbol{X}\\to\\boldsymbol{Y})> 1$.\n",
    "2. Calculate the following two dimensional integration numerically using Monte Carlo method, with Metropolis algorithm.\n",
    "   $$\n",
    "   I = \\frac{\\int_{x^2+y^2\\leq 1} dxdy\\, e^{-(x^2+y^2)^2}x^2}{\\int_{x^2+y^2\\leq 1} dxdy\\, e^{-(x^2 + y^2)}}\n",
    "   $$ \n",
    "   Please read the following instructions before implementation.\n",
    "   Please take the weight function\n",
    "   $$\n",
    "   w(x,y) = \\exp(-(x^2 + y^2))\n",
    "   $$\n",
    "   for $x^2 + y^2 \\leq 1$ and zero otherwise.\n",
    "   Thus, one can write\n",
    "   $$\n",
    "   I = \\frac{\\int_{x^2+y^2\\leq 1} dxdy\\, w(x,y)f(x,y)}{\\int_{x^2+y^2\\leq 1} dxdy\\, w(x,y)}, \\quad f(x,y) = x^2.\n",
    "   $$\n",
    "   We can introduce the (normalized) probability density function\n",
    "   $$\n",
    "   p(x,y) = \\frac{w(x,y)}{\\int_{x^2+y^2\\leq 1} dxdy\\, w(x,y)},\\quad \\Rightarrow \\int dxdy\\, p(x,y)  = 1,\n",
    "   $$\n",
    "   then\n",
    "   $$\n",
    "   I = \\int dx dy\\, p(x,y)f(x,y) \\equiv \\braket{f(X,Y)}.\n",
    "   $$\n",
    "   You see $I$ is simply the population mean of $f(X,Y)$, where $(X,Y)$ is the 2D random variable with probability density function $p(X,Y)$.\n",
    "\n",
    "   In class, you learned that the population mean can be approximated using the sample mean, namely\n",
    "   $$\n",
    "   I \\simeq \\frac{1}{\\mathcal{N}}\\sum_{i=0}^{\\mathcal{N}-1}f(X_i,Y_i),\n",
    "   $$\n",
    "   where $(X_0,Y_0), (X_1, Y_1), \\dots, (X_{\\mathcal{N}-1},Y_{\\mathcal{N}-1})$ are 2D random variables drawn from $p(X,Y)$. \n",
    "\n",
    "   Thus, in your program, you should\n",
    "\n",
    "   - design a Markov chain which generates the sequence $(X_0,Y_0), (X_1, Y_1), \\dots, (X_{\\mathcal{N}-1},Y_{\\mathcal{N}-1})$\n",
    "   - you evaluate the function $f(x,y)$ every time you generate $(X_i,Y_i)$\n",
    "   - Note that you should discard the first $M$ elements in the Markov chain, as they may not be drawn from stationary distribution of the chain.\n",
    "  \n",
    "   Concretely, let's choose the number of sample $\\mathcal{N}=100000$, and discard the first $M=2000$ elements. In otherwords, you need the $\\mathcal{N}+M=102000$ iterations for the Metropolis algorithm. Please also print out the overall acceptance probability, which should be in between 15\\% and 50\\%.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
