{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Roots II\"\n",
    "subtitle: \"Minimization\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "    html-to-math: katex\n",
    "    html-math-method: katex\n",
    "    callout-appearance: minimal\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimization\n",
    "In this lecture, instead of finding function zeros, we are now going to be locating function minima. Note that we will be studying the problem of unconstrained minimization, meaning that we will not be imposing any further constraints on our variables; we are simply looking for the variable values that minimize a scalar function. \n",
    "\n",
    "## One-Dimensional Minimization\n",
    "For simplicity, let's start from the case of a function of a single variable, $\\phi(x)$. As you may recall from elementary calculus, a _stationary point_ (which, for a differentiable function, is also known as a _critical point_) is a point at which the derivative vanishes, namely\n",
    "$$\n",
    "\\phi'(x^*) = 0\n",
    "$$\n",
    "where we are now using $x^*$ to denote the stationary point. If $\\phi''(x^∗) > 0$d we are dealing with a _local minimum_, whereas if $\\phi''(x^*)$< 0 a _local maximum_. Minima and maxima together are known as _extrema_.\n",
    "\n",
    "A simple example is our function $\\phi(x) = e^{x - \\sqrt{x}} - x$. In the previous section we see that it has two zeros at $\\simeq 1$ and $\\simeq 2.5$. We are interested in its minimum, which is located at $x^* \\simeq 1.8$, as can be seen in @fig-function. \n",
    "\n",
    "![The function $f(x) = e^{x - \\sqrt{x}} - x$](week10_files/function.png){#fig-function}\n",
    "\n",
    "It is easy to see that $\\phi''(x^*)>0$ so that is a (single) minimum. To find out this minimum, we can in principle apply the root finding method you learned last time on the function $f(x) = \\phi'(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multidimensional Minimization\n",
    "The problem of multidimensional minimization is, in general, much harder to solve; as the dimensionality grows one cannot even visualize what's going on very effectively. We start with some mathematical aspects of the problem, then turn to a two-dimensional example, and after that discuss specific minimization methods.\n",
    "\n",
    "### General Features\n",
    "Consider a scalar function of many variables, i.e., $\\phi(\\boldsymbol{x})$, where $\\boldsymbol{x}$ bundles together the variables $x_0, x_1,\\dots, x_{n−1}$ but $\\phi$ produces scalar values. We will now employ a multidimensional Taylor expansion.  Also, in order to keep things general, we will not expand around our latest iterate, $\\boldsymbol{x}^{(k−1)}$, since we are not introducing a specific method right now; we are simply trying to explore features of the problem of minimizing $\\phi(x)$.\n",
    "\n",
    "We assume $\\phi(\\boldsymbol{x})$ has bounded first, second, and third derivatives. Then \n",
    "$$\n",
    "\\phi(\\boldsymbol{x} + \\boldsymbol{q}) = \\phi(\\boldsymbol{x}) + \n",
    "(\\nabla\\phi(\\boldsymbol{x}))^T\\boldsymbol{q}\n",
    "+ \\frac{1}{2}\\boldsymbol{q}^T\\boldsymbol{H}(\\boldsymbol{x})\\boldsymbol{q}\n",
    "+ O(\\|\\boldsymbol{q} \\|^3).\n",
    "$$\n",
    "\n",
    "Here, the first-order term involves $\\nabla\\phi(\\boldsymbol{x})$, the _gradient_ vector of $\\phi$ at $\\boldsymbol{x}$. This is \n",
    "$$\n",
    "\\nabla \\phi(\\boldsymbol{x}) = \\left(\\frac{\\partial\\phi}{\\partial x_0} \\quad \n",
    "\\frac{\\partial\\phi}{\\partial x_1} \\quad \\dots \\quad \\frac{\\partial\\phi}{\\partial x_{n-1}} \\right)^T.\n",
    "$$\n",
    "The first order term is simply\n",
    "$$\n",
    "(\\nabla\\phi(\\boldsymbol{x}))^T\\boldsymbol{q}  = \n",
    "\\sum_{j=0}^{n-1} \\frac{\\partial \\phi}{\\partial x_j} q_j = \\nabla\\phi(\\boldsymbol{x})\\cdot \\boldsymbol{q}.\n",
    "$$\n",
    "\n",
    "Note that $\\nabla \\phi(\\boldsymbol{x})$ is the direction of _steepest ascent_, to which we will come back later. \n",
    "To see this, we have for small $\\boldsymbol{q}$ the term linear in $\\boldsymbol{q}$ is the dominant contribution since $\\|\\boldsymbol{q} \\|^2 \\ll \\| \\boldsymbol{q} \\|$. If we choose $\\boldsymbol{q}$ to be aligned with $\\nabla \\phi(\\boldsymbol{x})$, then the dot product $\\nabla\\phi(\\boldsymbol{x})\\cdot \\boldsymbol{q}$ will be maximized.\n",
    "\n",
    "Assuming $\\boldsymbol{x}^∗$ is a local minimum of $\\phi$ and ignoring higher-order terms we have\n",
    "$$\n",
    "\\phi(\\boldsymbol{x}* + \\boldsymbol{q}) \\simeq \\phi(\\boldsymbol{x}^*) \n",
    "+ \\nabla\\phi(\\boldsymbol{x^*})\\cdot \\boldsymbol{q}.\n",
    "$$\n",
    "On the other hand, if we choose $\\boldsymbol{q}$ to be aligned in the direction of $-\\nabla\\phi(\\boldsymbol{x})$, then $\\nabla\\phi(\\boldsymbol{x})\\cdot \\boldsymbol{q}<0$ whenever $\\nabla\\phi(\\boldsymbol{x})\\neq 0$. This then means $\\phi(\\boldsymbol{x}* + \\boldsymbol{q}) < \\phi(\\boldsymbol{x}^*)$, in contradiction to the fact $\\phi(x^*)$ is a local minimum. Because of this, we must have\n",
    "$$\n",
    "\\nabla \\phi(\\boldsymbol{x^*}) = \\boldsymbol{0}\n",
    "$$\n",
    "at local minima (in general extrema or critical points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established that the gradient vector vanishes at a critical point, we now turn to the second-order term in the Taylor expansion, which involves the _Hessian matrix_, $\\boldsymbol{H}(\\boldsymbol{x})$. To see what this is, we expand the quadratic form as follows\n",
    "$$\n",
    "\\frac{1}{2}\\boldsymbol{q}^T \\boldsymbol{H}(\\boldsymbol{x})\\boldsymbol{q}\n",
    "=\\frac{1}{2} \\sum_{i,j = 0}^{n-1} \\frac{\\partial^2\\phi}{\\partial x_i\\partial x_j} q_i q_j.\n",
    "$$\n",
    "This means the matrix element\n",
    "$$\n",
    "H_{ij}(\\boldsymbol{x}) \\equiv (\\boldsymbol{H}(\\boldsymbol{x}))_{ij}\n",
    "=\\frac{\\partial^2\\phi}{\\partial x_i\\partial x_j}.\n",
    "$$\n",
    "\n",
    "Since the lowest order in Taylor expansion is the second order, we have\n",
    "$$\n",
    "\\phi(\\boldsymbol{x}* + \\boldsymbol{q}) \\simeq \\phi(\\boldsymbol{x}^*) \n",
    "+ \\frac{1}{2}\\boldsymbol{q}^T \\boldsymbol{H}(\\boldsymbol{x}^*)\\boldsymbol{q}\n",
    "+ O(\\|\\boldsymbol{q} \\|^3).\n",
    "$$\n",
    "\n",
    "If we now further assume that $\\boldsymbol{H}(\\boldsymbol{x}^∗)$ is _positive definite_ (meaning $\\boldsymbol{v}^T \\boldsymbol{H}\\boldsymbol{v}>0$, $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}$), then we can see that, indeed, $\\phi(\\boldsymbol{x}^* + \\boldsymbol{q}) > \\phi(\\boldsymbol{x}^*)$, as it should, since $\\boldsymbol{x}^*$ is a minimum.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "i. a necessary condition for $x^∗$ being a local minimum is that it be\n",
    "a critical point, i.e., that its gradient vector vanish, and \n",
    "\n",
    "ii. a sufficient condition for the critical point $x^∗$ being a local minimum is that its Hessian matrix be positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Two-Dimensional Example\n",
    "Let us consider\n",
    "$$\n",
    "\\phi(x_0, x_1) = x_0^2 − 2x_0 + x_1^4 - 2x_1^2 + x_1.\n",
    "$$\n",
    "This function is shown in @fig-two-variable.\n",
    "\n",
    "![Example of a scalar function of two variables](week10_files/two_variable.png){#fig-two-variable}\n",
    "\n",
    "This is attempting both to visualize the third dimension and to draw equipotential curves (also known as contour lines).\n",
    "\n",
    "We find that we are dealing with two local minima. The one on the \"right\" leads to smaller/more negative function values, so it appears to be the global minimum.\n",
    "If you place a marble somewhere near these two well, it will roll down to one of the minima; which of the two minima you end up in depends on where you start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "We now turn to a simple and intuitively clear approach to multidimensional minimization. This method, known as gradient descent, does not exhibit great convergence properties and can get in trouble for non-differentiable functions. Even so, it is a pedagogical, straightforward approach.\n",
    "\n",
    "### Algorithm and Interpretation\n",
    "Recall that $\\nabla\\phi(\\boldsymbol{\\boldsymbol{x}})$ is in the direction of steepest ascent. This leads to the conclusion that $-\\nabla\\phi(\\boldsymbol{x})$ is the direction of steepest descent.  We know that choosing $\\boldsymbol{q}$ to point along the negative gradient guarantees that the function value decrease will be the fastest. The method we are about to introduce, which employs $−\\nabla \\phi(\\boldsymbol{x})$, is known as _gradient descent_.\n",
    "Qualitatively, this\n",
    "approach makes use of local information: if you’re exploring a mountainous region (with\n",
    "your eyes closed), you can take a small step downhill at that point; this doesn’t mean\n",
    "that you’re always actually moving in the direction that will most quickly bring you to a\n",
    "(possibly distant) local minimum, simply that you are moving in a downward direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1fa0d82bdabb5288f7efc0788d29c4d5bb5f690328690a3d32d2cd65de760c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
