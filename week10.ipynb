{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Roots II\"\n",
    "subtitle: \"Minimization\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    page-layout: full\n",
    "    fig-cap-location: bottom\n",
    "    number-sections: true\n",
    "    number-depth: 2\n",
    "    html-to-math: katex\n",
    "    html-math-method: katex\n",
    "    callout-appearance: minimal\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimization\n",
    "In this lecture, instead of finding function zeros, we are now going to be locating function minima. Note that we will be studying the problem of unconstrained minimization, meaning that we will not be imposing any further constraints on our variables; we are simply looking for the variable values that minimize a scalar function. \n",
    "\n",
    "## One-Dimensional Minimization\n",
    "For simplicity, let's start from the case of a function of a single variable, $\\phi(x)$. As you may recall from elementary calculus, a _stationary point_ (which, for a differentiable function, is also known as a _critical point_) is a point at which the derivative vanishes, namely\n",
    "$$\n",
    "\\phi'(x^*) = 0\n",
    "$$\n",
    "where we are now using $x^*$ to denote the stationary point. If $\\phi''(x^∗) > 0$d we are dealing with a _local minimum_, whereas if $\\phi''(x^*)$< 0 a _local maximum_. Minima and maxima together are known as _extrema_.\n",
    "\n",
    "A simple example is our function $\\phi(x) = e^{x - \\sqrt{x}} - x$. In the previous section we see that it has two zeros at $\\simeq 1$ and $\\simeq 2.5$. We are interested in its minimum, which is located at $x^* \\simeq 1.8$, as can be seen in @fig-function. \n",
    "\n",
    "![The function $f(x) = e^{x - \\sqrt{x}} - x$](week10_files/function.png){#fig-function}\n",
    "\n",
    "It is easy to see that $\\phi''(x^*)>0$ so that is a (single) minimum. To find out this minimum, we can in principle apply the root finding method you learned last time on the function $f(x) = \\phi'(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multidimensional Minimization\n",
    "The problem of multidimensional minimization is, in general, much harder to solve; as the dimensionality grows one cannot even visualize what's going on very effectively. We start with some mathematical aspects of the problem, then turn to a two-dimensional example, and after that discuss specific minimization methods.\n",
    "\n",
    "### General Features\n",
    "Consider a scalar function of many variables, i.e., $\\phi(\\boldsymbol{x})$, where $\\boldsymbol{x}$ bundles together the variables $x_0, x_1,\\dots, x_{n−1}$ but $\\phi$ produces scalar values. We will now employ a multidimensional Taylor expansion.  Also, in order to keep things general, we will not expand around our latest iterate, $\\boldsymbol{x}^{(k−1)}$, since we are not introducing a specific method right now; we are simply trying to explore features of the problem of minimizing $\\phi(x)$.\n",
    "\n",
    "We assume $\\phi(\\boldsymbol{x})$ has bounded first, second, and third derivatives. Then \n",
    "$$\n",
    "\\phi(\\boldsymbol{x} + \\boldsymbol{q}) = \\phi(\\boldsymbol{x}) + \n",
    "(\\nabla\\phi(\\boldsymbol{x}))^T\\boldsymbol{q}\n",
    "+ \\frac{1}{2}\\boldsymbol{q}^T\\boldsymbol{H}(\\boldsymbol{x})\\boldsymbol{q}\n",
    "+ O(\\|\\boldsymbol{q} \\|^3).\n",
    "$$\n",
    "\n",
    "Here, the first-order term involves $\\nabla\\phi(\\boldsymbol{x})$, the _gradient_ vector of $\\phi$ at $\\boldsymbol{x}$. This is \n",
    "$$\n",
    "\\nabla \\phi(\\boldsymbol{x}) = \\left(\\frac{\\partial\\phi}{\\partial x_0} \\quad \n",
    "\\frac{\\partial\\phi}{\\partial x_1} \\quad \\dots \\quad \\frac{\\partial\\phi}{\\partial x_{n-1}} \\right)^T.\n",
    "$$\n",
    "The first order term is simply\n",
    "$$\n",
    "(\\nabla\\phi(\\boldsymbol{x}))^T\\boldsymbol{q}  = \n",
    "\\sum_{j=0}^{n-1} \\frac{\\partial \\phi}{\\partial x_j} q_j = \\nabla\\phi(\\boldsymbol{x})\\cdot \\boldsymbol{q}.\n",
    "$$\n",
    "\n",
    "Note that $\\nabla \\phi(\\boldsymbol{x})$ is the direction of _steepest ascent_, to which we will come back later. \n",
    "To see this, we have for small $\\boldsymbol{q}$ the term linear in $\\boldsymbol{q}$ is the dominant contribution since $\\|\\boldsymbol{q} \\|^2 \\ll \\| \\boldsymbol{q} \\|$. If we choose $\\boldsymbol{q}$ to be aligned with $\\nabla \\phi(\\boldsymbol{x})$, then the dot product $\\nabla\\phi(\\boldsymbol{x})\\cdot \\boldsymbol{q}$ will be maximized.\n",
    "\n",
    "Assuming $\\boldsymbol{x}^∗$ is a local minimum of $\\phi$ and ignoring higher-order terms we have\n",
    "$$\n",
    "\\phi(\\boldsymbol{x}* + \\boldsymbol{q}) \\simeq \\phi(\\boldsymbol{x}^*) \n",
    "+ \\nabla\\phi(\\boldsymbol{x^*})\\cdot \\boldsymbol{q}.\n",
    "$$\n",
    "On the other hand, if we choose $\\boldsymbol{q}$ to be aligned in the direction of $-\\nabla\\phi(\\boldsymbol{x})$, then $\\nabla\\phi(\\boldsymbol{x})\\cdot \\boldsymbol{q}<0$ whenever $\\nabla\\phi(\\boldsymbol{x})\\neq 0$. This then means $\\phi(\\boldsymbol{x}* + \\boldsymbol{q}) < \\phi(\\boldsymbol{x}^*)$, in contradiction to the fact $\\phi(x^*)$ is a local minimum. Because of this, we must have\n",
    "$$\n",
    "\\nabla \\phi(\\boldsymbol{x^*}) = \\boldsymbol{0}\n",
    "$$\n",
    "at local minima (in general extrema or critical points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established that the gradient vector vanishes at a critical point, we now turn to the second-order term in the Taylor expansion, which involves the _Hessian matrix_, $\\boldsymbol{H}(\\boldsymbol{x})$. To see what this is, we expand the quadratic form as follows\n",
    "$$\n",
    "\\frac{1}{2}\\boldsymbol{q}^T \\boldsymbol{H}(\\boldsymbol{x})\\boldsymbol{q}\n",
    "=\\frac{1}{2} \\sum_{i,j = 0}^{n-1} \\frac{\\partial^2\\phi}{\\partial x_i\\partial x_j} q_i q_j.\n",
    "$$\n",
    "This means the matrix element\n",
    "$$\n",
    "H_{ij}(\\boldsymbol{x}) \\equiv (\\boldsymbol{H}(\\boldsymbol{x}))_{ij}\n",
    "=\\frac{\\partial^2\\phi}{\\partial x_i\\partial x_j}.\n",
    "$$\n",
    "\n",
    "Since the lowest order in Taylor expansion is the second order, we have\n",
    "$$\n",
    "\\phi(\\boldsymbol{x}* + \\boldsymbol{q}) \\simeq \\phi(\\boldsymbol{x}^*) \n",
    "+ \\frac{1}{2}\\boldsymbol{q}^T \\boldsymbol{H}(\\boldsymbol{x}^*)\\boldsymbol{q}\n",
    "+ O(\\|\\boldsymbol{q} \\|^3).\n",
    "$$\n",
    "\n",
    "If we now further assume that $\\boldsymbol{H}(\\boldsymbol{x}^∗)$ is _positive definite_ (meaning $\\boldsymbol{v}^T \\boldsymbol{H}\\boldsymbol{v}>0$, $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}$), then we can see that, indeed, $\\phi(\\boldsymbol{x}^* + \\boldsymbol{q}) > \\phi(\\boldsymbol{x}^*)$, as it should, since $\\boldsymbol{x}^*$ is a minimum.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "i. a necessary condition for $x^∗$ being a local minimum is that it be\n",
    "a critical point, i.e., that its gradient vector vanish, and \n",
    "\n",
    "ii. a sufficient condition for the critical point $x^∗$ being a local minimum is that its Hessian matrix be positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Two-Dimensional Example\n",
    "Let us consider\n",
    "$$\n",
    "\\phi(x_0, x_1) = x_0^2 − 2x_0 + x_1^4 - 2x_1^2 + x_1.\n",
    "$$\n",
    "This function is shown in @fig-two-variable.\n",
    "\n",
    "![Example of a scalar function of two variables](week10_files/two_variable.png){#fig-two-variable}\n",
    "\n",
    "This is attempting both to visualize the third dimension and to draw equipotential curves (also known as contour lines).\n",
    "\n",
    "We find that we are dealing with two local minima. The one on the \"right\" leads to smaller/more negative function values, so it appears to be the global minimum.\n",
    "If you place a marble somewhere near these two well, it will roll down to one of the minima; which of the two minima you end up in depends on where you start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "We now turn to a simple and intuitively clear approach to multidimensional minimization. This method, known as gradient descent, does not exhibit great convergence properties and can get in trouble for non-differentiable functions. Even so, it is a pedagogical, straightforward approach.\n",
    "\n",
    "### Algorithm and Interpretation\n",
    "Recall that $\\nabla\\phi(\\boldsymbol{\\boldsymbol{x}})$ is in the direction of steepest ascent. This leads to the conclusion that $-\\nabla\\phi(\\boldsymbol{x})$ is the direction of steepest descent.  We know that choosing $\\boldsymbol{q}$ to point along the negative gradient guarantees that the function value decrease will be the fastest. The method we are about to introduce, which employs $-\\nabla \\phi(\\boldsymbol{x})$, is known as _gradient descent_. \n",
    "\n",
    "Qualitatively, this approach makes use of local information: if you're exploring a mountainous region (with your eyes closed), you can take a small step downhill at that point; this doesn't mean that you're always actually moving in the direction that will most quickly bring you to a (possibly distant) local minimum, simply that you are moving in a downward direction. \n",
    "\n",
    "Implicit in our discussion above is the fact that the steps we make will be _small_: while $-\\nabla \\phi(\\boldsymbol{x})$ helps you pick the direction, it doesn't tell you how far in that direction you should go, i.e., how large a $\\|\\boldsymbol{q} \\|$ you should employ. The simplest possible choice, analogously to our closed-eyes example, is to make small fixed steps, quantified by a parameter $\\gamma$. This leads to the following prescription:\n",
    "$$\n",
    "\\boldsymbol{x}^{(k)} = \\boldsymbol{x}^{(k-1)} -\\gamma \\nabla \\phi(\\boldsymbol{x}^{(k-1)}).\n",
    "$$\n",
    "At each step, this method picks the direction that is perpendicular to the contour line, as illustrated in @fig-gradient-descent.\n",
    "\n",
    "![Illustrate of gradient descent](week10_files/Gradient_Descent.png){#fig-gradient-descent}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we may not know the gradient analytically, and we typically approximate it using a forward-difference scheme. In equation form, this means that\n",
    "$$\n",
    "\\nabla\\phi(\\boldsymbol{x}) = \\begin{pmatrix}\n",
    "[\\phi(\\boldsymbol{x} + \\boldsymbol{e}_0 h) - \\phi(\\boldsymbol{x})]/h\\\\\n",
    "[\\phi(\\boldsymbol{x} + \\boldsymbol{e}_1 h) - \\phi(\\boldsymbol{x})]/h\\\\\n",
    "\\cdots \\\\\n",
    "[\\phi(\\boldsymbol{x} + \\boldsymbol{e}_{n-1} h) - \\phi(\\boldsymbol{x})]/h\\\\\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "for a given spacing h. This involves \"only\" $n+1$ function evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Note that in the function `gradient()`, we used `xs*np.ones((n,n)).T`. The meaning of this expression can be understood in the following way.\n",
    "\n",
    "- Let's say `xs=np.array([x0,x1])`\n",
    "- Then `xs*np.ones((2,2)) = np.array([[x0,x1],[x0,x1]])`\n",
    "- Thus,`(xs*np.ones((2,2))).T = np.array([[x0,x0],[x1,x1]])`\n",
    "\n",
    "Then, `Xph = (xs*np.ones((n,n))).T + np.identity(n)*h = np.array([[x0+h,x0],[x1,x1+h]])`.\n",
    "\n",
    "When `Xph` is inserted into `phi()`, we have `x0,x1 = Xph` which leads to `x0 = np.array([x0+h,x0])`, `x1 = np.array([x1,x1+h])`. Then the function `phi(Xph)` returns a vector\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\phi(\\boldsymbol{x}+\\boldsymbol{e}_0 h) \\\\\n",
    "\\phi(\\boldsymbol{x}+\\boldsymbol{e}_1 h) .\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1.69999985 0.24062524] 0.21543067857184484 -0.38182351336797915\n",
      "2 [1.48999974 0.22664124] 0.20264073065897387 -0.6333530209502826\n",
      "3 [1.34299967 0.20564122] 0.21157625945177916 -0.7594983275463574\n",
      "4 [1.24009962 0.17380848] 0.2661256195506861 -0.8280498618293368\n",
      "5 [1.16806958 0.12494345] 0.45276305522029103 -0.8777871985954815\n",
      "6 [1.11764856 0.04873952] 1.608607274900684 -0.9421647380026318\n",
      "7 [ 1.08235384 -0.07208595] 1.7087398591446958 -1.0756695552415236\n",
      "8 [ 1.05764754 -0.26511247] 0.7514526354432409 -1.3974185387806914\n",
      "9 [ 1.04035313 -0.56299971] 0.5457308766418241 -2.0948395460096236\n",
      "10 [ 1.02824704 -0.94372756] 0.41520335042339795 -2.9309660540915905\n",
      "11 [ 1.01977278 -1.15566205] 0.19169789499758141 -3.0426740826911614\n",
      "12 [ 1.01384079 -1.0729902 ] 0.08289908836262229 -3.0499045321707055\n",
      "13 [ 1.00968841 -1.12557975] 0.05083473301209256 -3.054234380060766\n",
      "14 [ 1.00678173 -1.09531014] 0.03052274820652257 -3.05538233943229\n",
      "15 [ 1.00474706 -1.11406803] 0.018862352410894102 -3.05589334211737\n",
      "16 [ 1.00332279 -1.10287596] 0.011567627294141984 -3.056063921191618\n",
      "17 [ 1.00232581 -1.1097221 ] 0.007163907055812726 -3.056132246972263\n",
      "18 [ 1.00162791 -1.10559374] 0.004430823076617533 -3.056157117969501\n",
      "19 [ 1.00113939 -1.10810556] 0.0027547414719851855 -3.0561667943226265\n",
      "20 [ 1.00079742 -1.10658539] 0.0017154484765218943 -3.0561704829239886\n",
      "21 [ 1.00055805 -1.10750841] 0.0010726689437897882 -3.0561719231405475\n",
      "22 [ 1.00039048 -1.10694906] 0.0006728037904015246 -3.056172494841105\n",
      "23 [ 1.00027319 -1.10728843] 0.0004237463185425745 -3.056172722103985\n",
      "24 [ 1.00019108 -1.10708268] 0.00026793975693115976 -3.0561728168266145\n",
      "25 [ 1.00013361 -1.10720748] 0.00017018001506541433 -3.0561728552582403\n",
      "26 [ 1.00009337 -1.1071318 ] 0.00010858056296563575 -3.0561728723063135\n",
      "27 [ 1.00006521 -1.1071777 ] 6.961331368515191e-05 -3.0561728792903975\n",
      "28 [ 1.0000455  -1.10714986] 4.485147212742855e-05 -3.0561728826380756\n",
      "29 [ 1.0000317  -1.10716674] 2.9044432169820975e-05 -3.056172883986762\n",
      "30 [ 1.00002204 -1.10715651] 1.8904903190473045e-05 -3.0561728846981224\n",
      "31 [ 1.00001528 -1.10716272] 1.236866979687839e-05 -3.056172884967764\n",
      "32 [ 1.00001054 -1.10715895] 8.133575043645464e-06 -3.0561728851287384\n",
      "33 [ 1.00000723 -1.10716123] 5.37538599036186e-06 -3.056172885182245\n",
      "34 [ 1.00000491 -1.10715985] 3.5698772250060277e-06 -3.0561728852203363\n",
      "35 [ 1.00000329 -1.10716069] 2.3819480385739737e-06 -3.056172885230078\n",
      "36 [ 1.00000215 -1.10716018] 1.5963972365386816e-06 -3.056172885239327\n",
      "37 [ 1.00000136 -1.10716049] 1.0743651801166516e-06 -3.0561728852405894\n",
      "38 [ 1.0000008 -1.1071603] 7.259538200711832e-07 -3.0561728852428383\n",
      "39 [ 1.00000041 -1.10716041] 4.923441634847997e-07 -3.0561728852427184\n",
      "40 [ 1.00000014 -1.10716035] 3.3505999120764203e-07 -3.0561728852432424\n",
      "41 [ 0.99999995 -1.10716039] 2.287712532568859e-07 -3.0561728852430368\n",
      "42 [ 0.99999981 -1.10716036] 1.564959858869905e-07 -3.0561728852431402\n",
      "43 [ 0.99999972 -1.10716038] 1.0724305579788708e-07 -3.056172885243015\n",
      "44 [ 0.99999965 -1.10716037] 7.378388107643756e-08 -3.0561728852430234\n",
      "45 [ 0.99999961 -1.10716037] 5.101074560430894e-08 -3.056172885242959\n",
      "46 [ 0.99999957 -1.10716037] 3.5236294141767044e-08 -3.0561728852429515\n",
      "47 [ 0.99999955 -1.10716037] 2.431386477902596e-08 -3.056172885242921\n",
      "48 [ 0.99999954 -1.10716037] 1.6863918806819815e-08 -3.0561728852429133\n",
      "49 [ 0.99999953 -1.10716037] 1.1653038863784987e-08 -3.056172885242899\n",
      "50 [ 0.99999952 -1.10716037] 8.088151816259404e-09 -3.0561728852428938\n",
      "[ 0.99999952 -1.10716037]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def phi(xs):\n",
    "    x0, x1 = xs\n",
    "    return x0**2 - 2*x0 + x1**4 - 2*x1**2 + x1\n",
    "\n",
    "def gradient(phi,xs,h=1.e-6):\n",
    "    n = xs.size\n",
    "    phi0 = phi(xs)\n",
    "    Xph = (xs*np.ones((n,n))).T + np.identity(n)*h\n",
    "    grad = (phi(Xph) - phi0)/h\n",
    "    return grad\n",
    "\n",
    "def descent(phi,gradient,xolds,gamma=0.15,kmax=200,tol=1.e-8):\n",
    "    for k in range(1,kmax):\n",
    "        xnews = xolds - gamma*gradient(phi,xolds)\n",
    "\n",
    "        err = termcrit(xolds,xnews)\n",
    "        print(k, xnews, err, phi(xnews))\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        xolds = np.copy(xnews)\n",
    "    else:\n",
    "        xnews = None\n",
    "    return xnews\n",
    "\n",
    "def termcrit(xolds,xnews):\n",
    "    errs = np.abs((xnews - xolds)/xnews)\n",
    "    return np.sum(errs)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    xolds = np.array([2.,0.25])\n",
    "    xnews = descent(phi, gradient, xolds)\n",
    "    print(xnews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Newton's Method\n",
    "Gradient descent is nice and simple but, as we will see in the problem set, either your $\\gamma$ is very small and you waste iterations or you are carrying out a line-search at each step to determine the optimal $γ^{(k)}$, which is starting to get costly. \n",
    "\n",
    "A distinct approach goes as follows: instead of using only the value of the gradient at a given point, perhaps we should be building in more information. \n",
    "Specifically, we have that gradient vanishes at a critical point\n",
    "$$\n",
    "\\nabla\\phi(\\boldsymbol{x}^*) = \\boldsymbol{0}.\n",
    "$$\n",
    "\n",
    "Combine that with the fact that the gradient of a scalar function is a column vector, and you can recast that equation as a set of $n$ coupled nonlinear equations\n",
    "$$\n",
    "\\boldsymbol{f}(\\boldsymbol{x}) = \\nabla\\phi(\\boldsymbol) = \\boldsymbol{0}.\n",
    "$$\n",
    "\n",
    "In other words, finding a critical point is simply a special case of solving a nonlinear system of equations. We shall apply the Newton's method in the following to find out the critical point. \n",
    "\n",
    "### Map to a finding roots of multiple nonlinear equations\n",
    "We assume that $\\boldsymbol{f}$ has bounded first and second derivatives; the actual solution of our problem $\\boldsymbol{f}(\\boldsymbol{x})=\\boldsymbol{0}$ is $\\boldsymbol{x}^∗$ and we will be trying to approximate it using iterates, which this time are themselves vectors, $\\boldsymbol{x}^{(k)}$.\n",
    "In order to make the transition to the general problem as simple as possible, let's start from a Taylor expansion of a single function component $f_i$ around our latest iterate $\\boldsymbol{x}^{(k-1)}$:\n",
    "$$\n",
    "f_i(\\boldsymbol{x}) = f_i(\\boldsymbol{x}^{(k-1)}) + (\\nabla f_i(\\boldsymbol{x}^{(k-1)}))^T(\\boldsymbol{x} - \\boldsymbol{x}^{(k-1)}) \n",
    "+ O(\\|\\boldsymbol{x} - \\boldsymbol{x}^{(k-1)} \\|^2),\n",
    "$$\n",
    "where $i = 0, 1, \\dots, n-1$ is the index for each component of $\\boldsymbol{f}$. We can rewrite the second term on the right-hand side in terms of vector components\n",
    "$$\n",
    "(\\nabla f_i(\\boldsymbol{x}^{(k-1)}))^T(\\boldsymbol{x} - \\boldsymbol{x}^{(k-1)})\n",
    "=\n",
    "\\sum_{j=0}^{n-1}\\left. \\frac{\\partial f_i}{\\partial x_i}\\right|_{x_j^{(k-1)}}(x_j - x_j^{(k-1)}).\n",
    "$$\n",
    "\n",
    "With a view to collecting the n function components together, we now introduce the _Jacobian matrix_:\n",
    "$$\n",
    "\\boldsymbol{J}(\\boldsymbol{x}) = \\left\\{\\frac{\\partial f_i}{\\partial x_j} \\right\\} \n",
    "= \\begin{pmatrix}\n",
    "\\frac{\\partial f_0}{\\partial x_0} &\\frac{\\partial f_0}{\\partial x_1} &\\cdots &\\frac{\\partial f_0}{\\partial x_{n-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_0} &\\frac{\\partial f_1}{\\partial x_1} &\\cdots &\\frac{\\partial f_1}{\\partial x_{n-1}} \\\\\n",
    "\\vdots &\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_{n-1}}{\\partial x_0} &\\frac{\\partial f_{n-1}}{\\partial x_1} &\\cdots &\\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We can thus write\n",
    "$$\n",
    "\\boldsymbol{f}(\\boldsymbol{x}) = \\boldsymbol{f}(\\boldsymbol{x}^{(k-1)}) + \\boldsymbol{J}(\\boldsymbol{x}^{(k-1)})(\\boldsymbol{x} - \\boldsymbol{x}^{(k-1)})\n",
    "+ O(\\|\\boldsymbol{x} - \\boldsymbol{x}^{(k-1)} \\|^2).\n",
    "$$\n",
    "\n",
    "The solution correspoding to $\\boldsymbol{f}(\\boldsymbol{x}^*)=\\boldsymbol{0}$ satisfies\n",
    "$$\n",
    "\\boldsymbol{0} = \\boldsymbol{f}(\\boldsymbol{x}^{(k-1)}) + \\boldsymbol{J}(\\boldsymbol{x}^{(k-1)})(\\boldsymbol{x}^* - \\boldsymbol{x}^{(k-1)}),\n",
    "$$\n",
    "where we neglected the second order terms.\n",
    "\n",
    "In practice, one iteration will not be enough to find the solution so, instead, we use our latest formula to introduce the prescription of _Newton's method_ for the next iterate, $x^{(k)}$:\n",
    "$$\n",
    "\\boldsymbol{J}(\\boldsymbol{x}^{(k-1)})(\\boldsymbol{x}^{(k)}-\\boldsymbol{x}^{(k-1)}) = -\\boldsymbol{f}(\\boldsymbol{x}^{(k-1)}).\n",
    "$$\n",
    "\n",
    "When we are minimizing a function $\\phi(\\boldsymbol{x})$, we take the above function $\\boldsymbol{f}(\\boldsymbol{x}) = \\nabla\\phi(\\boldsymbol{\\phi})$. Thus, the Jacobian\n",
    "$$\n",
    "\\boldsymbol{J}(\\boldsymbol{x}) = \\boldsymbol{H}(\\boldsymbol{x}),\n",
    "$$\n",
    "where $\\boldsymbol{H}(\\boldsymbol{x})$ is the Hessian matrix at point $\\boldsymbol{x}$. We hence have\n",
    "$$\n",
    "\\boldsymbol{H}(\\boldsymbol{x}^{(k-1)})(\\boldsymbol{x}^{(k)}-\\boldsymbol{x}^{(k-1)}) = - \\nabla\\phi(\\boldsymbol{\\phi}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all quantities at the location of our previous iterate, $x^{(k−1)}$, are known, this equation has the form of $\\boldsymbol{Ax} = \\boldsymbol{b}$, i.e., it is a linear system of $n$ equations in $n$ unknowns. Assuming $\\boldsymbol{J}(\\boldsymbol{x}^{(k−1)})$ is non-singular, we can solve this system and then we will be able to find the\n",
    "$\\boldsymbol{x}^{(k)}$. This process is repeated, until we satisfy a termination criterion, which could be taken to be \n",
    "$$\n",
    "\\sum_{j=0}^{n-1} \\left|\\frac{x_j^{(k)}-x_j^{(k-1)}}{x_j^{(k)}} \\right| \\leq \\epsilon.\n",
    "$$\n",
    "\n",
    "In the following, we shall apply the Newton's method to a physics problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Extremizing the Action in Classical Mechanics\n",
    "## Defining and Extremizing the Action\n",
    "Let us study a single particle in one dimension. We can denote the particle's location by $x(t)$, where we're explicitly showing that the position is a function of time. \n",
    "\n",
    "The kinetic energy of the particle will be a function of only $\\dot{x}(t)$, i.e., of the time derivative of the position: $K = K(\\dot{x}(t))$. Specifically, since we are dealing with a single particle, we know that:\n",
    "$$\n",
    "K = \\frac{1}{2} m\\dot{x}^2\n",
    "$$\n",
    "where $m$ is the mass of the particle. Similarly, in the absence of time-dependent external fields,  the potential energy is a function of only $x(t)$: $V = V(x(t))$. The difference of these two quantities is defined as the _Lagrangian_:\n",
    "$$\n",
    "L(x(t),\\dot{x}(t)) \\equiv K(\\dot{x}(t)) - V(x(t)),\n",
    "$$\n",
    "where, for our case, there is no explicit dependence of the Lagrangian on time. \n",
    "\n",
    "We are interested in studying the particle from time $t = 0$ to time $t = T$. Then, one can define the _action functional_ as the integral of the Lagrangian over time:\n",
    "$$\n",
    "S[x(t)] = \\int_0^T dt \\, L(x(t),\\dot{x}(t)) = \\int_0^T dt\\, \\left(\\frac{1}{2}m\\dot{x}^2  - V(x)\\right).\n",
    "$$\n",
    "\n",
    "Notice that we called the action a _functional_ and used square brackets on the left-hand side. Roughly speaking, a _functional_ is a function of a function. A reminder: an ordinary function $\\phi$ takes us from one number $t$ to another number $\\phi(t)$. A functional is an entity that takes in an entire function and gives back a number. In other words, a functional is a mapping from a space of functions into the real (or complex) numbers.\n",
    "\n",
    "In case you haven't encountered functionals before, let us start with a simple case: a functional $F$ of $\\phi(t)$ (where $t$ is a regular variable -- this is a one-dimensional problem): $F[\\phi(t)]$. Being a functional, $F$ depends simultaneously on the values of $\\phi$ at all points $t$ but it does not depend on $t$ itself: we provide it with the entire function and it provides us with one number as a result. A trivial example: $F[\\phi] =\\int_1^0 dt\\phi(t)$ gives us one number for $\\phi(t) = t$ and a different number for $\\phi(t) = t^2$. In both cases, however, the answer is an ordinary number that does not depend on $t$.\n",
    "\n",
    "Applied to our mechanics problem, we see that the action $S$ depends on the position of the particle $x(t)$ at all times from $0$ to $T$, but not on $t$ directly, since $t$ has been \"integrated out”. For a given trajectory $x(t)$ from $t = 0$ to $t = T$, the action produces a single number, $S$. The question then arises: which trajectory $x(t)$ from $t = 0$ to time $t = T$ does the particle actually \"choose\"? The answer comes from Hamilton's principle: of all possible paths, the path that is actually followed is that which minimizes the action. As it so happens, the action only needs to be stationary, i.e., we are extremizing and not necessarily minimizing,\n",
    "but we will usually be dealing with a minimum. This extremization is taking place with the endpoints kept fixed, i.e., $x(0)$ and $x(T)$ are not free to vary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Action\n",
    "We will assume the positions $x(t)$ from $t = 0$ to $t = T$ can be accessed only at a discrete set of $n$ points:\n",
    "$$\n",
    "t_k = k\\Delta t = k\\frac{T}{n-1}\n",
    "$$\n",
    "where $k = 1, 2, \\dots, n-1$. We will denote $x_k \\equiv x(t_k)$ to denote the possible position of the particle at $t = t_k$.\n",
    "After we discretize the time, the action functional can be approximated as\n",
    "$$\n",
    "S_n \\equiv \\sum_{k=0}^{n-2} \\Delta t\\left[ \\frac{1}{2}m \\left(\\frac{x_{k+1} - x_{k}}{\\Delta t}\\right)^2 - V(x_k)\\right],\n",
    "$$\n",
    "in which the integral is replaced by summation of small rectangles. \n",
    "\n",
    "Since we want to extremize the action with end points fixed, we should fix $x_0$ and $x_{n-1}$ in our case. Thus, we shall consider the following function\n",
    "$$\n",
    "S_n = S_n(x_1, x_2, \\dots, x_{n-2}), \n",
    "$$\n",
    "and we will be finding a minimum of $S_n$ in the $(n-2)$-dimensional space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton’s Method for the Discrete Action\n",
    "We shall employ the Newton's method for this multidimensional minimization problem. To apply this method, we require the gradient vecotr as well as the Hessian. \n",
    "\n",
    "We can compute the gradient\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial S_n}{\\partial x_i} &= \n",
    "\\sum_{k=0}^{n-2}\\Delta t\\left[ \\frac{m}{\\Delta t}^2(x_{k+1} - x_k)(\\delta_{i,k+1} - \\delta_{i,k}) - \\frac{\\partial V(x_k)}{\\partial x_i}\\delta_{i,k} \\right] \\\\\n",
    "&= \\frac{m}{\\Delta t} (2 x_i - x_{i-1} - x_{i+1}) - \\Delta t\\frac{\\partial V(x_i)}{\\partial x_i}\n",
    "\\end{align*}.\n",
    "$$\n",
    "\n",
    "Similarly, the Hessian\n",
    "$$\n",
    "H_{ji} = \\frac{\\partial^2 S_n}{\\partial x_j \\partial x_i}\n",
    "= \\frac{m}{\\Delta t} (2\\delta_{j,i} - \\delta_{j,i-1} - \\delta_{j, i+1})\n",
    "- \\Delta t \\frac{\\partial^2 V(x_i)}{\\partial x_i^2} \\delta_{j,i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Let us pick a specific form for the potential energy \n",
    "$$\n",
    "V = \\frac{1}{4}x^4\n",
    "$$\n",
    "which describes a _quartic oscillator_. \n",
    "\n",
    "With this potential, we can compute its first and second order derivatives analytically:\n",
    "$$\n",
    "F(x) = -\\frac{\\partial V}{\\partial x} = -x^3, \\quad \n",
    "F'(x) = -\\frac{\\partial^2 V}{\\partial x^2} = -3x^2.\n",
    "$$\n",
    "\n",
    "With this, we have the gradient vector\n",
    "$$\n",
    "\\frac{\\partial S_n}{\\partial x_i} = \\frac{m}{\\Delta t} (2 x_i - x_{i-1} - x_{i+1}) - \\Delta t x_i^3\n",
    "$$\n",
    "or in a vector form\n",
    "$$\n",
    "\\nabla S_n = \n",
    "\\begin{pmatrix}\n",
    "\\frac{m}{\\Delta t} (2 x_1 - x_{0} - x_{2}) - \\Delta t x_1^3 \\\\\n",
    "\\frac{m}{\\Delta t} (2 x_2 - x_{1} - x_{3}) - \\Delta t x_2^3 \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{m}{\\Delta t} (2 x_{n-2} - x_{n-3} - x_{n-1}) - \\Delta t x_{n-2}^3 \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "We also have the Hessian\n",
    "$$\n",
    "H_{ji} = \\frac{\\partial^2 S_n}{\\partial x_j \\partial x_i}\n",
    "= \\frac{m}{\\Delta t} (2\\delta_{j,i} - \\delta_{j,i-1} - \\delta_{j, i+1})\n",
    "- 3\\Delta t x_i^2 \\delta_{j,i} \n",
    "$$\n",
    "or in matrix form\n",
    "$$\n",
    "\\boldsymbol{H} = \\begin{pmatrix}\n",
    "\\frac{2m}{\\Delta t} - 3\\Delta t x_1^2 &-\\frac{m}{\\Delta t} & 0 &\\cdots &0\\\\\n",
    "-\\frac{m}{\\Delta t} &\\frac{2m}{\\Delta t} - 3\\Delta t x_2^2 &-\\frac{m}{\\Delta t} & \\cdots & 0\\\\\n",
    "0 &-\\frac{m}{\\Delta t} &\\frac{2m}{\\Delta t} - 3\\Delta t x_3^2 & \\ddots & 0\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots &-\\frac{m}{\\Delta t} &\\frac{2m}{\\Delta t} - 3\\Delta t x_{n-2}^2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "We see that the Hessian matrix is a _tri-diagonal matrix_, in which the main diagonal consists of $\\frac{2m}{\\Delta t} - 3\\Delta t x_i^2$ with $i=1,2,\\dots,n-2$. The two neighboring _subdiagonals_ are both made of $-\\frac{m}{\\Delta t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [2.00243064 2.00405866 2.00488295 2.00490384 2.00412305 2.00254359\n",
      " 2.00016973 1.9970069  1.99306166 1.98834158 1.98285524 1.9766121\n",
      " 1.96962249 1.96189752 1.95344901 1.94428947 1.93443198 1.92389019\n",
      " 1.91267825 1.90081072 1.88830256 1.87516909 1.86142587 1.84708874\n",
      " 1.83217371 1.81669697 1.8006748  1.78412356 1.76705965 1.74949946\n",
      " 1.73145938 1.71295568 1.6940046  1.67462221 1.65482445 1.6346271\n",
      " 1.61404573 1.59309569 1.57179212 1.55014988 1.52818357 1.50590752\n",
      " 1.48333574 1.46048195 1.43735953 1.41398156 1.39036074 1.36650947\n",
      " 1.34243976 1.31816329 1.29369138 1.26903496 1.24420463 1.2192106\n",
      " 1.19406273 1.1687705  1.14334304 1.1177891  1.09211709 1.06633505\n",
      " 1.04045067 1.0144713  0.98840394 0.96225525 0.93603156 0.90973887\n",
      " 0.88338287 0.85696893 0.83050211 0.80398719 0.77742863 0.75083064\n",
      " 0.72419714 0.69753177 0.67083794 0.6441188  0.61737725 0.59061599\n",
      " 0.56383745 0.5370439  0.51023736 0.4834197  0.45659257 0.42975747\n",
      " 0.40291572 0.3760685  0.34921681 0.32236157 0.29550351 0.26864329\n",
      " 0.24178145 0.21491843 0.18805458 0.16119016 0.1343254  0.10746044\n",
      " 0.08059537 0.05373025 0.02686513] 20.58576209557278\n",
      "2 [2.00641498 2.01202226 2.01681505 2.02078758 2.02393506 2.02625371\n",
      " 2.02774078 2.02839456 2.02821437 2.02720058 2.02535461 2.02267892\n",
      " 2.01917696 2.01485324 2.00971323 2.00376337 1.99701108 1.98946466\n",
      " 1.98113333 1.97202715 1.962157   1.95153453 1.94017215 1.92808295\n",
      " 1.91528067 1.90177966 1.88759483 1.8727416  1.85723584 1.84109384\n",
      " 1.82433226 1.80696806 1.78901849 1.77050101 1.75143324 1.73183293\n",
      " 1.71171793 1.69110612 1.67001536 1.64846349 1.62646824 1.60404726\n",
      " 1.581218   1.55799776 1.53440359 1.51045232 1.48616048 1.46154432\n",
      " 1.43661973 1.41140231 1.38590724 1.36014936 1.33414308 1.30790243\n",
      " 1.28144099 1.25477193 1.22790797 1.20086138 1.173644   1.14626718\n",
      " 1.11874184 1.09107845 1.063287   1.03537704 1.00735768 0.97923755\n",
      " 0.95102487 0.92272743 0.89435256 0.86590719 0.83739785 0.80883065\n",
      " 0.78021131 0.75154518 0.72283722 0.69409206 0.66531395 0.63650682\n",
      " 0.6076743  0.57881966 0.54994593 0.5210558  0.49215175 0.46323595\n",
      " 0.43431035 0.40537669 0.37643646 0.34749098 0.31854137 0.28958857\n",
      " 0.26063339 0.23167645 0.20271829 0.17375932 0.14479982 0.11584003\n",
      " 0.08688008 0.05792007 0.02896003] 5.506719009111121\n",
      "3 [2.00682081 2.01283341 2.01803051 2.02240578 2.02595386 2.0286704\n",
      " 2.03055203 2.03159645 2.03180236 2.0311695  2.02969866 2.02739166\n",
      " 2.02425135 2.0202816  2.01548729 2.00987426 2.00344935 1.99622032\n",
      " 1.98819584 1.97938548 1.96979963 1.95944951 1.94834711 1.93650514\n",
      " 1.92393701 1.91065678 1.89667909 1.88201913 1.86669261 1.85071568\n",
      " 1.83410491 1.8168772  1.7990498  1.78064016 1.761666   1.74214517\n",
      " 1.72209564 1.70153546 1.6804827  1.65895542 1.63697163 1.61454924\n",
      " 1.59170603 1.56845961 1.54482739 1.52082654 1.49647399 1.47178636\n",
      " 1.44677997 1.42147078 1.39587442 1.37000612 1.34388072 1.31751265\n",
      " 1.29091592 1.2641041  1.23709031 1.20988722 1.18250707 1.15496158\n",
      " 1.12726206 1.09941932 1.07144371 1.04334512 1.01513298 0.98681624\n",
      " 0.95840343 0.9299026  0.90132137 0.87266694 0.84394606 0.81516508\n",
      " 0.78632994 0.75744619 0.72851899 0.69955314 0.67055305 0.64152282\n",
      " 0.6124662  0.5833866  0.55428715 0.52517068 0.49603972 0.46689656\n",
      " 0.43774322 0.4085815  0.37941296 0.35023896 0.32106066 0.29187905\n",
      " 0.26269496 0.23350905 0.20432188 0.17513384 0.14594528 0.1167564\n",
      " 0.08756736 0.05837825 0.02918913] 0.5892695101103118\n",
      "4 [2.00682568 2.01284315 2.0180451  2.0224252  2.02597809 2.0286994\n",
      " 2.03058578 2.03163488 2.03184542 2.03121714 2.02975081 2.02744824\n",
      " 2.02431228 2.02034679 2.01555663 2.00994766 2.00352669 1.99630149\n",
      " 1.98828071 1.97947392 1.9698915  1.95954467 1.94844542 1.93660645\n",
      " 1.92404116 1.91076361 1.89678844 1.88213083 1.8668065  1.85083159\n",
      " 1.83422267 1.81699664 1.79917074 1.78076244 1.76178945 1.74226961\n",
      " 1.7222209  1.70166138 1.68060911 1.65908216 1.63709855 1.61467617\n",
      " 1.59183283 1.56858612 1.54495346 1.52095205 1.49659879 1.47191033\n",
      " 1.44690297 1.4215927  1.39599513 1.37012552 1.34399869 1.3176291\n",
      " 1.29103075 1.26421721 1.23720162 1.20999666 1.18261454 1.15506702\n",
      " 1.1273654  1.09952049 1.07154266 1.04344179 1.01522732 0.98690821\n",
      " 0.95849297 0.92998968 0.90140595 0.87274898 0.84402554 0.81524197\n",
      " 0.78640421 0.75751782 0.72858797 0.69961943 0.67061666 0.64158372\n",
      " 0.61252437 0.58344205 0.55433986 0.52522064 0.49608693 0.46694101\n",
      " 0.43778491 0.40862042 0.3794491  0.35027232 0.32109125 0.29190686\n",
      " 0.26271999 0.23353131 0.20434135 0.17515053 0.14595918 0.11676752\n",
      " 0.0875757  0.05838382 0.02919191] 0.007134034897490792\n",
      "5 [2.00682568 2.01284315 2.0180451  2.02242521 2.0259781  2.02869941\n",
      " 2.03058578 2.03163489 2.03184543 2.03121715 2.02975081 2.02744825\n",
      " 2.02431229 2.0203468  2.01555664 2.00994767 2.0035267  1.9963015\n",
      " 1.98828072 1.97947393 1.96989151 1.95954469 1.94844543 1.93660646\n",
      " 1.92404118 1.91076363 1.89678845 1.88213085 1.86680652 1.85083161\n",
      " 1.83422269 1.81699666 1.79917076 1.78076246 1.76178946 1.74226962\n",
      " 1.72222092 1.70166139 1.68060913 1.65908218 1.63709857 1.61467619\n",
      " 1.59183284 1.56858614 1.54495348 1.52095207 1.49659881 1.47191034\n",
      " 1.44690299 1.42159272 1.39599515 1.37012553 1.34399871 1.31762912\n",
      " 1.29103076 1.26421723 1.23720164 1.20999667 1.18261455 1.15506704\n",
      " 1.12736541 1.09952051 1.07154267 1.04344181 1.01522733 0.98690822\n",
      " 0.95849298 0.92998969 0.90140596 0.872749   0.84402555 0.81524198\n",
      " 0.78640422 0.75751784 0.72858798 0.69961944 0.67061667 0.64158373\n",
      " 0.61252438 0.58344206 0.55433987 0.52522064 0.49608693 0.46694101\n",
      " 0.43778491 0.40862042 0.37944911 0.35027233 0.32109125 0.29190687\n",
      " 0.26272    0.23353131 0.20434135 0.17515054 0.14595919 0.11676753\n",
      " 0.08757571 0.05838382 0.02919191] 1.045648689538472e-06\n",
      "6 [2.00682568 2.01284315 2.0180451  2.02242521 2.0259781  2.02869941\n",
      " 2.03058578 2.03163489 2.03184543 2.03121715 2.02975081 2.02744825\n",
      " 2.02431229 2.0203468  2.01555664 2.00994767 2.0035267  1.9963015\n",
      " 1.98828072 1.97947393 1.96989151 1.95954469 1.94844543 1.93660646\n",
      " 1.92404118 1.91076363 1.89678845 1.88213085 1.86680652 1.85083161\n",
      " 1.83422269 1.81699666 1.79917076 1.78076246 1.76178946 1.74226962\n",
      " 1.72222092 1.70166139 1.68060913 1.65908218 1.63709857 1.61467619\n",
      " 1.59183284 1.56858614 1.54495348 1.52095207 1.49659881 1.47191034\n",
      " 1.44690299 1.42159272 1.39599515 1.37012553 1.34399871 1.31762912\n",
      " 1.29103076 1.26421723 1.23720164 1.20999667 1.18261455 1.15506704\n",
      " 1.12736541 1.09952051 1.07154267 1.04344181 1.01522733 0.98690822\n",
      " 0.95849298 0.92998969 0.90140596 0.872749   0.84402555 0.81524198\n",
      " 0.78640422 0.75751784 0.72858798 0.69961944 0.67061667 0.64158373\n",
      " 0.61252438 0.58344206 0.55433987 0.52522064 0.49608693 0.46694101\n",
      " 0.43778491 0.40862042 0.37944911 0.35027233 0.32109125 0.29190687\n",
      " 0.26272    0.23353131 0.20434135 0.17515054 0.14595919 0.11676753\n",
      " 0.08757571 0.05838382 0.02919191] 2.2573685220323112e-14\n",
      "[2.00682568 2.01284315 2.0180451  2.02242521 2.0259781  2.02869941\n",
      " 2.03058578 2.03163489 2.03184543 2.03121715 2.02975081 2.02744825\n",
      " 2.02431229 2.0203468  2.01555664 2.00994767 2.0035267  1.9963015\n",
      " 1.98828072 1.97947393 1.96989151 1.95954469 1.94844543 1.93660646\n",
      " 1.92404118 1.91076363 1.89678845 1.88213085 1.86680652 1.85083161\n",
      " 1.83422269 1.81699666 1.79917076 1.78076246 1.76178946 1.74226962\n",
      " 1.72222092 1.70166139 1.68060913 1.65908218 1.63709857 1.61467619\n",
      " 1.59183284 1.56858614 1.54495348 1.52095207 1.49659881 1.47191034\n",
      " 1.44690299 1.42159272 1.39599515 1.37012553 1.34399871 1.31762912\n",
      " 1.29103076 1.26421723 1.23720164 1.20999667 1.18261455 1.15506704\n",
      " 1.12736541 1.09952051 1.07154267 1.04344181 1.01522733 0.98690822\n",
      " 0.95849298 0.92998969 0.90140596 0.872749   0.84402555 0.81524198\n",
      " 0.78640422 0.75751784 0.72858798 0.69961944 0.67061667 0.64158373\n",
      " 0.61252438 0.58344206 0.55433987 0.52522064 0.49608693 0.46694101\n",
      " 0.43778491 0.40862042 0.37944911 0.35027233 0.32109125 0.29190687\n",
      " 0.26272    0.23353131 0.20434135 0.17515054 0.14595919 0.11676753\n",
      " 0.08757571 0.05838382 0.02919191]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def params():\n",
    "    nvar = 99; m = 1. # We have 99 middle points, 1,2,...,99\n",
    "    xini, xfin = 2., 0. # end points are fixed at 2 and 0\n",
    "    tt = 1.; dt = tt/(nvar+1) # total time 1\n",
    "    return nvar, m, xini, xfin, dt\n",
    "\n",
    "def fod(der,x):\n",
    "    return -x**3 if der==0 else -3*x**2\n",
    "\n",
    "def gradient(xs):\n",
    "    nvar, m, xini, xfin, dt = params()\n",
    "    arr = np.zeros(nvar)\n",
    "    arr[0] = (m/dt)*(2*xs[0]-xini-xs[1]) + dt * fod(0,xs[0])\n",
    "    arr[1:-1] = (m/dt)*(2*xs[1:-1] - xs[:-2] - xs[2:]) + dt*fod(0,xs[1:-1])\n",
    "    arr[-1] = (m/dt)*(2*xs[-1]-xs[-2]-xfin) +dt * fod(0,xs[-1])\n",
    "    return arr\n",
    "\n",
    "def hessian(xs):\n",
    "    nvar, m, xini, xfin, dt = params()\n",
    "    he = np.diag(2*m/dt+ dt*fod(1,xs))\n",
    "    np.fill_diagonal(he[1:,:], -m/dt)   \n",
    "    np.fill_diagonal(he[:,1:], -m/dt)\n",
    "    return he\n",
    "\n",
    "def multi_newton(gradient,hessian,xolds,kmax=200,tol=1.e-8):\n",
    "    for k in range(1,kmax):\n",
    "        grad_xolds = gradient(xolds)\n",
    "        he_xolds = hessian(xolds)\n",
    "        xnews = xolds + gauelim_pivot(he_xolds, -grad_xolds)\n",
    "        err = termcrit(xolds,xnews)\n",
    "        print(k, xnews, err)\n",
    "        if err < tol:\n",
    "            break\n",
    "        xolds = np.copy(xnews)\n",
    "    else:\n",
    "        xnews = None\n",
    "    return xnews\n",
    "\n",
    "\n",
    "def gauelim_pivot(inA,inbs):\n",
    "    A = np.copy(inA)\n",
    "    bs = np.copy(inbs)\n",
    "    n = bs.size\n",
    "\n",
    "    for j in range(n-1):\n",
    "        k = np.argmax(np.abs(A[j:,j])) + j\n",
    "        if k != j:\n",
    "            A[j,:], A[k,:] = A[k,:], A[j,:].copy()\n",
    "            bs[j], bs[k] = bs[k], bs[j]\n",
    "\n",
    "        for i in range(j+1,n):\n",
    "            coeff = A[i,j]/A[j,j]\n",
    "            A[i,j:] -= coeff*A[j,j:]\n",
    "            bs[i] -= coeff*bs[j]\n",
    "\n",
    "    xs = backsub(A,bs)\n",
    "    return xs\n",
    "\n",
    "def backsub(U,bs):\n",
    "    n = bs.size\n",
    "    xs = np.zeros(n)\n",
    "    for i in reversed(range(n)):\n",
    "        xs[i] = (bs[i] - U[i,i+1:]@xs[i+1:])/U[i,i]\n",
    "    return xs\n",
    "\n",
    "def termcrit(xolds,xnews):\n",
    "    errs = np.abs((xnews - xolds)/xnews)\n",
    "    return np.sum(errs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nvar, m, xini, xfin, dt = params()\n",
    "    xolds = np.linspace(2,0,nvar+2)[1:-1]\n",
    "    xnews = multi_newton(gradient, hessian, xolds); print(xnews)\n",
    "    tlist = np.linspace(0,1,nvar+2)\n",
    "    plt.plot(tlist[1:-1],xolds,label='initial guess')\n",
    "    plt.plot(tlist[1:-1],xnews, label='minimized')\n",
    "    plt.legend()\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x(t)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "Modify the above code for minimizing the action to address the physical context of the _harmonic oscillator_, namely \n",
    "$$\n",
    "V(x) = \\frac{1}{2}x^2\n",
    "$$\n",
    "Take $m=1$, $x_0 = 0$, $x_{n−1} = 1$ and $T = 1$. \n",
    "Plot the coordinate $x(t)$ of the particle as a function of time;\n",
    "also show the (analytically known) answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1fa0d82bdabb5288f7efc0788d29c4d5bb5f690328690a3d32d2cd65de760c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
