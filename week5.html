<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Matrices I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="week5_files/libs/clipboard/clipboard.min.js"></script>
<script src="week5_files/libs/quarto-html/quarto.js"></script>
<script src="week5_files/libs/quarto-html/popper.min.js"></script>
<script src="week5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="week5_files/libs/quarto-html/anchor.min.js"></script>
<link href="week5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="week5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="week5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="week5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="week5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation"><span class="toc-section-number">1</span>  Motivation</a>
  <ul class="collapse">
  <li><a href="#examples-from-physics" id="toc-examples-from-physics" class="nav-link" data-scroll-target="#examples-from-physics"><span class="toc-section-number">1.1</span>  Examples from Physics</a>
  <ul class="collapse">
  <li><a href="#rotations-in-two-dimensions" id="toc-rotations-in-two-dimensions" class="nav-link" data-scroll-target="#rotations-in-two-dimensions">Rotations in two dimensions</a></li>
  <li><a href="#electrostatic-potentials" id="toc-electrostatic-potentials" class="nav-link" data-scroll-target="#electrostatic-potentials">Electrostatic potentials</a></li>
  <li><a href="#principle-moments-of-inertia" id="toc-principle-moments-of-inertia" class="nav-link" data-scroll-target="#principle-moments-of-inertia">Principle moments of inertia</a></li>
  </ul></li>
  <li><a href="#the-problems-to-be-solved" id="toc-the-problems-to-be-solved" class="nav-link" data-scroll-target="#the-problems-to-be-solved"><span class="toc-section-number">1.2</span>  The problems to be solved</a></li>
  </ul></li>
  <li><a href="#error-analysis" id="toc-error-analysis" class="nav-link" data-scroll-target="#error-analysis"><span class="toc-section-number">2</span>  Error Analysis</a>
  <ul class="collapse">
  <li><a href="#from-a-posteriori-to-a-priori-estimates" id="toc-from-a-posteriori-to-a-priori-estimates" class="nav-link" data-scroll-target="#from-a-posteriori-to-a-priori-estimates"><span class="toc-section-number">2.1</span>  From <em>a posteriori</em> to <em>a priori</em> Estimates</a></li>
  <li><a href="#magnitude-of-determinant" id="toc-magnitude-of-determinant" class="nav-link" data-scroll-target="#magnitude-of-determinant"><span class="toc-section-number">2.2</span>  Magnitude of Determinant?</a>
  <ul class="collapse">
  <li><a href="#example-1" id="toc-example-1" class="nav-link" data-scroll-target="#example-1">Example 1</a></li>
  <li><a href="#example-2" id="toc-example-2" class="nav-link" data-scroll-target="#example-2">Example 2</a></li>
  <li><a href="#example-3" id="toc-example-3" class="nav-link" data-scroll-target="#example-3">Example 3</a></li>
  </ul></li>
  <li><a href="#norms-for-matrices-and-vectors" id="toc-norms-for-matrices-and-vectors" class="nav-link" data-scroll-target="#norms-for-matrices-and-vectors"><span class="toc-section-number">2.3</span>  Norms for Matrices and Vectors</a>
  <ul class="collapse">
  <li><a href="#example-4" id="toc-example-4" class="nav-link" data-scroll-target="#example-4">Example 4</a></li>
  <li><a href="#definitions-and-properties-for-matrices" id="toc-definitions-and-properties-for-matrices" class="nav-link" data-scroll-target="#definitions-and-properties-for-matrices">Definitions and Properties for Matrices</a></li>
  <li><a href="#definitions-for-vectors" id="toc-definitions-for-vectors" class="nav-link" data-scroll-target="#definitions-for-vectors">Definitions for Vectors</a></li>
  </ul></li>
  <li><a href="#condition-number-for-linear-systems" id="toc-condition-number-for-linear-systems" class="nav-link" data-scroll-target="#condition-number-for-linear-systems"><span class="toc-section-number">2.4</span>  Condition Number for Linear Systems</a>
  <ul class="collapse">
  <li><a href="#example-5" id="toc-example-5" class="nav-link" data-scroll-target="#example-5">Example 5</a></li>
  <li><a href="#example-6" id="toc-example-6" class="nav-link" data-scroll-target="#example-6">Example 6</a></li>
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation">Derivation</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  </ul></li>
  <li><a href="#condition-number-for-simple-eigenvalues" id="toc-condition-number-for-simple-eigenvalues" class="nav-link" data-scroll-target="#condition-number-for-simple-eigenvalues"><span class="toc-section-number">2.5</span>  Condition Number for Simple Eigenvalues</a>
  <ul class="collapse">
  <li><a href="#example-7" id="toc-example-7" class="nav-link" data-scroll-target="#example-7">Example 7</a></li>
  <li><a href="#example-8" id="toc-example-8" class="nav-link" data-scroll-target="#example-8">Example 8</a></li>
  <li><a href="#derivation-1" id="toc-derivation-1" class="nav-link" data-scroll-target="#derivation-1">Derivation</a></li>
  <li><a href="#remarks" id="toc-remarks" class="nav-link" data-scroll-target="#remarks">Remarks</a></li>
  </ul></li>
  <li><a href="#sensitivity-of-eigenvectors" id="toc-sensitivity-of-eigenvectors" class="nav-link" data-scroll-target="#sensitivity-of-eigenvectors"><span class="toc-section-number">2.6</span>  Sensitivity of eigenvectors</a>
  <ul class="collapse">
  <li><a href="#derivation-2" id="toc-derivation-2" class="nav-link" data-scroll-target="#derivation-2">Derivation</a></li>
  <li><a href="#example-9" id="toc-example-9" class="nav-link" data-scroll-target="#example-9">Example 9</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework"><span class="toc-section-number">3</span>  Homework</a></li>
  </ul>
</nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Matrices I</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="motivation" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Motivation</h1>
<p>Linear algebra pops up almost everywhere in physics, so the matrix-related techniques developed below will be used repeatedly in later lectures. As a result, we will spend lots of time on matrices. We will take the time to introduce several numerical techniques in detail.</p>
<section id="examples-from-physics" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="examples-from-physics"><span class="header-section-number">1.1</span> Examples from Physics</h2>
<p>We discuss some elementary examples from undergraduate physics.</p>
<section id="rotations-in-two-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="rotations-in-two-dimensions">Rotations in two dimensions</h3>
<p>Consider a two-dimensional Cartesian coordinate system. A point <span class="math inline">\boldsymbol{r} = (x,y)^T</span> can be rotated counter-clockwise through an angle <span class="math inline">\theta</span> about the origin, producing a new point <span class="math inline">\boldsymbol{r}' = (x',y')^T</span>. The two points’ coordinates are related as follows: <span class="math display">
\begin{pmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
=
\begin{pmatrix}
x' \\
y'
\end{pmatrix}
</span> The <span class="math inline">2\times 2</span> matrix appearing here is an example of a <em>rotation matrix</em> in Euclidean space. If you know <span class="math inline">\boldsymbol{r}'</span> and wish to calculate <span class="math inline">\boldsymbol{r}</span>, you need to solve this system of two linear equations.</p>
</section>
<section id="electrostatic-potentials" class="level3">
<h3 class="anchored" data-anchor-id="electrostatic-potentials">Electrostatic potentials</h3>
<p>Assume you have <span class="math inline">n</span> electric charges <span class="math inline">q_j</span> (which are unknown) held at the positions <span class="math inline">\boldsymbol{R}_j</span> (which are known). Further assume that you have measured the electric potential <span class="math inline">\phi(\boldsymbol{r}_i)</span> at the <span class="math inline">n</span> known positions <span class="math inline">\boldsymbol{r}_i</span>. From the definition of the potential (as well as the fact that the potential obeys the principle of superposition), we see that: <span class="math display">
\phi(\boldsymbol{r}_i) = \sum_{j=0}^{n-1}\left(\frac{k q_j}{|\boldsymbol{r}_i - \boldsymbol{R}_j|}\right),
</span> where <span class="math inline">i = 0,1,\dots,n-1</span>. If you assume you have four charges, the above relation turns into the following <span class="math inline">4\times 4</span> linear systems of equations: <span class="math display">
\begin{pmatrix}
k/|\boldsymbol{r}_0 - \boldsymbol{R}_0| &amp;k/|\boldsymbol{r}_0 - \boldsymbol{R}_1| &amp;k/|\boldsymbol{r}_0 - \boldsymbol{R}_2| &amp;k/|\boldsymbol{r}_0 - \boldsymbol{R}_3| \\
k/|\boldsymbol{r}_1 - \boldsymbol{R}_0| &amp;k/|\boldsymbol{r}_1 - \boldsymbol{R}_1| &amp;k/|\boldsymbol{r}_1 - \boldsymbol{R}_2| &amp;k/|\boldsymbol{r}_1 - \boldsymbol{R}_3| \\
k/|\boldsymbol{r}_2 - \boldsymbol{R}_0| &amp;k/|\boldsymbol{r}_2 - \boldsymbol{R}_1| &amp;k/|\boldsymbol{r}_2 - \boldsymbol{R}_2| &amp;k/|\boldsymbol{r}_2 - \boldsymbol{R}_3| \\
k/|\boldsymbol{r}_3 - \boldsymbol{R}_0| &amp;k/|\boldsymbol{r}_3 - \boldsymbol{R}_1| &amp;k/|\boldsymbol{r}_3 - \boldsymbol{R}_2| &amp;k/|\boldsymbol{r}_3 - \boldsymbol{R}_3|
\end{pmatrix}
\begin{pmatrix}
q_0 \\ q_1 \\ q_2 \\ q_3
\end{pmatrix}
=
\begin{pmatrix}
\phi(\boldsymbol{r}_0) \\ \phi(\boldsymbol{r}_1) \\ \phi(\boldsymbol{r}_2) \\ \phi(\boldsymbol{r}_3)
\end{pmatrix}
</span> which needs to be solved for the 4 unknowns <span class="math inline">q_0</span>, <span class="math inline">q_1</span>, <span class="math inline">q_2</span> and <span class="math inline">q_3</span>.</p>
</section>
<section id="principle-moments-of-inertia" class="level3">
<h3 class="anchored" data-anchor-id="principle-moments-of-inertia">Principle moments of inertia</h3>
<p>In study of the rotation of a rigid body about an arbitrary axis in three dimensions, you may have encountered the moment of inertia tensor: <span class="math display">
I_{\alpha \beta} = \int \rho(\boldsymbol{r}) \left(\delta_{\alpha \beta}r^2 - \boldsymbol{r}_\alpha \boldsymbol{r}_\beta\right)d^3 r,
</span> where <span class="math inline">\rho(\boldsymbol{r})</span> is the mass density, <span class="math inline">\alpha</span> and <span class="math inline">\beta</span> denote Cartesian components, and <span class="math inline">\delta_{\alpha \beta}</span> is the Kronecker delta.</p>
<p>The moment of inertia tensor is represented by a <span class="math inline">3\times 3</span> matrix: <span class="math display">
\boldsymbol{I} =
\begin{pmatrix}
I_{xx} &amp; I_{xy} &amp; I_{xz} \\
I_{yx} &amp; I_{yy} &amp; I_{yz} \\
I_{zx} &amp; I_{zy} &amp; I_{zz}.
\end{pmatrix}
</span> This is a symmetric matrix. It is possible to choose a coordinate system such that the off-diagonal elements vanish. This axes of this coordinate system are known as the <em>principal axes</em> for the body at the origin. Then the moment of inertian tensor is represented by a diagonal matrix, with diagonal elements <span class="math inline">I_0</span>, <span class="math inline">I_1</span>, and <span class="math inline">I_2</span>, known as the principal moments. This is an instance of the “eigenvalue problem”.</p>
</section>
</section>
<section id="the-problems-to-be-solved" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="the-problems-to-be-solved"><span class="header-section-number">1.2</span> The problems to be solved</h2>
<p>First, we look at the problem where we have <span class="math inline">n</span> unknowns <span class="math inline">x_i</span>, along with <span class="math inline">n\times n</span> coefficients <span class="math inline">A_{ij}</span> and <span class="math inline">n</span> constants <span class="math inline">b_i</span>: <span class="math display">
\begin{pmatrix}
A_{00} &amp; A_{01} &amp; \dots &amp; A_{0,n-1} \\
A_{10} &amp; A_{11} &amp; \dots &amp; A_{1,n-1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{n-1,0} &amp; A_{n-1,1} &amp; \dots &amp; A_{n-1,n-1}
\end{pmatrix}
\begin{pmatrix}
x_0 \\ x_1 \\ \vdots \\ x_{n-1}
\end{pmatrix}
=
\begin{pmatrix}
b_0 \\ b_1 \\ \vdots \\ b_{n-1}
\end{pmatrix}
</span> where we used a comma to separate two indices when this was necessary to avoid confusion. These are <span class="math inline">n</span> equations linear in <span class="math inline">n</span> unknowns.</p>
<p>In compact matrix form, this problem is written as <span class="math display">
\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b},
</span> where <span class="math inline">\boldsymbol{A}</span> is called the <em>coefficient matrix</em>. This is a problem that we will spend considerable time solving in this lecture. We will be doing this mainly by using the <em>augmented coefficient matrix</em> which places together the elements of <span class="math inline">\boldsymbol{A}</span> and <span class="math inline">\boldsymbol{b}</span>, i.e.: <span class="math display">
(\boldsymbol{A}|\boldsymbol{b})= \left(
\begin{matrix}
A_{00} &amp; A_{01} &amp; \dots &amp; A_{0,n-1} \\
A_{10} &amp; A_{11} &amp; \dots &amp; A_{1,n-1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{n-1,0} &amp; A_{n-1,1} &amp; \dots &amp; A_{n-1,n-1}
\end{matrix}\right|
\left.
\begin{matrix}
b_0 \\ b_1 \\ \vdots \\b_{n-1}
\end{matrix}
\right).
</span> For now we assume the determinant of <span class="math inline">\boldsymbol{A}</span> satisfy <span class="math inline">|\boldsymbol{A}| \neq 0</span>.</p>
<p>In a course on linear algebra you have seen examples of legitimate operations one can carry out while solving the system of linear equations. Such operations change the elements of <span class="math inline">\boldsymbol{A}</span> and <span class="math inline">\boldsymbol{b}</span>, but leave the solution vector <span class="math inline">\boldsymbol{x}</span> unchanged. More generally, we are allowed to carry the following elementary row operations:</p>
<ul>
<li><em>Scaling</em>: each row/equation may be multiplied by a constant (multiplies <span class="math inline">|\boldsymbol{A}|</span> by the same constant).</li>
<li><em>Pivoting</em>: two rows/equations may be interchanged (changes sign of <span class="math inline">|\boldsymbol{A}|</span>).</li>
<li><em>Elimination</em>: a row/equation may be replaced by a linear combination of that row/equation with any other row/equation (doesn’t change <span class="math inline">|\boldsymbol{A}|</span>).</li>
</ul>
<p>Keep in mind that these are operations that are carried out on the augmented coefficient matrix <span class="math inline">(\boldsymbol{A}|\boldsymbol{b})</span>.</p>
<p>Second, we wish to tackle the standard form of the matrix eigenvalue problem: <span id="eq-eigenvalue"><span class="math display">
\boldsymbol{A}\boldsymbol{v} = \lambda \boldsymbol{v}.
\tag{1}</span></span> Here, both <span class="math inline">\lambda</span> and the column vector <span class="math inline">\boldsymbol{v}</span> are unknown. This <span class="math inline">\lambda</span> is called an <em>eigenvalue</em> and <span class="math inline">\boldsymbol{v}</span> is called an <em>eigenvector</em>.</p>
<p>Let’s sketch one possible approach to solve this problem. We can move everything to the left-hand side, we have <span class="math display">
(\boldsymbol{A} - \lambda \boldsymbol{I})\boldsymbol{v} = \boldsymbol{0},
</span> where <span class="math inline">\boldsymbol{I}</span> is the <span class="math inline">n\times n</span> identity matrix and <span class="math inline">\boldsymbol{0}</span> is an <span class="math inline">n\times 1</span> column vector made up of <span class="math inline">0</span>s. It is easy to see that we are faced with a system of <span class="math inline">n</span> linear equations: the coefficient matrix here is <span class="math inline">A - \lambda \boldsymbol{I}</span>.</p>
<p>The trivial solution is <span class="math inline">\boldsymbol{v} = 0</span>. In order for a non-trivial solution to exist, we must have vanishing determinant <span class="math inline">|\boldsymbol{A} - \lambda \boldsymbol{I}| = 0</span>. In other words, the matrix <span class="math inline">\boldsymbol{A} - \lambda \boldsymbol{I}</span> is singular. Expanding the determinant gives us a polynomial equation, known as the <em>characteristic equation</em>: <span class="math display">
(-1)^n\lambda^n + c_{n-1} \lambda^{n-1} + \cdots + c_1 \lambda + c_0 = 0.
</span></p>
<p>Thus, an <span class="math inline">n \times n</span> matrix has at most <span class="math inline">n</span> distinct eigenvalues, which are the roots of the characteristic polynomial. When a root occurs twice, we say that root has multiplicity <span class="math inline">2</span>. If a root occurs only once, in other words if it has multiplicity 1, we are dealing with a <em>simple</em> eigenvalue.</p>
<p>Having calculated the eigenvalues, one way to evaluate the eigenvectors is simply by using <a href="#eq-eigenvalue">Equation&nbsp;1</a> again.</p>
<ul>
<li>Specifically, for a given/known eigenvalue, <span class="math inline">\lambda_i</span>, one tries to solve the system of linear equations <span class="math inline">(\boldsymbol{A}-\lambda_i\boldsymbol{I})\boldsymbol{v}_i = 0</span> for <span class="math inline">\boldsymbol{v}_i</span>.</li>
<li>For each value <span class="math inline">\lambda_i</span>, we will not be able to determine unique values of <span class="math inline">\boldsymbol{v}_i</span>, so we will limit ourselves to computing the relative values of the components of <span class="math inline">\boldsymbol{v}_i</span>.</li>
<li>We will in the following use the notation <span class="math inline">(v_j)_0</span>, <span class="math inline">(v_j)_1</span> etc. to denote the <span class="math inline">n</span> elements of the column vector <span class="math inline">\boldsymbol{v}_j</span>.</li>
</ul>
</section>
</section>
<section id="error-analysis" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Error Analysis</h1>
<p>We now turn to a discussion of practical error estimation in work with matrices. we will provide some general derivations and examples of when a problem is “well-conditioned”, typically by using matrix perturbation theory (i.e., by checking what happens if there are uncertainties in the input data).</p>
<p>After some preliminary comments, examples, and definitions, we will investigate quantitatively how linear systems, eigenvalues, and eigenvectors depend on the input data. We will be examining in each case the simplest scenario but, hopefully, this will be enough to help you grasp the big picture.</p>
<section id="from-a-posteriori-to-a-priori-estimates" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="from-a-posteriori-to-a-priori-estimates"><span class="header-section-number">2.1</span> From <em>a posteriori</em> to <em>a priori</em> Estimates</h2>
<p>Let us look at a specific <span class="math inline">2\times 2</span> linear system, namely <span class="math inline">\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}</span> for the case where <span id="eq-linear_eq"><span class="math display">
(\boldsymbol{A}|\boldsymbol{b}) = \left(
\begin{matrix}
0.2161 &amp; 0.1441 \\
1.2969 &amp; 0.8648
\end{matrix}\ \right|
\left.
\begin{matrix}
0.1440 \\
0.8642
\end{matrix}
\right).
\tag{2}</span></span></p>
<p>Simply put, there are two options on how to analyze errors:</p>
<ol type="a">
<li>an <em>a priori</em> analysis, in which case we try to see how easy/hard the problem is to solve before we begin solving it.</li>
<li>an <em>a posteriori</em> analysis, where we have produced a solution, and attempt to see how good it is.</li>
</ol>
<p>Let us start with the latter option, an <em>a posteriori</em> approach. Say you are provided with the following approximate solution to the problem defined in <a href="#eq-linear_eq">Equation&nbsp;2</a>: <span id="eq-approximate_sol"><span class="math display">
\tilde{\boldsymbol{x}}^T = (0.9911 \quad -0.4870).
\tag{3}</span></span></p>
<p>One way of testing how good a solution is, is to evaluate the residue vector: <span class="math display">
\boldsymbol{r} = \boldsymbol{b} - \boldsymbol{A} \tilde{\boldsymbol{x}}.
</span> Plugging in <a href="#eq-approximate_sol">Equation&nbsp;3</a>, we find the residue vector <span class="math display">
\boldsymbol{r}^T = (-10^{-8} \quad 10^{-8})
</span> which might naturally lead you to the conclusion that our approximate solution <span class="math inline">\tilde{\boldsymbol{x}}</span> is pretty good!</p>
<p>However, here’s the thing: the exact solution to our problem is actually: <span class="math display">
\boldsymbol{x}^T = (-2 \quad 2).
</span> The approximate solution <span class="math inline">\tilde{\boldsymbol{x}}</span> doesn’t contain even a single correct significant figure!</p>
<p>With the disclaimer that there’s much more that could be said at the a posteriori level, we now drop this line of attack and turn to an <em>a priori</em> analysis: could we have realized that solving the problem in <a href="#eq-linear_eq">Equation&nbsp;2</a> was difficult? How could we know that there’s something pathological about it?</p>
</section>
<section id="magnitude-of-determinant" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="magnitude-of-determinant"><span class="header-section-number">2.2</span> Magnitude of Determinant?</h2>
<section id="example-1" class="level3">
<h3 class="anchored" data-anchor-id="example-1">Example 1</h3>
<p>In an attempt to see what is wrong with our previous example in <a href="#eq-linear_eq">Equation&nbsp;2</a>, we start to make small perturbation to the input data. Imagine we didn’t know the values of the coefficients in <span class="math inline">\boldsymbol{A}</span> all that precisely. Would anything change?</p>
<p>Let us take <span class="math display">
\Delta \boldsymbol{A} =
\begin{pmatrix}
0.0001 &amp; 0 \\
0 &amp; 0
\end{pmatrix}
</span></p>
<p>We want to study the effect of this perturbation on the solution, namely <span class="math display">
(\boldsymbol{A} + \Delta \boldsymbol{A})(\boldsymbol{x} + \Delta \boldsymbol{x}) = \boldsymbol{b},
</span> where <span class="math inline">\boldsymbol{b}</span> is kept fixed/unperturbed. For the specific case studied here, we find (using the following program) <span class="math display">
(\boldsymbol{x} + \Delta \boldsymbol{x})^T  = (-2.31294091\times 10^{-4} \quad 0.99653059\times 10^{-1}).
</span></p>
<p>We see that this is not a “small” effect. Our perturbation amounted to changing only one element of <span class="math inline">\boldsymbol{A}</span> by less than <span class="math inline">0.1\%</span>, and had a dramatic impact on the solution to our problem.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="fl">0.2161</span>, <span class="fl">0.1441</span>],[<span class="fl">1.2969</span>, <span class="fl">0.8648</span>]])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>deltaA <span class="op">=</span> np.array([[<span class="fl">0.0001</span>, <span class="dv">0</span>],[<span class="dv">0</span>, <span class="dv">0</span>]])</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="fl">0.1440</span>, <span class="fl">0.8642</span>])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we use np.linalg.solve(A,b) to solve the equation Ax = b.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linalg.solve(A,b)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>x_dx <span class="op">=</span> np.linalg.solve(A<span class="op">+</span>deltaA,b)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_dx)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"determinant of A is: "</span>, np.linalg.det(A))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 2. -2.]
[-2.31294091e-04  9.99653059e-01]
determinant of A is:  -9.999999998544968e-09</code></pre>
</div>
</div>
</section>
<section id="example-2" class="level3">
<h3 class="anchored" data-anchor-id="example-2">Example 2</h3>
<p>Let us look at the following example <span class="math display">
(\boldsymbol{A} | \boldsymbol{b}) =
\left(
    \begin{matrix}
    1 &amp; 1 \\
    1 &amp; 1.001
    \end{matrix}\
\right|
\left.
    \begin{matrix}
    2 \\
    2.001
    \end{matrix}
\right).
</span> The exact solution is (from the following program) <span class="math display">
\boldsymbol{x}^T = (1 \quad 1).
</span> We can then add a perturbation <span class="math display">
\Delta\boldsymbol{A} =
\begin{pmatrix}
0 &amp; 0 \\
0 &amp; 0.001
\end{pmatrix}.
</span> Then, the perturbed solution is <span class="math display">
(\boldsymbol{x} + \Delta\boldsymbol{x})^T = (1.5 \quad 0.5).
</span></p>
<p>Instead of adding a perturbation to <span class="math inline">\boldsymbol{A}</span>, one can also add a perturbation to <span class="math inline">\boldsymbol{b}</span>, with <span class="math display">
\Delta \boldsymbol{b}^T = (0 \quad 0.001).
</span> We find (in the following program) <span class="math display">
(\boldsymbol{x} + \Delta \boldsymbol{x})^T = (0 \quad 2).
</span></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>],[<span class="dv">1</span>, <span class="fl">1.001</span>]])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>deltaA <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>],[<span class="dv">0</span>, <span class="fl">0.001</span>]])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="fl">2.001</span>])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>deltab <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="fl">0.001</span>])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># we use np.linalg.solve(A,b) to solve the equation Ax = b.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linalg.solve(A,b)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>x_dx <span class="op">=</span> np.linalg.solve(A<span class="op">+</span>deltaA,b)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>x_dx2 <span class="op">=</span> np.linalg.solve(A,b<span class="op">+</span>deltab)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_dx)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_dx2)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"determinant of A is: "</span>, np.linalg.det(A))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1. 1.]
[1.5 0.5]
[0. 2.]
determinant of A is:  0.00099999999999989</code></pre>
</div>
</div>
</section>
<section id="example-3" class="level3">
<h3 class="anchored" data-anchor-id="example-3">Example 3</h3>
<p>There are also cases where small perturbations won’t lead to dramatic consequences in the solutions. See the following code.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># A|b: 2   1 | 2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#      1   2 | 7</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>],[<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">2</span>,<span class="dv">7</span>])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta A: 0    0</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#          0.01 0</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>deltaA <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>],[<span class="fl">0.01</span>, <span class="dv">0</span>]])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta b: 0.01</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">#          0</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>deltab <span class="op">=</span> np.array([<span class="fl">0.01</span>, <span class="dv">0</span>])</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># we use np.linalg.solve(A,b) to solve the equation Ax = b.</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linalg.solve(A,b)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>x_dx <span class="op">=</span> np.linalg.solve(A<span class="op">+</span>deltaA,b)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>x_dx2 <span class="op">=</span> np.linalg.solve(A,b<span class="op">+</span>deltab)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_dx)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_dx2)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"determinant of A is: "</span>, np.linalg.det(A))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[-1.  4.]
[-1.00334448  4.00668896]
[-0.99333333  3.99666667]
determinant of A is:  2.9999999999999996</code></pre>
</div>
</div>
</section>
</section>
<section id="norms-for-matrices-and-vectors" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="norms-for-matrices-and-vectors"><span class="header-section-number">2.3</span> Norms for Matrices and Vectors</h2>
<section id="example-4" class="level3">
<h3 class="anchored" data-anchor-id="example-4">Example 4</h3>
<p>Consider the following question: what does “small determinant” mean? If the definition is “much less than 1”, then one might counter-argue: what about the following matrix: <span class="math display">
\boldsymbol{A} =
\begin{pmatrix}
0.2 &amp; 0.1 \\
0.1 &amp; 0.2
\end{pmatrix},
</span> which is just the matrix <span class="math inline">\boldsymbol{A}</span> in Example 3 multiplied by 0.1. This matrix has a determinant <span class="math inline">\det(\boldsymbol{A}) = 0.03</span>, which is certainly much less than <span class="math inline">1</span>. If you also multiply each element of <span class="math inline">\boldsymbol{b}</span> in the previsou example by <span class="math inline">0.1</span>, you should get the same answer. What’s more, this linear system of equations should be equally sensitive to perturbations.</p>
<p>Thus, the value of the determinant should be compared with the magnitude of the relevant matrix elements.</p>
</section>
<section id="definitions-and-properties-for-matrices" class="level3">
<h3 class="anchored" data-anchor-id="definitions-and-properties-for-matrices">Definitions and Properties for Matrices</h3>
<p>Let us provide our intuitions with quantitative backing. We shall introduce the <em>matrix norm</em>, which measures the magnitude of <span class="math inline">\boldsymbol{A}</span>. There are several possible definitions of a norm, but we will employ two possibilities.</p>
<ol type="1">
<li><strong>Euclidean norm</strong>: <span class="math display">
\|\boldsymbol{A} \|_E = \sqrt{\sum_{i=0}^{n-1}\sum_{j=0}^{n-1} |A_{ij}|^2},
</span> which is sometimes also called the <em>Frobenius norm</em>.</li>
<li><strong>Infinity norm</strong>: <span class="math display">
\| \boldsymbol{A} \|_{\infty} = \max_{0\leq i \leq n-1} \sum_{j=0}^{n-1} |A_{ij}|,
</span> which is also known as the <em>maximum row-sum norm</em>.</li>
</ol>
<p>Regardless of the specific norm definitions, all matrix norms for square matrices obey:</p>
<ul>
<li><span class="math inline">\|\boldsymbol{A} \| \geq 0</span></li>
<li><span class="math inline">\| \boldsymbol{A} \| = 0</span> if and only if all <span class="math inline">A_{ij} = 0</span></li>
<li><span class="math inline">\| k \boldsymbol{A} \| = |k| \|\boldsymbol{A}\|</span></li>
<li><span class="math inline">\| \boldsymbol{A} + \boldsymbol{B} \| \leq \| \boldsymbol{A} \| + \| \boldsymbol{B} \|</span></li>
<li><span class="math inline">\| \boldsymbol{A B} \| \leq \|\boldsymbol{A}\|\|\boldsymbol{B}\|</span></li>
</ul>
<p>Now, we return to the question when the determinant is “small”. A reasonable definition would be <span class="math inline">|\det(\boldsymbol{A})| \ll \|\boldsymbol{A}\|</span>.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>A1 <span class="op">=</span> np.array([[<span class="fl">0.2161</span>, <span class="fl">0.1441</span>],[<span class="fl">1.2969</span>, <span class="fl">0.8648</span>]])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>A2 <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>],[<span class="dv">1</span>, <span class="fl">1.001</span>]])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>A3 <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>],[<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>A4<span class="op">=</span> np.array([[<span class="fl">0.2</span>, <span class="fl">0.1</span>],[<span class="fl">0.1</span>, <span class="fl">0.2</span>]])</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>Alist <span class="op">=</span> [A1, A2, A3, A4]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ii,A <span class="kw">in</span> <span class="bu">enumerate</span>(Alist):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Example"</span>, ii<span class="op">+</span><span class="dv">1</span>, <span class="st">": det(A) ="</span>, np.linalg.det(A), <span class="st">", Euclidean norm ="</span>, np.linalg.norm(A))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Example 1 : det(A) = -9.999999998544968e-09 , Euclidean norm = 1.5802824652573981
Example 2 : det(A) = 0.00099999999999989 , Euclidean norm = 2.000500187453128
Example 3 : det(A) = 2.9999999999999996 , Euclidean norm = 3.1622776601683795
Example 4 : det(A) = 0.03000000000000001 , Euclidean norm = 0.31622776601683794</code></pre>
</div>
</div>
<p>These results seem to be consistent with what we had seen above:</p>
<ul>
<li>Examples 1 and 2 are near-singular, while Example 3 is not singular.</li>
<li>For Example 4, this criterion claims that our matrix is not quite singular (though it’s getting there).</li>
</ul>
<p>Our introduction of the concept of the matrix norm seems to have served its purpose: a small determinant needs to be compared to the matrix norm, so Example 4 (despite having a small determinant) is not singular, given that its matrix elements are small, too.</p>
</section>
<section id="definitions-for-vectors" class="level3">
<h3 class="anchored" data-anchor-id="definitions-for-vectors">Definitions for Vectors</h3>
<p>Let us also introduce norms for vector norms: <span class="math display">
\|\boldsymbol{x}\|_E = \sqrt{\sum_{i = 0}^{n-1} |x_i|^2}, \quad \|\boldsymbol{x} \|_{\infty} = \max_{0 \leq i \leq n-1} |x_i|.
</span></p>
</section>
</section>
<section id="condition-number-for-linear-systems" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="condition-number-for-linear-systems"><span class="header-section-number">2.4</span> Condition Number for Linear Systems</h2>
<p>Unfortunately, our criterion <span class="math inline">|\det(\boldsymbol{A})|\ll \| \boldsymbol{A} \|</span> is flawed, though this appears in many textbooks. We will look at two examples.</p>
<section id="example-5" class="level3">
<h3 class="anchored" data-anchor-id="example-5">Example 5</h3>
<p>We shall consider <span class="math display">
\boldsymbol{A} =
\begin{pmatrix}
2 \times 10^{-10} &amp; 1 \times 10^{-10} \\
1 \times 10^{-10} &amp; 2 \times 10^{-10}
\end{pmatrix},
</span> which is the matrix in Example 3 multiplied by <span class="math inline">10^{-10}</span>. Here, we have <span class="math inline">|\det(\boldsymbol{A})| = 3 \times 10^{-20}</span>, and <span class="math inline">\|\boldsymbol{A} \|_E \simeq 3.16 \times 10^{-10}</span>, so <span class="math inline">|\det(\boldsymbol{A})| \ll \|\boldsymbol{A} \|_E</span> holds.</p>
<p>But isn’t this strange? Simply multiplying a set of equations with a small number cannot be enough to make the problem near-singular.</p>
</section>
<section id="example-6" class="level3">
<h3 class="anchored" data-anchor-id="example-6">Example 6</h3>
<p>Let us look at the following <span class="math inline">8 \times 8</span> problem: <span class="math display">
\boldsymbol{A} =
\begin{pmatrix*}[r]
2 &amp; -2 &amp; -2 &amp; \cdots &amp; -2 \\
0 &amp; 2 &amp; -2 &amp; \cdots &amp; -2 \\
0 &amp; 0 &amp; 2 &amp; \cdots &amp; -2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 2
\end{pmatrix*}
</span> The corresponding results are <span class="math inline">|\det(\boldsymbol{A})| = 256</span>, and <span class="math inline">\|\boldsymbol{A} \|_E = 12</span>, so <span class="math inline">|\det(\boldsymbol{A})| \gg \|\boldsymbol{A} \|_E</span> holds.</p>
<p>Now, take a look at the following code.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">2</span>]])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>deltaA <span class="op">=</span> np.zeros((<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>deltaA[<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>] <span class="op">=</span> <span class="op">-</span><span class="fl">0.01</span>  <span class="co"># bottom-left elememnt is -0.01</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Delta b: 0.01</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">#          0</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>deltab <span class="op">=</span> np.array([<span class="fl">0.01</span>, <span class="dv">0</span>])</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># we use np.linalg.solve(A,b) to solve the equation Ax = b.</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linalg.solve(A,b)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>x_dx <span class="op">=</span> np.linalg.solve(A<span class="op">+</span>deltaA,b)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_dx)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"determinant of A is: "</span>, np.linalg.det(A))</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"norm of A is: "</span>, np.linalg.norm(A))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[-21.  -11.   -5.   -3.   -1.   -1.    0.   -0.5]
[-30.88235294 -15.94117647  -7.47058824  -4.23529412  -1.61764706
  -1.30882353  -0.15441176  -0.65441176]
determinant of A is:  255.99999999999994
norm of A is:  12.0</code></pre>
</div>
</div>
</section>
<section id="derivation" class="level3">
<h3 class="anchored" data-anchor-id="derivation">Derivation</h3>
<p>In the present subsection, we will carry out an informal derivation that will point us toward a quantitative measure of ill-conditioning. This measure of the sensitivity of our problem to small changes in its elements will be called the condition number.</p>
<p>Let us start with the unperturbed problem <span class="math display">
\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}
</span> and the perturbed one <span class="math display">
(\boldsymbol{A} + \Delta \boldsymbol{A})(\boldsymbol{x} + \Delta \boldsymbol{x}) = \boldsymbol{b}.
</span></p>
<p>Combining the above two equations, we have <span class="math display">
\boldsymbol{A}\Delta \boldsymbol{x} = -\Delta \boldsymbol{A}(\boldsymbol{x} + \Delta \boldsymbol{x}).
</span> Assuming <span class="math inline">\boldsymbol{A}</span> is nonsingular (so you can invert it), we get <span class="math display">
\Delta \boldsymbol{x} = -\boldsymbol{A}^{-1}\Delta \boldsymbol{A} (\boldsymbol{x} + \Delta \boldsymbol{x}).
</span> Now, we take the norm on both sides, we obtain <span class="math display">
\| \Delta \boldsymbol{x}\| = \| \boldsymbol{A}^{-1}\Delta \boldsymbol{A} (\boldsymbol{x} + \Delta \boldsymbol{x})\|
\leq \| \boldsymbol{A}^{-1} \| \| \Delta \boldsymbol{A} \| \| \boldsymbol{x}+ \Delta \boldsymbol{x} \|.
</span></p>
<p>This means <span class="math display">
\frac{\| \Delta \boldsymbol{x} \|}{\|\boldsymbol{x} + \Delta \boldsymbol{x} \|}
\leq \| \boldsymbol{A}^{-1} \| \|\Delta \boldsymbol{A} \| =
\| \boldsymbol{A}\|\| \boldsymbol{A}^{-1} \| \frac{\|\Delta \boldsymbol{A} \|}{\| \boldsymbol{A}\|}.
</span></p>
<p>In other words, if you know an error bound on <span class="math inline">\|\Delta \boldsymbol{A} \|/ \|\boldsymbol{A} \|</span> then translates to an error bound on <span class="math inline">\|\Delta \boldsymbol{x}\|/\|\boldsymbol{x}\| \simeq \|\Delta \boldsymbol{x}\|/\|\boldsymbol{x} + \Delta \boldsymbol{x}\|</span>.</p>
<p>This leads to the introduction of the <em>condition number</em>: <span class="math display">
\kappa(\boldsymbol{A}) = \| \boldsymbol{A}\|\| \boldsymbol{A}^{-1} \|,
</span> which determines if a small perturbation gets amplified when solving for <span class="math inline">\boldsymbol{x}</span> or not.</p>
<p>A large condition number leads to an amplification of a small perturbation: we say we are dealing with an <em>ill-conditioned</em> problem. If the condition number is of order unity, then a small perturbation is not amplified, so we are dealing with a <em>well-conditioned</em> problem.</p>
</section>
<section id="examples" class="level3">
<h3 class="anchored" data-anchor-id="examples">Examples</h3>
<ul>
<li>Example 1: <span class="math inline">\kappa(\boldsymbol{A}) = 249729267.388</span>, ill-conditioned.</li>
<li>Example 2: <span class="math inline">\kappa(\boldsymbol{A}) = 4002.001</span>, ill-conditioned.</li>
<li>Example 3: <span class="math inline">\kappa(\boldsymbol{A}) = 3.33</span>, well-conditioned</li>
<li>Example 4: <span class="math inline">\kappa(\boldsymbol{A}) = 3.33</span>, well-conditioned</li>
<li>Example 5: <span class="math inline">\kappa(\boldsymbol{A}) = 3.33</span>, well-conditioned</li>
<li>Example 6: <span class="math inline">\kappa(\boldsymbol{A}) = 512.18</span>, ill-conditioned</li>
</ul>
</section>
</section>
<section id="condition-number-for-simple-eigenvalues" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="condition-number-for-simple-eigenvalues"><span class="header-section-number">2.5</span> Condition Number for Simple Eigenvalues</h2>
<p>Having studied the sensitivity of a linear system to small perturbations, the natural place to go from there is to carry out an analogous study for the eigenvalue problem. For now, let us focus on the effect of the perturbations on the evaluation of the eigenvalues, <span class="math inline">\lambda</span>.</p>
<section id="example-7" class="level3">
<h3 class="anchored" data-anchor-id="example-7">Example 7</h3>
<p>Take <span class="math display">
\boldsymbol{A} =
\begin{pmatrix}
4 &amp; 3 &amp; 2 &amp; 1 \\
3 &amp; 3 &amp; 2 &amp; 1 \\
0 &amp; 2 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 1
\end{pmatrix}.
</span> You can evaluate its eigenvalues using the following python code, with <code>np.linalg.eigvals()</code>.</p>
<p>We now add a perturbation 0.01 to the top right element. It is easy to see that the smaller the eigenvalue, the bigger the impact of the small perturbation.</p>
<p>You may be thinking that this specific matrix may have a large condition number <span class="math inline">\kappa(\boldsymbol{A})</span>, which is indeed large as <span class="math inline">\kappa(\boldsymbol{A})\simeq 126.744</span>.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]],dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Eigenvalues of A:'</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.sort(np.linalg.eigvals(A)))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'======================'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Condition number of A:'</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.linalg.norm(A)<span class="op">*</span>np.linalg.norm(np.linalg.inv(A)))</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'======================'</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>A[<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span>  <span class="fl">0.01</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Eigenvalues of A + dA:'</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.sort(np.linalg.eigvals(A)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Eigenvalues of A:
[0.13674761 0.48387934 2.06663092 7.31274213]
======================
Condition number of A:
126.74383614203887
======================
Eigenvalues of A + dA:
[0.12477715 0.49937427 2.06287308 7.3129755 ]</code></pre>
</div>
</div>
</section>
<section id="example-8" class="level3">
<h3 class="anchored" data-anchor-id="example-8">Example 8</h3>
<p>Let look at another example with a smaller condition number. However, its eigenvalues are more sensitive to perturbations. In the following code.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>],</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]],dtype<span class="op">=</span>np.float64)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Eigenvalues of A:'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.sort(np.linalg.eigvals(A)))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'======================'</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Condition number of A:'</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.linalg.norm(A)<span class="op">*</span>np.linalg.norm(np.linalg.inv(A)))</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'======================'</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Perturb the bottom left one</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>A[<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>] <span class="op">=</span> A[<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>] <span class="op">+</span>  <span class="fl">0.005</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Eigenvalues of A + dA:'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.sort(np.linalg.eigvals(A)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Eigenvalues of A:
[1. 2. 3. 4.]
======================
Condition number of A:
40.129477943277564
======================
Eigenvalues of A + dA:
[0.95115768 2.18205744 2.81794256 4.04884232]</code></pre>
</div>
</div>
</section>
<section id="derivation-1" class="level3">
<h3 class="anchored" data-anchor-id="derivation-1">Derivation</h3>
<p>We saw that some matrices have eigenvalues that are sensitive to small perturbations, whereas others do not. In the present subsection, we will carry out an informal derivation that will point us toward a quantitative measure of conditioning eigenvalues (this is not <span class="math inline">\kappa(\boldsymbol{A})</span>). This quantitative measure of the sensitivity of our problem to small changes in the input matrix elements will be called the <em>condition number for simple eigenvalues</em>; “simple” means we don’t have repeated eigenvalues.</p>
<p>Let us start with the unperturbed problem <span id="eq-unperturbed"><span class="math display">
\boldsymbol{A}\boldsymbol{v}_i = \lambda_i \boldsymbol{v}_i,
\tag{4}</span></span> and the perturbed problem <span id="eq-perturbed"><span class="math display">
(\boldsymbol{A} + \Delta\boldsymbol{A})(\boldsymbol{v}_i + \Delta \boldsymbol{v}_i) = (\lambda_i + \Delta \lambda_i)( \boldsymbol{v}_i + \Delta \boldsymbol{v}_i).
\tag{5}</span></span></p>
<p>We do not assume <span class="math inline">\boldsymbol{A}</span> is symmetric, so the “eigenvectors” should actually be called <em>right eigenvectors</em>. We can also introduce the <em>left eigenvectors</em> as follows <span class="math display">
\boldsymbol{u}_i^T \boldsymbol{A} = \lambda_i\boldsymbol{u}_i^T.
</span> Note that more generally we should take the hermitian conjugate/conjugate-transpose <span class="math inline">\dagger</span>, but this is not important here as <span class="math inline">\boldsymbol{A}</span> is real.</p>
<p>Notice that the left eigenvectors of <span class="math inline">\boldsymbol{A}</span> are actually right eigenvectors of <span class="math inline">\boldsymbol{A}^T</span>, namely <span class="math display">
\boldsymbol{A}^T\boldsymbol{u}_i = \lambda_i \boldsymbol{u}_i.
</span> In fact, <span class="math inline">\boldsymbol{A}^T</span> and <span class="math inline">\boldsymbol{A}</span> have the same eigenvalues. In other words, left and right eigenvectors have the same eigenvalues (homework).</p>
<p>Now, combining <a href="#eq-unperturbed">Equation&nbsp;4</a> and <a href="#eq-perturbed">Equation&nbsp;5</a>, and dropping terms of second order in <span class="math inline">\Delta</span>, we obtain <span class="math display">
\boldsymbol{A}\Delta \boldsymbol{v}_i + \Delta \boldsymbol{A} \boldsymbol{v}_i
=
\lambda_i \Delta \boldsymbol{v}_i + \Delta \lambda_i \boldsymbol{v}_i.
</span></p>
<p>Multiplying the last equation with <span class="math inline">\boldsymbol{u}_i^T</span> on the left, we obtain <span class="math display">
\boldsymbol{u}_i^T \boldsymbol{A} \Delta \boldsymbol{v}_i
+ \boldsymbol{u}_i^T \Delta \boldsymbol{A} \boldsymbol{v}_i
=
\lambda_i \boldsymbol{u}_i^T \Delta \boldsymbol{v}_i
+ \Delta \lambda_i \boldsymbol{u}_i^T \boldsymbol{v}_i.
</span></p>
<p>The first term on the LHS and the first term on the RHS cancel, which gives rise to <span class="math display">
\boldsymbol{u}_i^T \Delta \boldsymbol{A} \boldsymbol{v}_i
= \Delta \lambda_i \boldsymbol{u}_i^T \boldsymbol{v}_i.
</span></p>
<p>Thus, we obtain <span class="math display">
|\Delta \lambda_i| = \frac{|\boldsymbol{u}_i^T \Delta \boldsymbol{A} \boldsymbol{v}_i|}{|\boldsymbol{u}_i^T \boldsymbol{v}_i|} \leq
\frac{1}{|\boldsymbol{u}_i^T \boldsymbol{v}_i|} \|\Delta \boldsymbol{A}\|,
</span> where in the last inequality we have used the fact <span class="math inline">|\boldsymbol{u}_i^T \Delta \boldsymbol{A} \boldsymbol{v}_i|\leq \|\boldsymbol{u}_i\| \|\Delta \boldsymbol{A}\| \|\boldsymbol{v}_i\| = \|\Delta \boldsymbol{A}\|</span>, with the assumption <span class="math inline">\|\boldsymbol{u}_i\| = \|\boldsymbol{v}_i\| = 1</span>.</p>
<p>Thus, we are led to introduce a new <em>condition number for simple eigenvalues</em>: <span class="math display">
\kappa_{ev}^{\lambda_i}(\boldsymbol{A}) = \frac{1}{|\boldsymbol{u}_i^T \boldsymbol{v}_i|},
</span> where the subscript is there to remind us that this is a condition number for a specific problem: for the evaluation of eigenvalues. The superscript keeps track of which specific eigenvalue’s sensitivity we are referring to.</p>
<p>Thus, we have the bound <span class="math display">
|\Delta \lambda_i| \leq \kappa_{ev}^{\lambda_i}(\boldsymbol{A}) \|\Delta \boldsymbol{A}\|
</span></p>
<p>To calculate the condition number for a given eigenvalue you first have to calculate the product of the corresponding left- and right-eigenvectors.</p>
</section>
<section id="remarks" class="level3">
<h3 class="anchored" data-anchor-id="remarks">Remarks</h3>
<p>Real symmetric (or complex hermitian) matrices are very common in physics. For these matrices, the left- and right-eigenvector are the same and thus we have <span class="math display">
\kappa_{ev}^{\lambda_i}(\boldsymbol{A}) = 1.
</span></p>
<p>Hence, for these matrices the eigenvalue problems is always <em>well-conditioned</em>.</p>
</section>
</section>
<section id="sensitivity-of-eigenvectors" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sensitivity-of-eigenvectors"><span class="header-section-number">2.6</span> Sensitivity of eigenvectors</h2>
<section id="derivation-2" class="level3">
<h3 class="anchored" data-anchor-id="derivation-2">Derivation</h3>
<p>Let us now derive the quantative measure of conditioning eigenvectors. Let us start with <span id="eq-perturb-eigenvector"><span class="math display">
\boldsymbol{A}\Delta \boldsymbol{v}_i + \Delta \boldsymbol{A} \boldsymbol{v}_i
=
\lambda_i \Delta \boldsymbol{v}_i + \Delta \lambda_i \boldsymbol{v}_i.
\tag{6}</span></span></p>
<p>We now expand the perturbation in the eigenvector in terms of the other eigenvectors <span class="math display">
\Delta \boldsymbol{v}_i = \sum_{j\neq i} t_{ji} \boldsymbol{v}_j.
</span></p>
<p>Here, the coefficients <span class="math inline">t_{ji}</span> are to be determined. We are employing here the linear independence of the eigenvectors. Note that this sum does not include a <span class="math inline">j=i</span> term. This is because, for <span class="math inline">\boldsymbol{v}_i + \Delta \boldsymbol{v}_i</span>, if <span class="math inline">\Delta \boldsymbol{v}_i</span> has a component of <span class="math inline">\boldsymbol{v}_i</span>, then we can simply combine this component with <span class="math inline">\boldsymbol{v}_i</span>, which means we simply adjust the coefficient in front <span class="math inline">\boldsymbol{v}_i</span>.</p>
<p>Now, if we plug this expansion into <a href="#eq-perturb-eigenvector">Equation&nbsp;6</a>, we obtain <span class="math display">
\sum_{j\neq i}(\lambda_j - \lambda_i) t_{ji} \boldsymbol{v}_j + \Delta \boldsymbol{A} \boldsymbol{v}_i = \Delta \lambda_i \boldsymbol{v}_i.
</span></p>
<p>Multiplying <span class="math inline">\boldsymbol{u}_k^T</span> from the left, and using the fact the left- and right-eigenvectors for different eigenvalues are orthogonal, namely <span class="math inline">\boldsymbol{u}_k^T \boldsymbol{v}_i = 0</span> for <span class="math inline">k\neq i</span>, we find <span class="math display">
(\lambda_k - \lambda_i) t_{ki} \boldsymbol{u}_k^T\boldsymbol{v}_k
+ \boldsymbol{u}_k^T \Delta \boldsymbol{A} \boldsymbol{v}_i = 0,
</span> and this means <span class="math display">
t_{ki} = \frac{\boldsymbol{u}_k^T \Delta \boldsymbol{A} \boldsymbol{v}_i}{(\lambda_i - \lambda_k)\boldsymbol{u}_k^T \boldsymbol{v}_k}.
</span></p>
<p>Thus, we obtain <span class="math display">
\Delta \boldsymbol{v}_i =
\sum_{j\neq i }
\frac{\boldsymbol{u}_j^T \Delta \boldsymbol{A} \boldsymbol{v}_i}{(\lambda_i - \lambda_j)\boldsymbol{u}_j^T \boldsymbol{v}_j} \boldsymbol{v}_j.
</span></p>
<ul>
<li>We see that the numerator contains the perturbation <span class="math inline">\Delta \boldsymbol{A}</span>.</li>
<li>The denominator contains two distinct contributions. (1) a <span class="math inline">\boldsymbol{u}_j^T \boldsymbol{v}_j</span> term, which is the same thing when we define the condition number for a simple eigenvalue. (2) a <span class="math inline">(\lambda_i - \lambda_j)</span> term, which is the separation between <span class="math inline">\lambda_i</span> and other eigenvalues.</li>
</ul>
</section>
<section id="example-9" class="level3">
<h3 class="anchored" data-anchor-id="example-9">Example 9</h3>
<p>See the following code. We see that while the eigenvalues are well-conditioned, they happen to be very closely spaced, which is a small number in the denominator, and thus makes the eigenvectors sensitive to perturbation.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="fl">1.01</span>, <span class="fl">0.01</span>],</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,    <span class="fl">0.99</span>]])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Find eigenvalues and eigenvectors</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Before perturbation:'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>ee,ev <span class="op">=</span> np.linalg.eig(A) </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(ee)):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Eigenvalue'</span>, ee[jj], <span class="st">'with eigenvector'</span>, ev[:,jj])</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'===================================================='</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After perturbation:'</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>A[<span class="dv">0</span>,] <span class="op">+=</span> <span class="fl">0.005</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>ee,ev <span class="op">=</span> np.linalg.eig(A) </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(ee)):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Eigenvalue'</span>, ee[jj], <span class="st">'with eigenvector'</span>, ev[:,jj])</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'===================================================='</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before perturbation:
Eigenvalue 1.01 with eigenvector [1. 0.]
====================================================
Eigenvalue 0.99 with eigenvector [-0.4472136   0.89442719]
====================================================
After perturbation:
Eigenvalue 1.01 with eigenvector [1. 0.]
====================================================
Eigenvalue 0.99 with eigenvector [-0.6  0.8]
====================================================</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="homework" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Homework</h1>
<ol type="1">
<li>Prove that left and right eigenvalues are equivalent.</li>
<li>Compute the eigenvalues and eigenvectors (using <code>np.linalg.eig()</code>) of <span class="math display">
\boldsymbol{A} =
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 4 &amp; 5 \\
0 &amp; 0 &amp; 4.001
\end{pmatrix}.
</span> We then introduce a perturbation, which adds 0.005 to the bottom left element. Please compute eigenvalues and eigenvectors for the perturbed matrix. Please also calculate the condition number for each eigenvalues (using python).</li>
</ol>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>